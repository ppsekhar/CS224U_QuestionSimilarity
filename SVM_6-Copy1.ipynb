{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following is the SVM implimentation for the SVM\n",
    "# Files needed: file of negative questions and posistive questions (new_neg_data_3.txt, new_pos_data_3.txt)\n",
    "# Dependecy tree features imported from pickle file created separately\n",
    "# This file must have all glove dependencies imported in order to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM Precision recall and F score (after tuning)\n",
    "vsmdata_home = \"vsmdata\"\n",
    "glove_home = \"glove.6B\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import unicodecsv as csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils\n",
    "import numpy\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "glv = utils.build_glove(os.path.join(glove_home, 'glove.6B.50d.txt'))\n",
    "\n",
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
    "\n",
    "#neighbors(word='gnarly', mat=gnmat_lsa[0], rownames=gnmat_lsa[1])\n",
    "\n",
    "\n",
    "def distance_between_words(word1, word2, mat = glv[0], rownames=glv[1], distfunc=cosine):\n",
    "    if word1 not in rownames or word2 not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word1)\n",
    "    w1 = mat[rownames.index(word1)]\n",
    "    w2 = mat[rownames.index(word2)]\n",
    "    \n",
    "    return distfunc(w1,w2)\n",
    "    \n",
    "def distance_between_sentences(str1, str2, mat = glv[0], rownames = glv[1], distfunc = cosine):\n",
    "    str1_list = str1.split()\n",
    "    str2_list = str2.split()\n",
    "    \n",
    "    str1_vecs = [mat[rownames.index(word)] for word in str1_list]\n",
    "    str2_vecs = [mat[rownames.index(word)] for word in str2_list]\n",
    "    \n",
    "    str1_combined_vecs = 0\n",
    "    str2_combined_vecs = 0\n",
    "    \n",
    "    for vec in str1_vecs:\n",
    "        str1_combined_vecs += vec\n",
    "    \n",
    "    for vec in str2_vecs:\n",
    "        str2_combined_vecs += vec\n",
    "        \n",
    "    return distfunc(str1_combined_vecs, str2_combined_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim #make sure you did pip install gensim\n",
    "import gensim.models.doc2vec as doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "def make_distance_vectors(file_name):\n",
    "    #expects a file with one sentence per line\n",
    "    #Returns a dict with format {sentence : vector}\n",
    "    \n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be sloww otherwise\"\n",
    "\n",
    "    data_file = \"data.txt\" \n",
    "\n",
    "    with open(file_name) as f:\n",
    "        file_content = f.readlines()\n",
    "    \n",
    "    \n",
    "    sentences = doc2vec.TaggedLineDocument(file_name)\n",
    "    model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4, iter=20) #can take a while, can also modify parameters to change vectors\n",
    "    model.save(\"doc2vec_model_1\")\n",
    "\n",
    "    model = Doc2Vec.load(\"doc2vec_model_1\") #unnecessary in this case, but can use it after you have already created a model\n",
    "    vector_dict = []\n",
    "\n",
    "    cur_pair = 0\n",
    "    for i in range(0, len(file_content), 2):\n",
    "        arr = cosine(model.docvecs[i], model.docvecs[i+1])\n",
    "        vector_dict.append(arr)\n",
    "        #arr.append(distance_between_sentences(file_content[i],file_content[i+1]))\n",
    "        #vector_dict.append([distance_between_sentences(file_content[i],file_content[i+1]), cosine(model.docvecs[i], model.docvecs[i+1])])\n",
    "        #vector_dict.append(distance_between_sentences(model.docvecs[i], model.docvecs[i+1]))\n",
    "        cur_pair += 1\n",
    "    #print cur_pair\n",
    "    return (vector_dict, file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "\n",
    "def train(X, y,c, g):\n",
    "    clf = svm.SVC(C = c, gamma = g)\n",
    "    clf.fit(X,y)\n",
    "    print 'finished training svm'\n",
    "    return clf\n",
    "\n",
    "def test(clf, X):\n",
    "    predictions = clf.predict(X)\n",
    "    print 'finished predicting'\n",
    "    return predictions\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def word_overlap(s1, s2):\n",
    "    overlap = 0\n",
    "    s1_tokenize = s1.split()\n",
    "    s2_tokenize = s2.split()\n",
    "    used_array = []\n",
    "    for token in s1_tokenize:\n",
    "        if token in s2_tokenize:\n",
    "            if token not in used_array:\n",
    "                overlap += 1\n",
    "                used_array.append(token)\n",
    "    return overlap\n",
    "\n",
    "print word_overlap(\"hi hi\", \"hi hi\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(sentence1, sentence2):\n",
    "    s1_tokenize = sentence1.split()\n",
    "    s2_tokenize = sentence2.split()\n",
    "    s1_set = set(s1_tokenize)\n",
    "    s2_set = set(s2_tokenize)\n",
    "    #print s1_set.intersection(s2_set)\n",
    "    #print s1_set.union(s2_set)\n",
    "    return float(len(s1_set.intersection(s2_set)))/float(len(s1_set.union(s2_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean(u, v):    \n",
    "    \"\"\"Eculidean distance between 1d np.arrays `u` and `v`, which must \n",
    "    have the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.euclidean(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return vector_length(u - v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_words = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]\n",
    "def qword_indicator(s1, s2):\n",
    "    s1_tokenize = s1.split()\n",
    "    s2_tokenize = s2.split()\n",
    "    s1_set = set(s1_tokenize)\n",
    "    s2_set = set(s2_tokenize)\n",
    "    for word in question_words:\n",
    "        if word in s1_set.intersection(s2_set):\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_overlap(s1, s2):\n",
    "    s1_tokenize = s1.split()\n",
    "    s2_tokenize = s2.split()\n",
    "    bigram_overlap = 0\n",
    "    s1_bigrams = set([(s1_tokenize[i], s1_tokenize[i+1]) for i in range(0, len(s1_tokenize)-1)])\n",
    "    s2_bigrams = set([(s2_tokenize[i], s2_tokenize[i+1]) for i in range(0, len(s2_tokenize)-1)])\n",
    "    bigram_overlap = len(s1_bigrams.intersection(s2_bigrams))\n",
    "    return bigram_overlap\n",
    "\n",
    "bigram_overlap(\"How are you doing\", \"How are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_jaccard(s1, s2):\n",
    "    s1_tokenize = s1.split()\n",
    "    s2_tokenize = s2.split()\n",
    "    bigram_overlap = 0\n",
    "    s1_bigrams = set([(s1_tokenize[i], s1_tokenize[i+1]) for i in range(0, len(s1_tokenize)-1)])\n",
    "    s2_bigrams = set([(s2_tokenize[i], s2_tokenize[i+1]) for i in range(0, len(s2_tokenize)-1)])\n",
    "    return float(len(s1_bigrams.intersection(s2_bigrams)))/float(len(s1_bigrams.union(s2_bigrams)))\n",
    "\n",
    "bigram_jaccard(\"How are doing\", \"How are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_filecontents(filename):\n",
    "    with open(filename) as f:\n",
    "        file_content = f.readlines()\n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_features(filename):\n",
    "    \n",
    "    #doc2vec + cosine distance (TODO: get euclidean between vectors)\n",
    "    pair = make_distance_vectors(filename)\n",
    "    d2vresults = pair[0]\n",
    "    file_contents = pair[1]\n",
    "    features = []\n",
    "    \n",
    "    #Cosine distance between glove vector sentences\n",
    "    cosresults = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        cos_dist = distance_between_sentences(file_contents[i], file_contents[i+1])\n",
    "        cosresults.append(cos_dist)\n",
    "    \n",
    "    #word overlap (single word)\n",
    "    wordoverlap = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        overlap = word_overlap(file_contents[i], file_contents[i+1])\n",
    "        wordoverlap.append(overlap)\n",
    "    \n",
    "    #jaccard word overlap\n",
    "    jaccards = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        j_dist = jaccard(file_contents[i], file_contents[i+1])\n",
    "        jaccards.append(j_dist)\n",
    "        \n",
    "    #euclidean between golve vector sentences\n",
    "    eucs = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        euc_dist = distance_between_sentences(file_contents[i], file_contents[i+1], distfunc = euclidean)\n",
    "        eucs.append(euc_dist)\n",
    "    \n",
    "    #Question word indicator\n",
    "    qword = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        q = qword_indicator(file_contents[i], file_contents[i+1])\n",
    "        qword.append(q)\n",
    "        \n",
    "    #bigram overlap\n",
    "    bigrams = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        b_overlap = bigram_overlap(file_contents[i], file_contents[i+1])\n",
    "        bigrams.append(b_overlap)\n",
    "        \n",
    "    #bigram jaccard\n",
    "    bigram_jaccard_arr = []\n",
    "    for i in range(0, len(file_contents), 2):\n",
    "        b_jaccard = bigram_jaccard(file_contents[i], file_contents[i+1])\n",
    "        bigram_jaccard_arr.append(b_jaccard)\n",
    "    \n",
    "    #NER --> made below\n",
    "    #Dependency Tree Features (Cosine dist of roots and diff in tree depth) --> made below\n",
    "    \n",
    "    features = [d2vresults, cosresults, wordoverlap, jaccards, eucs, qword, bigrams, bigram_jaccard_arr]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples vectorized\n"
     ]
    }
   ],
   "source": [
    "X_neg_all_features = build_features('new_neg_data_3.txt')\n",
    "X_pos_all_features = build_features('new_pos_data_3.txt')\n",
    "print 'examples vectorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('stanford-corenlp-full-2015-12-09/negative_ner_raw.txt') as f:\n",
    "    file_content = f.readlines()\n",
    "\n",
    "NER = set([\"LOCATION\\n\", \"PERSON\\n\", \"ORGANIZATION\\n\", \"MONEY\\n\", \"PERCENT\\n\", \"DATE\\n\", \"TIME\\n\"])\n",
    "neg_named_entities = []\n",
    "pos_named_entities = []\n",
    "\n",
    "count = 0\n",
    "for line in file_content:\n",
    "    if line == '\\n':\n",
    "        neg_named_entities.append(count)\n",
    "        count = 0\n",
    "    elif line != '?\\n':\n",
    "        tokenized = line.split('\\t')\n",
    "        if tokenized[1] in NER:\n",
    "            count += 1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "with open('stanford-corenlp-full-2015-12-09/positive_ner_raw.txt') as f2:\n",
    "    file_content = f2.readlines()\n",
    "\n",
    "count = 0\n",
    "for line in file_content:\n",
    "    if line == '\\n':\n",
    "        pos_named_entities.append(count)\n",
    "        count = 0\n",
    "    elif line != '?\\n':\n",
    "        tokenized = line.split('\\t')\n",
    "        if tokenized[1] in NER:\n",
    "            count += 1\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8492\n",
      "8492\n",
      "9448\n",
      "9448\n"
     ]
    }
   ],
   "source": [
    "print len(pos_named_entities)\n",
    "print len(X_pos_all_features[1])*2\n",
    "print len(neg_named_entities)\n",
    "print len(X_neg_all_features[1])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_named_entities.pop()\n",
    "neg_named_entities.pop()\n",
    "neg_named_entities.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ner_neg_pickled = open('ner_neg_pickled_2.txt', 'wb')\n",
    "pickle.dump(neg_named_entities, ner_neg_pickled)\n",
    "ner_neg_pickled.close()\n",
    "\n",
    "ner_pos_pickled = open('ner_pos_pickled_2.txt', 'wb')\n",
    "pickle.dump(pos_named_entities, ner_pos_pickled)\n",
    "ner_pos_pickled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-179a298f96b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mner_neg_pickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ner_neg_pickled.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mneg_named_entities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner_neg_pickled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mner_neg_pickled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/priyanka/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/priyanka/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/priyanka/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_eof\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "ner_neg_pickled = open('ner_neg_pickled.txt', 'rb')\n",
    "neg_named_entities = pickle.load(ner_neg_pickled)\n",
    "ner_neg_pickled.close()\n",
    "\n",
    "ner_pos_pickled = open('ner_neg_pickled.txt', 'rb')\n",
    "pos_named_entities = pickle.load(ner_pos_pickled)\n",
    "ner_pos_pickled.close()\n",
    "\n",
    "print len(neg_named_entities)\n",
    "print len(pos_named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one contains named entity and other doesn't\n",
    "ner_feature_indicator_neg = []\n",
    "for i in range(0, len(neg_named_entities), 2):\n",
    "    if neg_named_entities[i] == 0 and neg_named_entities[i+1]>0:\n",
    "        ner_feature_indicator_neg.append(1)\n",
    "    elif neg_named_entities[i+1] == 0 and neg_named_entities[i]>0:\n",
    "        ner_feature_indicator_neg.append(1)\n",
    "    else:\n",
    "        ner_feature_indicator_neg.append(0)\n",
    "\n",
    "ner_feature_indicator_pos = []\n",
    "for i in range(0, len(pos_named_entities), 2):\n",
    "    if pos_named_entities[i] == 0 and pos_named_entities[i+1]>0:\n",
    "        ner_feature_indicator_pos.append(1)\n",
    "    elif pos_named_entities[i+1] == 0 and pos_named_entities[i]>0:\n",
    "        ner_feature_indicator_pos.append(1)\n",
    "    else:\n",
    "        ner_feature_indicator_pos.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_neg_all_features.append(ner_feature_indicator_neg)\n",
    "X_pos_all_features.append(ner_feature_indicator_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_features_master_neg = open('X_features_master_neg.txt', 'wb')\n",
    "pickle.dump(X_neg_all_features, X_features_master_neg)\n",
    "X_features_master_neg.close()\n",
    "\n",
    "X_features_master_pos = open('X_features_master_pos.txt', 'wb')\n",
    "pickle.dump(X_neg_all_features, X_features_master_pos)\n",
    "X_features_master_pos.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = open('X_features_master_neg.txt', 'rb')\n",
    "test = pickle.load(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4724\n"
     ]
    }
   ],
   "source": [
    "print len(X_neg_all_features[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "subsets = []\n",
    "\n",
    "feature_indices = [0,1,2,3,5,6,7,8]\n",
    "for L in range(0, len(feature_indices)+1):\n",
    "    for subset in itertools.combinations(feature_indices, L):\n",
    "        subsets.append(subset)\n",
    "        \n",
    "subsets.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8492\n",
      "4724\n"
     ]
    }
   ],
   "source": [
    "y_pos = [1 for x in X_pos_all_features[0]]\n",
    "y_neg = [0 for x in X_neg_all_features[0]]\n",
    "\n",
    "print len(y_pos)*2\n",
    "print len(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.47593972196091539,\n",
       "  0.055519788290799399,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  5.207769374046325,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  1],\n",
       " [0.55812436839118407,\n",
       "  0.056775578995398868,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  26.28430666731734,\n",
       "  0,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.79972720617010495,\n",
       "  0.1851849305769776,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  28.590311390028837,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.3350368965811135,\n",
       "  0.035194860982142528,\n",
       "  2,\n",
       "  0.25,\n",
       "  8.673662261774767,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.36200039693730612,\n",
       "  0.032435658299625469,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  14.213654989702244,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.56853540615147269,\n",
       "  0.15340313566103658,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  18.625056546199886,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.4056427510592671,\n",
       "  0.33317840318623215,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  23.63077657716825,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.3923407856684108,\n",
       "  0.069576848459948604,\n",
       "  4,\n",
       "  0.4,\n",
       "  9.530292303674187,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  1],\n",
       " [0.88912893857564046,\n",
       "  0.09828724543905043,\n",
       "  2,\n",
       "  0.125,\n",
       "  14.10128793570784,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  1],\n",
       " [0.6175887746352009,\n",
       "  0.21090702525660876,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  21.76443340636375,\n",
       "  0,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  1],\n",
       " [1.052688421464246,\n",
       "  0.13600404912328712,\n",
       "  3,\n",
       "  0.25,\n",
       "  22.855097048459434,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.1070112406643087,\n",
       "  0.065350801300627759,\n",
       "  3,\n",
       "  0.375,\n",
       "  9.040689694510393,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.45855987068592896,\n",
       "  0.099039450974167442,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  19.155215998461625,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [1.0310424795568698,\n",
       "  0.14447253140639937,\n",
       "  4,\n",
       "  0.5,\n",
       "  11.153423232531475,\n",
       "  0,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  1],\n",
       " [0.54042055661184296,\n",
       "  0.40548814490824181,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  20.56456220181091,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0662775319757705,\n",
       "  0.024391611024746407,\n",
       "  5,\n",
       "  0.625,\n",
       "  7.704562083473881,\n",
       "  1,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.75487662164425662,\n",
       "  0.17773879448587548,\n",
       "  3,\n",
       "  0.25,\n",
       "  23.47410343070658,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.95060257787145197,\n",
       "  0.16352287031052348,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  16.762398215971004,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.56858472659163573,\n",
       "  0.059561841217294464,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  14.321970769453904,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.0239072050049876,\n",
       "  0.048349730075134989,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  9.864940822507723,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.48532564029886505,\n",
       "  0.035553106691486147,\n",
       "  4,\n",
       "  0.5,\n",
       "  7.671320736750535,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.44402391868333779,\n",
       "  0.15923828137533003,\n",
       "  3,\n",
       "  0.375,\n",
       "  9.308625799176697,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.45020789309752451,\n",
       "  0.0308670515025532,\n",
       "  6,\n",
       "  0.46153846153846156,\n",
       "  12.430320779500953,\n",
       "  0,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  0],\n",
       " [0.94073326947110691,\n",
       "  0.029627978174089575,\n",
       "  3,\n",
       "  0.1875,\n",
       "  17.04084392906256,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.8692416102525653,\n",
       "  0.15605755277121391,\n",
       "  2,\n",
       "  0.25,\n",
       "  13.62699628530761,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.97763217225903853,\n",
       "  0.028828586323440319,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  7.026720984388757,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.48957999615729964,\n",
       "  0.015222245144902669,\n",
       "  6,\n",
       "  0.8571428571428571,\n",
       "  4.896174601903817,\n",
       "  0,\n",
       "  4,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.83667544005805605,\n",
       "  0.28798882507476076,\n",
       "  1,\n",
       "  0.125,\n",
       "  14.855392403725102,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.44081108175094996,\n",
       "  0.046164467036237,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  14.400964498614531,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2245396681040202,\n",
       "  0.067868613597175553,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  11.582830752067997,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.25146383097822766,\n",
       "  0.0091758495342374458,\n",
       "  6,\n",
       "  0.75,\n",
       "  9.055427285740123,\n",
       "  0,\n",
       "  4,\n",
       "  0.5,\n",
       "  0],\n",
       " [1.2983982047933631,\n",
       "  0.096050589036723477,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  10.111264950421466,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.99242589580779961,\n",
       "  0.20086183392472567,\n",
       "  3,\n",
       "  0.2,\n",
       "  24.76799713821745,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  1],\n",
       " [0.79469116349843794,\n",
       "  0.12297987616050132,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.546827625578599,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.0898177479020923,\n",
       "  0.21656766523083781,\n",
       "  1,\n",
       "  0.125,\n",
       "  14.339331640571139,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.56693828499675925,\n",
       "  0.14477421543375601,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  29.34952974204222,\n",
       "  0,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  1],\n",
       " [0.91544080727234944,\n",
       "  0.039510602611145473,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  7.818936493660506,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.70242947024091396,\n",
       "  0.058313141023601966,\n",
       "  3,\n",
       "  0.375,\n",
       "  9.027441788184978,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [1.350896995657892,\n",
       "  0.050927360358548945,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  8.189390123264726,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.67966720848005646,\n",
       "  0.080758339854170358,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  18.974210915669953,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.5810038517163334,\n",
       "  0.15951522124024431,\n",
       "  3,\n",
       "  0.25,\n",
       "  38.25198834836448,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1360972669825795,\n",
       "  0.062094487789282726,\n",
       "  5,\n",
       "  0.5,\n",
       "  10.420590968502076,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.6965574944457873,\n",
       "  0.051346176765963869,\n",
       "  6,\n",
       "  0.35294117647058826,\n",
       "  32.93420994659525,\n",
       "  0,\n",
       "  1,\n",
       "  0.045454545454545456,\n",
       "  0],\n",
       " [0.10419713325516977,\n",
       "  0.010486378901579108,\n",
       "  7,\n",
       "  0.875,\n",
       "  5.117475465707758,\n",
       "  1,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.3653727510016056,\n",
       "  0.008938328501805648,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  4.665677602524736,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [1.0233700513388329,\n",
       "  0.053551675902523521,\n",
       "  4,\n",
       "  0.4,\n",
       "  11.758566251411828,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.88838429602790248,\n",
       "  0.068098361197382107,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  7.624505920730015,\n",
       "  1,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.94087808041533205,\n",
       "  0.52879442573309898,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  39.88652276776741,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  1],\n",
       " [0.41370674804976415,\n",
       "  0.092248112335214016,\n",
       "  3,\n",
       "  0.5,\n",
       "  13.388710550062507,\n",
       "  0,\n",
       "  2,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.82450834384703398,\n",
       "  0.085135651011571056,\n",
       "  3,\n",
       "  0.375,\n",
       "  9.379433596562313,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.5065051181162874,\n",
       "  0.031222285946963857,\n",
       "  3,\n",
       "  0.25,\n",
       "  13.913444363447724,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.80345923364390637,\n",
       "  0.17235996598279746,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  22.60284410001579,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.56750218869524727,\n",
       "  0.013940992353714488,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  5.371390986405849,\n",
       "  1,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.85722173923942457,\n",
       "  0.39679533961478741,\n",
       "  3,\n",
       "  0.1875,\n",
       "  34.45704173756704,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.72225746794316503,\n",
       "  0.17934543727648644,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  13.515949724011092,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.65606898646253931,\n",
       "  0.15521372573688152,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  16.538484571997174,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.45122629768728351,\n",
       "  0.068734660000985159,\n",
       "  3,\n",
       "  0.5,\n",
       "  7.520241432839507,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.91040127160400008,\n",
       "  0.036637954037603282,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  17.396557869949834,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.287534395734795,\n",
       "  0.0088307902035787977,\n",
       "  6,\n",
       "  0.8571428571428571,\n",
       "  5.299874923497156,\n",
       "  0,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  0],\n",
       " [0.80607827961439338,\n",
       "  0.082518431264714365,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  19.467629302213176,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.1198405041683444,\n",
       "  0.11748068230019992,\n",
       "  3,\n",
       "  0.3,\n",
       "  12.128630338938665,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.4014652810921926,\n",
       "  0.22673814618008337,\n",
       "  3,\n",
       "  0.5,\n",
       "  10.149836648362346,\n",
       "  1,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.0132733096979787,\n",
       "  0.24956790854900002,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  9.17216753547786,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.49251441256922812,\n",
       "  0.05872355930288109,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  13.20504631557525,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.98067101274535373,\n",
       "  0.037397829504299929,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.67817103884079,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.18994989729890266,\n",
       "  0.023105405835096349,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  12.768196922528618,\n",
       "  1,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  1],\n",
       " [0.72223282444812054,\n",
       "  0.10149234154435083,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  19.821878313809002,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.094082835813447896,\n",
       "  0.041396667158133282,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  12.15832510584427,\n",
       "  0,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  0],\n",
       " [1.2197477436820476,\n",
       "  0.039047087736878927,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  11.555097566588847,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1077728002218155,\n",
       "  0.14289848497118218,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  29.79767595670705,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.59839726047314734,\n",
       "  0.098588997752773411,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.77979757006182,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.20513329719934759,\n",
       "  0.07269589743143734,\n",
       "  2,\n",
       "  0.4,\n",
       "  6.814800014402631,\n",
       "  0,\n",
       "  1,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.67564493723199182,\n",
       "  0.08581493324712608,\n",
       "  4,\n",
       "  0.5,\n",
       "  10.032588573737938,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0391967933721389,\n",
       "  0.20816067254642667,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  40.75247802050397,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.73317842351167195,\n",
       "  0.07276997142866104,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  28.719199545244134,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.2188740944621104,\n",
       "  0.28694353921126892,\n",
       "  1,\n",
       "  0.125,\n",
       "  9.960076696952234,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.76806345079961713,\n",
       "  0.012090899253950793,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  9.722259477201835,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.91878702781294219,\n",
       "  0.047039330096206089,\n",
       "  2,\n",
       "  0.2,\n",
       "  7.746115923091662,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.84665694506150646,\n",
       "  0.11230557173029398,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  13.75668361430716,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.244853425092558,\n",
       "  0.074805280874578606,\n",
       "  2,\n",
       "  0.2,\n",
       "  15.74506815618902,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69213785111123904,\n",
       "  0.056460904406550716,\n",
       "  3,\n",
       "  0.2,\n",
       "  27.911011235231108,\n",
       "  0,\n",
       "  2,\n",
       "  0.11764705882352941,\n",
       "  0],\n",
       " [1.0233047313731329,\n",
       "  0.099426400221974243,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  13.588033933359412,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.1177223121928526,\n",
       "  0.052442000553847179,\n",
       "  5,\n",
       "  0.625,\n",
       "  14.465554067074617,\n",
       "  1,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [1.548079423517513,\n",
       "  0.1332728648568684,\n",
       "  1,\n",
       "  0.1,\n",
       "  14.468005674497984,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.74104201542763426,\n",
       "  0.033838907942384311,\n",
       "  7,\n",
       "  0.2916666666666667,\n",
       "  55.18871966817944,\n",
       "  1,\n",
       "  3,\n",
       "  0.0967741935483871,\n",
       "  1],\n",
       " [0.81333758713314919,\n",
       "  0.073571942436291615,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  21.65974493838216,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.59742219216697801,\n",
       "  0.08924579814314848,\n",
       "  2,\n",
       "  0.125,\n",
       "  17.81561897050239,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0830372214578246,\n",
       "  0.076925835278126575,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  10.548911397968844,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.053231400400888917,\n",
       "  0.0,\n",
       "  6,\n",
       "  1.0,\n",
       "  1.4459562744518878e-15,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.86672553139622488,\n",
       "  0.041852614242430031,\n",
       "  4,\n",
       "  0.4,\n",
       "  8.114710832788754,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.3522924894570345,\n",
       "  0.083117058262292098,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  7.2687870151413945,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.39717374881205836,\n",
       "  0.042442134453346103,\n",
       "  5,\n",
       "  0.625,\n",
       "  12.03907320881848,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.14477670079629879,\n",
       "  0.043848064246380991,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  6.755843699900925,\n",
       "  0,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  0],\n",
       " [1.0045695821796403,\n",
       "  0.065218467098025479,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  11.542687405149014,\n",
       "  1,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.50294120547877685,\n",
       "  0.34301767421419616,\n",
       "  1,\n",
       "  0.1,\n",
       "  11.28887372590666,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.44480311210363777,\n",
       "  0.29803454820735231,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  27.514692672722227,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  1],\n",
       " [0.22835353136009129,\n",
       "  0.2409089646640723,\n",
       "  3,\n",
       "  0.5,\n",
       "  15.181475331747786,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.3529200633405503,\n",
       "  0.11885634489763552,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  33.76467635979068,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.31381692103115633,\n",
       "  0.24057065781036013,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  13.815921581305743,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  1],\n",
       " [0.69098706883926098,\n",
       "  0.18277390813010941,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  33.08406676942822,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.91036760487562851,\n",
       "  0.18457702368200446,\n",
       "  1,\n",
       "  0.1,\n",
       "  19.88097946343452,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.86202499541063671,\n",
       "  0.086905856834313622,\n",
       "  3,\n",
       "  0.375,\n",
       "  8.011978865156674,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  1],\n",
       " [0.41450993632047561,\n",
       "  0.056645986457499853,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  18.234405002494945,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.159264548001421,\n",
       "  0.048055380434359751,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  10.047270974667317,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.29375053772334969,\n",
       "  0.017708361517797422,\n",
       "  5,\n",
       "  0.625,\n",
       "  5.231669538436157,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  1],\n",
       " [0.92731015379208248,\n",
       "  0.08760595216673539,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  11.719008884369282,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.5477585561045053,\n",
       "  0.020666769544801511,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  8.714757786801602,\n",
       "  1,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  0],\n",
       " [0.36320041658948621,\n",
       "  0.027656577994998321,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  6.569963171173893,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.84653522571787643,\n",
       "  0.027072776285148925,\n",
       "  6,\n",
       "  0.5,\n",
       "  15.705752548816635,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.27799376359611261,\n",
       "  0.137739352944107,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  32.40196451685682,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.17641511283543798,\n",
       "  0.04756448897701504,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  14.145944621152347,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1985656548132357,\n",
       "  0.038162957717180257,\n",
       "  5,\n",
       "  0.5,\n",
       "  14.32713415796549,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.78900479260368961,\n",
       "  0.036776218316860176,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  16.0083926529226,\n",
       "  0,\n",
       "  5,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.87614932170044657,\n",
       "  0.0080792365974604596,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  5.426996778330997,\n",
       "  1,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [1.1680499374314732,\n",
       "  0.084257577665489802,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  19.870546465738204,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.3224131782556009,\n",
       "  0.11706680762077193,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  30.37361357480473,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.51463231402409182,\n",
       "  0.024091275889826402,\n",
       "  4,\n",
       "  0.5,\n",
       "  5.0222843614422334,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.36091877271985073,\n",
       "  0.071112586520930221,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  13.625221488283309,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.97401428344355889,\n",
       "  0.069138778475265061,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  17.712506546725887,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.94799857687631039,\n",
       "  0.17410549105527628,\n",
       "  1,\n",
       "  0.05,\n",
       "  45.10883052491207,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.65856598980120495,\n",
       "  0.046256867732290874,\n",
       "  3,\n",
       "  0.375,\n",
       "  7.821899377364986,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.57715506570800179,\n",
       "  0.05094159487690153,\n",
       "  4,\n",
       "  0.25,\n",
       "  35.98640164529915,\n",
       "  1,\n",
       "  2,\n",
       "  0.11764705882352941,\n",
       "  0],\n",
       " [0.41635721810634729,\n",
       "  0.14599548256258577,\n",
       "  2,\n",
       "  0.25,\n",
       "  18.502492373274354,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.31352094727064495,\n",
       "  0.035809309888877983,\n",
       "  3,\n",
       "  0.25,\n",
       "  9.796030847272167,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.64757566264273847,\n",
       "  0.013991124477972794,\n",
       "  5,\n",
       "  0.625,\n",
       "  9.789488047538514,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.86039644207305077,\n",
       "  0.043807914569614637,\n",
       "  5,\n",
       "  0.3125,\n",
       "  11.53085446638335,\n",
       "  0,\n",
       "  1,\n",
       "  0.05555555555555555,\n",
       "  0],\n",
       " [0.78796030476764978,\n",
       "  0.030150967981578791,\n",
       "  4,\n",
       "  0.23529411764705882,\n",
       "  30.766384703122455,\n",
       "  0,\n",
       "  2,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.29670128145955699,\n",
       "  0.075077039694291692,\n",
       "  4,\n",
       "  0.5,\n",
       "  18.686789455909576,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.66735498774758628,\n",
       "  0.029533860946379886,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  9.158739597127772,\n",
       "  0,\n",
       "  3,\n",
       "  0.6,\n",
       "  1],\n",
       " [0.40823898818762816,\n",
       "  0.10432632319422697,\n",
       "  4,\n",
       "  0.4,\n",
       "  13.985083341751277,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.82862794270018725,\n",
       "  0.15798409150199244,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  27.952423390977188,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.63940380846708611,\n",
       "  0.10494273290378064,\n",
       "  4,\n",
       "  0.5,\n",
       "  9.263090843485815,\n",
       "  1,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0527856693099233,\n",
       "  0.039804241164016263,\n",
       "  3,\n",
       "  0.15789473684210525,\n",
       "  11.932528327197687,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1816106595820031,\n",
       "  0.11500859417602838,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  15.865032522085142,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.23063858786369329,\n",
       "  0.015937637006208982,\n",
       "  7,\n",
       "  0.7,\n",
       "  7.276399231570501,\n",
       "  1,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [0.72413006819468839,\n",
       "  0.085351001280951855,\n",
       "  2,\n",
       "  0.25,\n",
       "  16.593199912433498,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.31203833150133797,\n",
       "  0.098052166827884646,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  17.028665029541344,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.065220181822552292,\n",
       "  0.051818281620077622,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  23.501046296386566,\n",
       "  0,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [0.15695582568217903,\n",
       "  0.29313321328911035,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  19.29284709547443,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  1],\n",
       " [0.47782898245495908,\n",
       "  0.010542293235860756,\n",
       "  7,\n",
       "  0.7,\n",
       "  5.567675547357108,\n",
       "  0,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  0],\n",
       " [0.38637535231450981,\n",
       "  0.46097581979954105,\n",
       "  2,\n",
       "  0.4,\n",
       "  10.841911599586531,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1370931676700116,\n",
       "  0.15699187575691687,\n",
       "  2,\n",
       "  0.1111111111111111,\n",
       "  31.684248326747234,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  1],\n",
       " [0.71598684137046831,\n",
       "  0.072205838252320187,\n",
       "  3,\n",
       "  0.25,\n",
       "  14.046350610682861,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.71754278412639327,\n",
       "  0.04069096513615722,\n",
       "  5,\n",
       "  0.2777777777777778,\n",
       "  20.279619024960756,\n",
       "  0,\n",
       "  1,\n",
       "  0.047619047619047616,\n",
       "  0],\n",
       " [0.84295464623643679,\n",
       "  0.020128340175610826,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  5.750431054979175,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.48881857454755873,\n",
       "  0.064989171817768754,\n",
       "  3,\n",
       "  0.25,\n",
       "  16.24460724919516,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.92602316837276555,\n",
       "  0.078610764162119984,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  23.978188496660533,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.101938960074174,\n",
       "  0.04900907217291206,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  19.733265768499702,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.0357141302489299,\n",
       "  0.078298670747833699,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  15.826343114997922,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [1.3388191485829612,\n",
       "  0.1126589342621912,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  18.650382317932326,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.33749907281050451,\n",
       "  0.030792732959784908,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  6.884267767277374,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.64790786664102007,\n",
       "  0.10889848529060675,\n",
       "  3,\n",
       "  0.375,\n",
       "  10.44543667135994,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.50810104706826398,\n",
       "  0.43595294606232415,\n",
       "  3,\n",
       "  0.3,\n",
       "  23.053686541412414,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.81451655518664812,\n",
       "  0.064680814078308502,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  20.43512091889686,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.65889999931731613,\n",
       "  0.086661543705074839,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  19.737644316183943,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.60167586746466828,\n",
       "  0.038102121345106821,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  6.190615224291897,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [1.0044373372757451,\n",
       "  0.17173955712334998,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  32.61631747744239,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.81885888995821809,\n",
       "  0.047507355029311915,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  21.38163465544451,\n",
       "  1,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  0],\n",
       " [0.11995023030254115,\n",
       "  0.066719293352794584,\n",
       "  3,\n",
       "  0.375,\n",
       "  10.388573692355347,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [1.0243638563802608,\n",
       "  0.12060912273199154,\n",
       "  3,\n",
       "  0.3,\n",
       "  17.339534004186767,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0041802589754714,\n",
       "  0.15183177607882747,\n",
       "  3,\n",
       "  0.2,\n",
       "  22.86249411851482,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.62957193115556809,\n",
       "  0.045242846095631672,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  10.892972391614794,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.61513915582954604,\n",
       "  0.013593535299077586,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  7.696275412638425,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.5167386869994961,\n",
       "  0.11816290062340928,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  12.351015899612122,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.62852965291852603,\n",
       "  0.058883163722039389,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  9.539821788721474,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.5869095837010867,\n",
       "  0.034109494673097318,\n",
       "  5,\n",
       "  0.625,\n",
       "  7.496621257305912,\n",
       "  1,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.58993557232733318,\n",
       "  0.12492431799737813,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  17.500152843780878,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1033601386630658,\n",
       "  0.27748746610060493,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  15.830306973364392,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.25117531269236448,\n",
       "  0.0019601838279951211,\n",
       "  8,\n",
       "  1.0,\n",
       "  5.299874923497157,\n",
       "  0,\n",
       "  7,\n",
       "  0.875,\n",
       "  0],\n",
       " [0.41744814994853163,\n",
       "  0.10794600068260651,\n",
       "  4,\n",
       "  0.4,\n",
       "  28.695599587723933,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  1],\n",
       " [0.99098110081005997,\n",
       "  0.034596449275088759,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  40.57136215913177,\n",
       "  0,\n",
       "  1,\n",
       "  0.047619047619047616,\n",
       "  0],\n",
       " [1.1515609437424785,\n",
       "  0.037900649913039652,\n",
       "  2,\n",
       "  0.2,\n",
       "  7.077427142805286,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.93969258007176104,\n",
       "  0.033638156789416618,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  9.021078790965673,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.71683612724158707,\n",
       "  0.17397882714099133,\n",
       "  2,\n",
       "  0.25,\n",
       "  10.75908928065226,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0650774332790096,\n",
       "  0.049708535587878355,\n",
       "  5,\n",
       "  0.25,\n",
       "  27.318667321667395,\n",
       "  0,\n",
       "  1,\n",
       "  0.043478260869565216,\n",
       "  1],\n",
       " [0.72336952065860416,\n",
       "  0.11742272589364222,\n",
       "  3,\n",
       "  0.25,\n",
       "  23.397336468686994,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.50067783562345181,\n",
       "  0.15560732368000296,\n",
       "  3,\n",
       "  0.375,\n",
       "  10.36984722786742,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  1],\n",
       " [1.0948105537401573,\n",
       "  0.015274622263768078,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  8.79876612293226,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.18937714782146442,\n",
       "  0.0082882278564305745,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  4.069926145306188,\n",
       "  1,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  0],\n",
       " [0.34486200700131553,\n",
       "  0.023922737805012195,\n",
       "  4,\n",
       "  0.5,\n",
       "  5.351055413691242,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.34114986163162619,\n",
       "  0.093025232430440918,\n",
       "  3,\n",
       "  0.25,\n",
       "  18.603035910434617,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.11267867161274325,\n",
       "  0.023208773816079109,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  13.207039575063678,\n",
       "  1,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.29204348695979798,\n",
       "  0.14443440026066767,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  16.032623373452125,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.45007183928688144,\n",
       "  0.04148184359092133,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  16.024433382284727,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.57071776511306194,\n",
       "  0.2213210607907844,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  8.876610285948505,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.45568980338780896,\n",
       "  0.02620838595108621,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  17.768249306223122,\n",
       "  0,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  0],\n",
       " [1.1282808590543414,\n",
       "  0.059317488320435152,\n",
       "  4,\n",
       "  0.2,\n",
       "  21.567217882713106,\n",
       "  0,\n",
       "  3,\n",
       "  0.15,\n",
       "  0],\n",
       " [1.1089626019942018,\n",
       "  0.05632083241050545,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  9.485008701494383,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.9126973418503932,\n",
       "  0.077555764876537703,\n",
       "  3,\n",
       "  0.15789473684210525,\n",
       "  21.293545590609522,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53424814915361252,\n",
       "  0.060184789095535374,\n",
       "  5,\n",
       "  0.5,\n",
       "  9.912811416540702,\n",
       "  1,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.84699740235911336,\n",
       "  0.22540041562294055,\n",
       "  3,\n",
       "  0.25,\n",
       "  26.22514161791072,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.24155661233487347,\n",
       "  0.054289349450585833,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  8.16814894974922,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.30527879550465753,\n",
       "  0.079611695125741511,\n",
       "  3,\n",
       "  0.375,\n",
       "  8.86147512308184,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.61216429865716981,\n",
       "  0.18872764184355029,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  16.106948699438597,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.52519286105642204,\n",
       "  0.15807380780765179,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  14.301035445676643,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.69929985139917095,\n",
       "  0.053711751385957518,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  13.601777689311541,\n",
       "  1,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [0.2753769970908847,\n",
       "  0.084139869438834891,\n",
       "  5,\n",
       "  0.5,\n",
       "  13.350023201528604,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.55688108158216409,\n",
       "  0.11093036184261884,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  16.567963148528154,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.3242684670900564,\n",
       "  0.0545610683446065,\n",
       "  1,\n",
       "  0.1,\n",
       "  7.691649944924641,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0930803133004348,\n",
       "  0.06748663183037662,\n",
       "  3,\n",
       "  0.375,\n",
       "  8.789517101277566,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.24085608493444444,\n",
       "  0.021631296058166827,\n",
       "  5,\n",
       "  0.625,\n",
       "  5.2364253488250645,\n",
       "  1,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.56936357468315302,\n",
       "  0.12722453949488688,\n",
       "  3,\n",
       "  0.1875,\n",
       "  30.345939491862065,\n",
       "  0,\n",
       "  1,\n",
       "  0.05555555555555555,\n",
       "  0],\n",
       " [0.61873494059225109,\n",
       "  0.14598897996511984,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  30.931493572098795,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.65637990349374387,\n",
       "  0.15045046153585373,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.114244519647809,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.381534090604844,\n",
       "  0.044343931655655755,\n",
       "  4,\n",
       "  0.5,\n",
       "  6.4159763715394,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.40281413710504199,\n",
       "  0.11046043461297339,\n",
       "  4,\n",
       "  0.5,\n",
       "  9.805472724176068,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.93631930967863697,\n",
       "  0.10290474099122504,\n",
       "  0,\n",
       "  0.0,\n",
       "  35.06540409548725,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.2217457715328881,\n",
       "  0.23637023165541327,\n",
       "  2,\n",
       "  0.5,\n",
       "  6.3473265156739025,\n",
       "  0,\n",
       "  1,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.25787671704357829,\n",
       "  0.10143380560311566,\n",
       "  2,\n",
       "  0.5,\n",
       "  4.4546394967219705,\n",
       "  0,\n",
       "  1,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.87603026613237367,\n",
       "  0.079716792928935187,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.210215213332202,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  1],\n",
       " [0.92902827355588224,\n",
       "  0.060669094367798659,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  15.592117627308644,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.31243318038758483,\n",
       "  0.033572933715332764,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  8.802220294974779,\n",
       "  0,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.75808983191498569,\n",
       "  0.10949901019136543,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  24.790148285055007,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.0629181119937621,\n",
       "  0.076622322947468513,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  15.403583602785172,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.47924498222073619,\n",
       "  0.031210636208182874,\n",
       "  3,\n",
       "  0.3,\n",
       "  7.166242935148823,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.55428202122209758,\n",
       "  0.040542519937109334,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  8.344969087806762,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  1],\n",
       " [1.1857302744625418,\n",
       "  0.0623649284550718,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  7.2425256733474725,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.86095900082003507,\n",
       "  0.23356722466507129,\n",
       "  2,\n",
       "  0.2,\n",
       "  27.75543812180075,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.45279020134166759,\n",
       "  0.070440080554654338,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  9.372573641697628,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [1.1357330131419316,\n",
       "  0.30773239404809649,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  9.94477071096383,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.74292691168201253,\n",
       "  0.065807129929569164,\n",
       "  4,\n",
       "  0.4,\n",
       "  13.041642341222728,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.88786953189202533,\n",
       "  0.22507594320189328,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  15.492079752587296,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.39327532867612003,\n",
       "  0.13005089407731907,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  17.563884588213494,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2966351864841257,\n",
       "  0.076021289742871478,\n",
       "  3,\n",
       "  0.2,\n",
       "  20.32438431905636,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [1.1953676073550563,\n",
       "  0.050100026894925986,\n",
       "  3,\n",
       "  0.25,\n",
       "  19.88432314792413,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1164213716140461,\n",
       "  0.43674896338393376,\n",
       "  1,\n",
       "  0.25,\n",
       "  8.67008988450865,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0552050611525914,\n",
       "  0.084396651433569114,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  13.372824843175435,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.3774431370273279,\n",
       "  0.18711118141970773,\n",
       "  1,\n",
       "  0.125,\n",
       "  16.841669093360537,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.9944723566119138,\n",
       "  0.052908064917090436,\n",
       "  3,\n",
       "  0.375,\n",
       "  7.807077090229671,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.49342822814721587,\n",
       "  0.065513112831589337,\n",
       "  2,\n",
       "  0.25,\n",
       "  11.268779238369705,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0359340818364182,\n",
       "  0.10534499779721418,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  23.211005627326582,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.82239212869628786,\n",
       "  0.065915523835624046,\n",
       "  3,\n",
       "  0.375,\n",
       "  7.863966601804828,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.74767522589341628,\n",
       "  0.51189405720723113,\n",
       "  1,\n",
       "  0.125,\n",
       "  10.173128872228027,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.83889556880912342,\n",
       "  0.056674375513579456,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  15.271595291235633,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.58837375454732044,\n",
       "  0.040034508697401883,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  8.08677094273002,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.1761955674878513,\n",
       "  0.18228679914443768,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  13.605089543035215,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.14887880251958996,\n",
       "  0.035049796546367973,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  5.896266190074467,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.2175909454226124,\n",
       "  0.027937862135151481,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  8.45084010592047,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.7086352110078582,\n",
       "  0.05451058549022314,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.970752019440424,\n",
       "  0,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.65590129593124913,\n",
       "  0.10550553245212735,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  8.13209845726038,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.29888217823064678,\n",
       "  0.085442360575191945,\n",
       "  3,\n",
       "  0.375,\n",
       "  10.435572665865424,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.366203985582081,\n",
       "  0.18494877884567329,\n",
       "  1,\n",
       "  0.1,\n",
       "  25.435286137600333,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.40552528461830595,\n",
       "  0.071518140506122041,\n",
       "  2,\n",
       "  0.2,\n",
       "  12.166843334393713,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.49814773114230371,\n",
       "  0.054163954168254924,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  10.202504578932839,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.70025549247493313,\n",
       "  0.11343548082641242,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  12.282316818012594,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80395174712834416,\n",
       "  0.11664631880592113,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  11.707074096098568,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.97048722380937469,\n",
       "  1.06086748160768,\n",
       "  0,\n",
       "  0.0,\n",
       "  45.016183282270596,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.71819626034628836,\n",
       "  0.084603661419850873,\n",
       "  4,\n",
       "  0.4,\n",
       "  18.470799385790972,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.3289331812348335,\n",
       "  0.12320300602271106,\n",
       "  2,\n",
       "  0.25,\n",
       "  10.608705972467027,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [1.1577045982853886,\n",
       "  0.14041491562715536,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  17.90853206037882,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.95300855879240853,\n",
       "  0.045575171692189831,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  14.424845582364075,\n",
       "  0,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [0.013470544108197147,\n",
       "  0.037332099763994431,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  6.184194867510726,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.45654723463617197,\n",
       "  0.20068511697174207,\n",
       "  3,\n",
       "  0.375,\n",
       "  28.77670621891187,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1154015750489978,\n",
       "  0.18106379548075202,\n",
       "  2,\n",
       "  0.25,\n",
       "  11.75224110106285,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [1.2628507925383163,\n",
       "  0.031224302615179056,\n",
       "  3,\n",
       "  0.25,\n",
       "  9.202569422667139,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.63651050101699247,\n",
       "  0.05338411538588439,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  9.258188714799937,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1240578057629864,\n",
       "  0.055112637098962636,\n",
       "  3,\n",
       "  0.25,\n",
       "  12.227430935349567,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0589826134847926,\n",
       "  0.20171953196924297,\n",
       "  2,\n",
       "  0.2,\n",
       "  19.391379206070333,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.35247994377529224,\n",
       "  0.021977341268261785,\n",
       "  3,\n",
       "  0.75,\n",
       "  4.532661995576329,\n",
       "  0,\n",
       "  1,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.25705324967253,\n",
       "  0.11818565893950383,\n",
       "  1,\n",
       "  0.1,\n",
       "  9.129408224654794,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.75255867695265144,\n",
       "  0.033567625842604865,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  31.920213140126116,\n",
       "  0,\n",
       "  4,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.88779407030346491,\n",
       "  0.077780986662255747,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  19.101678752155564,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.67131613057973549,\n",
       "  0.053668354666580154,\n",
       "  4,\n",
       "  0.5,\n",
       "  7.933521340851013,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.1639550330180419,\n",
       "  0.18479590312306804,\n",
       "  3,\n",
       "  0.3,\n",
       "  19.761761345914497,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.49501497919284032,\n",
       "  0.14159196270221575,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  15.181475331747786,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.49842456802511759,\n",
       "  0.10649634772931438,\n",
       "  4,\n",
       "  0.4,\n",
       "  31.5556462032301,\n",
       "  0,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  1],\n",
       " [1.2562079094601013,\n",
       "  0.16192044296928809,\n",
       "  1,\n",
       "  0.125,\n",
       "  16.278603102264338,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.61316644464620862,\n",
       "  0.0077144698102157294,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  3.04142625976909,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.10557286154932832,\n",
       "  0.087416087395094544,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  15.426734901930752,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.56651480539503574,\n",
       "  0.090789174189998212,\n",
       "  5,\n",
       "  0.625,\n",
       "  14.347663081415048,\n",
       "  0,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.58179386715880665,\n",
       "  0.078319285834173469,\n",
       "  2,\n",
       "  0.4,\n",
       "  6.7673138454997614,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53731156678847691,\n",
       "  0.22905520814723723,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  19.237741574600932,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.65174004333238855,\n",
       "  0.10134998831635145,\n",
       "  3,\n",
       "  0.25,\n",
       "  26.140803859524027,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.71465440288703697,\n",
       "  0.095612506593822477,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  15.915261899485984,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.4832204645368755,\n",
       "  0.056427074490213447,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.409410927067922,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0741188905290313,\n",
       "  0.054455639340287409,\n",
       "  3,\n",
       "  0.2,\n",
       "  27.83673416006264,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.45475914329288047,\n",
       "  0.13145604615757089,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.2875728407528,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.84383963273638318,\n",
       "  0.077499603522623417,\n",
       "  1,\n",
       "  0.1,\n",
       "  15.353500996277107,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.99983249698043219,\n",
       "  0.068042623378145151,\n",
       "  1,\n",
       "  0.125,\n",
       "  9.419269579352301,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.33887859585877711,\n",
       "  0.044733596626228,\n",
       "  2,\n",
       "  0.5,\n",
       "  3.123865908807025,\n",
       "  0,\n",
       "  1,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.92245535763873354,\n",
       "  0.054180234810352834,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  16.441805086715227,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.18218650795397739,\n",
       "  0.079974517202916151,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  6.450191080777585,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [1.0839228365155129,\n",
       "  0.041378459922157051,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  9.934797463466934,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.61483371288255095,\n",
       "  0.020612029144156607,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  18.264675073450572,\n",
       "  0,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [1.2684109860398287,\n",
       "  0.05422411973837038,\n",
       "  3,\n",
       "  0.2,\n",
       "  25.730637682892645,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.90470093196851376,\n",
       "  0.05292103488507538,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  8.820520130711923,\n",
       "  0,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [1.1808131415590228,\n",
       "  0.17196101899086091,\n",
       "  1,\n",
       "  0.125,\n",
       "  15.802088339013366,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.76098930269052878,\n",
       "  0.17150767340392603,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  11.636782163034866,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.37072641773420612,\n",
       "  0.022631901294019263,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  7.488671777245943,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.72155388908078311,\n",
       "  0.10092322016342681,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.84402308695239,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.79499930648124328,\n",
       "  0.027526374068191362,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  12.385279585545494,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.61307445875395605,\n",
       "  0.02396765128601086,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  12.16252802692082,\n",
       "  0,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.0056475708277531345,\n",
       "  0.026281837428064669,\n",
       "  4,\n",
       "  0.8,\n",
       "  4.46174579063039,\n",
       "  0,\n",
       "  2,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.47880489774837443,\n",
       "  0.072788720412996333,\n",
       "  3,\n",
       "  0.375,\n",
       "  11.259011751294823,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.45634935460601977,\n",
       "  0.28633936565455131,\n",
       "  3,\n",
       "  0.375,\n",
       "  10.32341553429793,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.15123366711807706,\n",
       "  0.17063739098183417,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  8.145278872950826,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.18780566614030636,\n",
       "  0.1488799774915065,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  9.026827675835404,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.93936190408277187,\n",
       "  0.073838178225405948,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  24.278330171156036,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.26764308655646996,\n",
       "  0.10767698281225713,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  7.3853741064055365,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.6567076433772614,\n",
       "  0.047924824170502478,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  11.356868578334073,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  1],\n",
       " [0.6873652259126537,\n",
       "  0.070577902421547045,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  12.031164020705404,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.84630200663023925,\n",
       "  0.17964799123472686,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  8.903912121115892,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.91243798724058178,\n",
       "  0.15926329291635755,\n",
       "  2,\n",
       "  0.09523809523809523,\n",
       "  58.00334132204124,\n",
       "  0,\n",
       "  1,\n",
       "  0.047619047619047616,\n",
       "  0],\n",
       " [0.43140585568288814,\n",
       "  0.01462617265265953,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  13.715549620639234,\n",
       "  0,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.22247985967591655,\n",
       "  0.055374447794869663,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  11.066544660235422,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.1190674262238383,\n",
       "  0.047596741637575324,\n",
       "  2,\n",
       "  0.125,\n",
       "  13.90655181579884,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.00739643790110045,\n",
       "  0.017296846769619845,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  4.341959857641247,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.71408691060764884,\n",
       "  0.08431395589509405,\n",
       "  3,\n",
       "  0.5,\n",
       "  7.158666732883686,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.0255944557546139,\n",
       "  0.038917002836005632,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  7.46415181458872,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.15825792368262692,\n",
       "  0.063145434088883401,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  8.50293163018976,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0423812697990682,\n",
       "  0.11233912770668864,\n",
       "  1,\n",
       "  0.1,\n",
       "  10.199560318372566,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2410551319859375,\n",
       "  0.10730659958285282,\n",
       "  3,\n",
       "  0.2,\n",
       "  46.799088311590786,\n",
       "  0,\n",
       "  1,\n",
       "  0.05263157894736842,\n",
       "  0],\n",
       " [0.66480065771727903,\n",
       "  0.12188062692810064,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  27.177041802796296,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  1],\n",
       " [0.28216059595857035,\n",
       "  0.041850654677847654,\n",
       "  3,\n",
       "  0.6,\n",
       "  6.940609057262698,\n",
       "  0,\n",
       "  1,\n",
       "  0.2,\n",
       "  1],\n",
       " [0.28058651071104967,\n",
       "  0.032934750351735764,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  9.547913615167936,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.15812159237216683,\n",
       "  0.1385625217703168,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  14.277195475306797,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.89387981764175561,\n",
       "  0.041687275258748935,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  7.96397443615107,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53885539679823458,\n",
       "  0.062738341733924563,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  11.52707261587429,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.70624858372364385,\n",
       "  0.029210983183194039,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  10.308853717317875,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.70846007647929266,\n",
       "  0.025007922713568553,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  20.573570798646838,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0615351451759065,\n",
       "  0.098275528535781587,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  17.380831044288264,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.46126183975706281,\n",
       "  0.01786344289100994,\n",
       "  5,\n",
       "  0.625,\n",
       "  6.040507059680756,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [1.1278417802247735,\n",
       "  0.11258779193745994,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  12.628967483929463,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.55387197267866295,\n",
       "  0.2186183035369671,\n",
       "  3,\n",
       "  0.3,\n",
       "  20.12739926618846,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.47182059597200521,\n",
       "  0.048747067153105728,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.898153738504945,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.27911109387811717,\n",
       "  0.040019481738621243,\n",
       "  4,\n",
       "  0.5,\n",
       "  4.505922646595093,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  1],\n",
       " [0.013010990101809505,\n",
       "  0.045394318585726956,\n",
       "  3,\n",
       "  0.5,\n",
       "  5.392308747732918,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.0016118005103753,\n",
       "  0.10286069420834631,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  17.041137222168075,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3661485969325149,\n",
       "  0.14088652907183774,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  17.831905324017818,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.65910762099464637,\n",
       "  0.068440555052569629,\n",
       "  3,\n",
       "  0.5,\n",
       "  12.990528219802265,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0452985740393304,\n",
       "  0.035548703958598438,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  10.402834741486652,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.0679019849783886,\n",
       "  0.04112109811290332,\n",
       "  2,\n",
       "  0.25,\n",
       "  11.557741434032383,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.75700407944862991,\n",
       "  0.11589846692721439,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  45.33318928815148,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.64214702529967782,\n",
       "  0.030564807698529983,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  26.164693240164645,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1078287654204775,\n",
       "  0.11873425532615922,\n",
       "  2,\n",
       "  0.2,\n",
       "  13.93055485329489,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.96801742844471317,\n",
       "  0.17320532514235087,\n",
       "  2,\n",
       "  0.2,\n",
       "  8.192681757382713,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  1],\n",
       " [0.3706911342225867,\n",
       "  0.088526341391762742,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  7.561945483848497,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.89665338756980673,\n",
       "  0.040084939094687688,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  20.529226200688168,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.33213976285964697,\n",
       "  0.10140502402568186,\n",
       "  2,\n",
       "  0.25,\n",
       "  8.783312608708078,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.85016643062453356,\n",
       "  0.044625651625485818,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  25.67362043684433,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.2144401697494055,\n",
       "  0.0043997983277005437,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  2.1600680046304097,\n",
       "  0,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [1.0937463335974329,\n",
       "  0.087000801571581743,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  19.43413917234074,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.45548322962768117,\n",
       "  0.071020120157018418,\n",
       "  2,\n",
       "  0.2,\n",
       "  9.83310867557338,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.90255382772998649,\n",
       "  0.056787489588937756,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  17.932136668530536,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.1327088180374323,\n",
       "  0.038581136392303539,\n",
       "  4,\n",
       "  0.2222222222222222,\n",
       "  13.531971566844907,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2834624823618701,\n",
       "  0.097800495635860352,\n",
       "  2,\n",
       "  0.25,\n",
       "  20.483812855393467,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0205317830287834,\n",
       "  0.045397838787677003,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  7.548910654746404,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0654895747734385,\n",
       "  0.054115608969330764,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  27.40762015796436,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.57424039426980622,\n",
       "  0.23294429082928891,\n",
       "  3,\n",
       "  0.5,\n",
       "  8.707364474059641,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.93931041110469504,\n",
       "  0.066484155135215661,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  9.63340596093451,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.7219378246466337,\n",
       "  0.03823951856666441,\n",
       "  6,\n",
       "  0.42857142857142855,\n",
       "  14.240135780200214,\n",
       "  0,\n",
       "  3,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.41799647423532904,\n",
       "  0.065638799417606752,\n",
       "  3,\n",
       "  0.6,\n",
       "  6.117606374101624,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.85000850164613884,\n",
       "  0.24143375515110521,\n",
       "  2,\n",
       "  0.125,\n",
       "  18.944644743740678,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.35094626235934478,\n",
       "  0.095357723606548483,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  10.234114997772075,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.71652392137671805,\n",
       "  0.12570251205389293,\n",
       "  6,\n",
       "  0.3333333333333333,\n",
       "  23.24272890248124,\n",
       "  0,\n",
       "  3,\n",
       "  0.15,\n",
       "  0],\n",
       " [1.2226718922811823,\n",
       "  0.080635504847446859,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  17.980305820682275,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1589627273352312,\n",
       "  0.067130964684330441,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  11.578698117889067,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87768017934681164,\n",
       "  0.087388073445601622,\n",
       "  2,\n",
       "  0.25,\n",
       "  21.861413257611495,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.84723793678896275,\n",
       "  0.15591332967588578,\n",
       "  2,\n",
       "  0.25,\n",
       "  9.570852245580243,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.63627843300575671,\n",
       "  0.045124503359701085,\n",
       "  4,\n",
       "  0.5,\n",
       "  9.372308865345296,\n",
       "  0,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  1],\n",
       " [0.70151086858179901,\n",
       "  0.076529513664267301,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  7.246653071109953,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.85416456887400372,\n",
       "  0.050056616141703869,\n",
       "  4,\n",
       "  0.25,\n",
       "  27.01868874445579,\n",
       "  0,\n",
       "  2,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.37504192877690523,\n",
       "  0.073816834737867265,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  8.497913576511872,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [1.2042310205052176,\n",
       "  0.15798142152187589,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  20.577266709672582,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.56679598153474209,\n",
       "  0.075309539191430797,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  13.630096709367171,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0774093148612875,\n",
       "  0.10322033390872654,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  15.743006015587058,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.18516759796141835,\n",
       "  0.042083629296063463,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  7.8219712960512044,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.83390870842391918,\n",
       "  0.06839091467477576,\n",
       "  6,\n",
       "  0.4,\n",
       "  14.09851055975512,\n",
       "  0,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [0.3775128246985574,\n",
       "  0.031872902002611392,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.5032419038169,\n",
       "  0,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.81451785381966291,\n",
       "  0.067962734656186852,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  18.19981256091048,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  1],\n",
       " [0.61461293648442461,\n",
       "  0.16281293531324337,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  8.293807580404103,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.48640420130485629,\n",
       "  0.027327780153666215,\n",
       "  5,\n",
       "  0.5,\n",
       "  8.615387122341538,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.85104109435442243,\n",
       "  0.062762491557767919,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  26.9041260320707,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.95321882853465933,\n",
       "  0.04317146902415725,\n",
       "  6,\n",
       "  0.25,\n",
       "  19.92708015216965,\n",
       "  0,\n",
       "  1,\n",
       "  0.03333333333333333,\n",
       "  0],\n",
       " [0.33001924487857914,\n",
       "  0.03813555935238766,\n",
       "  5,\n",
       "  0.625,\n",
       "  13.533211511940136,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.89105578797084739,\n",
       "  0.11949359358085609,\n",
       "  3,\n",
       "  0.25,\n",
       "  17.04069187184151,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.98165485307034961,\n",
       "  0.021311665693957704,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  7.322851506032508,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [1.3159174816243824,\n",
       "  0.20816983383607157,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  16.643498328173266,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.52196638303816889,\n",
       "  0.061049068770633874,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  12.641882127803186,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.78899884850283941,\n",
       "  0.090058627679534653,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  11.466158924312472,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.53783231779600582,\n",
       "  0.074101251236698085,\n",
       "  2,\n",
       "  0.2,\n",
       "  14.243171373009133,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.83865340657275667,\n",
       "  0.55995591496384334,\n",
       "  2,\n",
       "  0.25,\n",
       "  14.962419460443545,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.75337821135226624,\n",
       "  0.080906554818939935,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  9.115829975367495,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.29972907951829197,\n",
       "  0.10106076015350463,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  18.5855663093268,\n",
       "  0,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [0.35549664939834269,\n",
       "  0.019608700896884379,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  6.6533491149753035,\n",
       "  0,\n",
       "  4,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.39864974235159956,\n",
       "  0.15018488809077868,\n",
       "  2,\n",
       "  0.2,\n",
       "  19.376773085976133,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.66238661190246151,\n",
       "  0.095849330599124261,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  10.666002051907562,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.28412917547514061,\n",
       "  0.038702327848149531,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  11.277439037243031,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.76945098322453331,\n",
       "  0.047743904365405876,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  9.94312065502518,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.3988162182720385,\n",
       "  0.040938417956757567,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  16.21702658194741,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.4505108208389238,\n",
       "  0.16447633020239827,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  16.826550992616294,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.1445851010542594,\n",
       "  0.04003083147852704,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  11.073570664656744,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.79984909183399977,\n",
       "  0.017239542253350892,\n",
       "  4,\n",
       "  0.4,\n",
       "  17.381823434179303,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.7811150306716993,\n",
       "  0.015104781377899279,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  5.509840819844202,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.59770384150340383,\n",
       "  0.094996040566864726,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  31.843064809816557,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.92198707959944604,\n",
       "  0.058941335376805681,\n",
       "  3,\n",
       "  0.3,\n",
       "  9.436205947045256,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.2636872364694489,\n",
       "  0.12039721802517733,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  15.858124534843787,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3238651608477092,\n",
       "  0.22050695584972446,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  24.899522505931532,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.47059635131647515,\n",
       "  0.081722285832898756,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  22.312589173725822,\n",
       "  0,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  1],\n",
       " [0.76863958438490598,\n",
       "  0.040545335143146066,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  8.190599425800398,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.33191813023014716,\n",
       "  0.057639167671182712,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  32.49168593631927,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80754745689524121,\n",
       "  0.021404800409326574,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  27.132371978798897,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.71626169384876281,\n",
       "  0.041918456094315548,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.592029939611715,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.91847556738645253,\n",
       "  0.07541913223897867,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.91467448323468,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.0539108534066548,\n",
       "  0.076009077072988096,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  9.304949131273078,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.0753830948020866,\n",
       "  0.19377452314387866,\n",
       "  1,\n",
       "  0.045454545454545456,\n",
       "  53.706775189077796,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1165056177681856,\n",
       "  0.32965394105697399,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  12.16496198308174,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.7827970441554153,\n",
       "  0.15743872711805107,\n",
       "  2,\n",
       "  0.2,\n",
       "  19.6566943649408,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69945868110995257,\n",
       "  0.02795899236948951,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  9.989549614073784,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.3728427891082563,\n",
       "  0.066640419723305144,\n",
       "  4,\n",
       "  0.5,\n",
       "  9.839683555159258,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.68556913475169379,\n",
       "  0.064044054077520784,\n",
       "  4,\n",
       "  0.23529411764705882,\n",
       "  18.51353552967444,\n",
       "  0,\n",
       "  1,\n",
       "  0.05263157894736842,\n",
       "  0],\n",
       " [0.96361159262413709,\n",
       "  0.071699231744910885,\n",
       "  3,\n",
       "  0.5,\n",
       "  8.809386640415344,\n",
       "  0,\n",
       "  2,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.923517710801671,\n",
       "  0.12250477570303009,\n",
       "  4,\n",
       "  0.4,\n",
       "  20.42011627597766,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.67751629720193574,\n",
       "  0.046997612662575183,\n",
       "  3,\n",
       "  0.3,\n",
       "  11.212683194827301,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.79844636651608814,\n",
       "  0.0087134868514879837,\n",
       "  5,\n",
       "  0.5,\n",
       "  5.795065455447854,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.32238752337494936,\n",
       "  0.033855217647947189,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  12.547087407898362,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1147718244339846,\n",
       "  0.08599756064905828,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  14.9033838783978,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.55900144786526451,\n",
       "  0.019233323707508965,\n",
       "  5,\n",
       "  0.5,\n",
       "  7.531244241912796,\n",
       "  0,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.078785154269481628,\n",
       "  0.089186439436640064,\n",
       "  3,\n",
       "  0.5,\n",
       "  9.806287357335599,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.22175232697742964,\n",
       "  0.026535834811402759,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  7.288053133297055,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [1.1277664239292819,\n",
       "  0.1109416236784424,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  15.132389936153448,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.89551494428149103,\n",
       "  0.0076948578016348135,\n",
       "  6,\n",
       "  0.6,\n",
       "  5.724804370961747,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.5216129117372692,\n",
       "  0.0075032291904382209,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  7.548589290728834,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [1.0609780657178387,\n",
       "  0.059882698466461548,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  18.81869543828127,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3389412872630393,\n",
       "  0.096141877464292369,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  13.950610942696548,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.95885273234098656,\n",
       "  0.10789460316142996,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  11.103922093272358,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1512440376821886,\n",
       "  0.088147811621015593,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  9.373644639365224,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.092713625900769703,\n",
       "  0.021808123086327069,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  8.128581345567557,\n",
       "  0,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [0.13528229325459273,\n",
       "  0.10740911385100949,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  9.62152638191789,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.3134635940671574,\n",
       "  0.026799086766379987,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.998837690107795,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.0157660078314286,\n",
       "  0.1952813096896403,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  56.417550962396156,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.93407545967634509,\n",
       "  0.15830625914020602,\n",
       "  4,\n",
       "  0.26666666666666666,\n",
       "  32.71731194381601,\n",
       "  0,\n",
       "  2,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.37720290488355013,\n",
       "  0.10210273759557831,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  11.161638699972082,\n",
       "  0,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [1.005978602869595,\n",
       "  0.062393142250256828,\n",
       "  3,\n",
       "  0.2,\n",
       "  18.540572819479767,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [1.0579000527305018,\n",
       "  0.05850455020389167,\n",
       "  4,\n",
       "  0.18181818181818182,\n",
       "  38.69064551337997,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.9395763144098519,\n",
       "  0.074779746733675045,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  8.768115737515297,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.004375528719604671,\n",
       "  0.064566569880535107,\n",
       "  3,\n",
       "  0.5,\n",
       "  5.971799986785322,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.37644660723283307,\n",
       "  0.094094004059370184,\n",
       "  3,\n",
       "  0.375,\n",
       "  16.357326124668266,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.95414057586995538,\n",
       "  0.12042510767853742,\n",
       "  2,\n",
       "  0.2,\n",
       "  13.196907596138278,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.46950669359692476,\n",
       "  0.0088409092906933662,\n",
       "  7,\n",
       "  0.7,\n",
       "  6.084601093399157,\n",
       "  1,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  0],\n",
       " [1.3045908664650181,\n",
       "  0.51636899965789074,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  21.696002021707553,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.65351179715490471,\n",
       "  0.11732609712321507,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  12.848448413066645,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2402145685407808,\n",
       "  0.097530989175097837,\n",
       "  2,\n",
       "  0.25,\n",
       "  13.848818310593343,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.70918122754169155,\n",
       "  0.2231843376375352,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  32.03481237249137,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53289405134631473,\n",
       "  0.032567968284172744,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  12.29480482839786,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.1774572052835361,\n",
       "  0.059369437160318284,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  14.549464820666666,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.6357019003806581,\n",
       "  0.05048371911251659,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  33.714016406707636,\n",
       "  0,\n",
       "  2,\n",
       "  0.125,\n",
       "  0],\n",
       " [1.0622517051060378,\n",
       "  0.094559268232655591,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  6.764273770848353,\n",
       "  1,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.53795015658576184,\n",
       "  0.39583493752514409,\n",
       "  1,\n",
       "  0.125,\n",
       "  20.039564005991014,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.60140779558695234,\n",
       "  0.060522082174601866,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  5.9452835907890424,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.80158155728403258,\n",
       "  0.11945959795504146,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  51.8348008848998,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  1],\n",
       " [0.87406108783119729,\n",
       "  0.033311416173820163,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  8.773744067972746,\n",
       "  0,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  0],\n",
       " [0.36413071261441843,\n",
       "  0.041222394330384104,\n",
       "  2,\n",
       "  0.25,\n",
       "  5.853905717002083,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.74957013148180218,\n",
       "  0.093962562660370175,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  8.258555404789094,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1589488485439905,\n",
       "  0.065555886656367135,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  13.991579829023328,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.30397420956298615,\n",
       "  0.03732194546677603,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  10.36659495759547,\n",
       "  0,\n",
       "  3,\n",
       "  0.6,\n",
       "  0],\n",
       " [1.5299138463075805,\n",
       "  0.092555448133372686,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  26.68096598755219,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.7015452841655605,\n",
       "  0.088476400558004875,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  11.810108293280935,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.63155501869574748,\n",
       "  0.18569728933241514,\n",
       "  2,\n",
       "  0.2,\n",
       "  12.78343576567373,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.2524845857896523,\n",
       "  0.05367185511983541,\n",
       "  2,\n",
       "  0.2,\n",
       "  9.040059966915933,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.30917346443398797,\n",
       "  0.07906584228908653,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  16.82726097941193,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.3166796138329575,\n",
       "  0.033721200159514964,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  10.071913459623941,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.37896911582040693,\n",
       "  0.021356799434221529,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  4.319069176256991,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.42313759542875484,\n",
       "  0.0095877193196908239,\n",
       "  6,\n",
       "  0.75,\n",
       "  3.9791670270611483,\n",
       "  1,\n",
       "  4,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.24231520260984407,\n",
       "  0.030479198266006757,\n",
       "  5,\n",
       "  0.5,\n",
       "  15.659591487809795,\n",
       "  0,\n",
       "  3,\n",
       "  0.3,\n",
       "  1],\n",
       " [0.70870689541370679,\n",
       "  0.072523744142776514,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  10.908836055595287,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.81405573738496961,\n",
       "  0.10170008721414225,\n",
       "  4,\n",
       "  0.26666666666666666,\n",
       "  17.474460893246274,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [1.0370035852133652,\n",
       "  0.056254768568356095,\n",
       "  3,\n",
       "  0.25,\n",
       "  10.683451983386645,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.21952138364833507,\n",
       "  0.13500174867841019,\n",
       "  4,\n",
       "  0.5,\n",
       "  19.369171823212987,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.82481585066303775,\n",
       "  0.083889697121838358,\n",
       "  3,\n",
       "  0.2,\n",
       "  15.2548416284503,\n",
       "  0,\n",
       "  2,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.41982839838885677,\n",
       "  0.38741459942817125,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  11.290060906194084,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.30755707699010049,\n",
       "  0.025336277991944156,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  11.051459492821078,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.60836848225520934,\n",
       "  0.097523066436255634,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  8.08118930013326,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.51838455040559417,\n",
       "  0.031183518525256249,\n",
       "  3,\n",
       "  0.375,\n",
       "  8.0324615001813,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.27108166279686574,\n",
       "  0.036230694580902445,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  7.190711247597094,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [1.4729070533263806,\n",
       "  0.071686278402362191,\n",
       "  2,\n",
       "  0.11764705882352941,\n",
       "  36.52128077950345,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.48618066880068067,\n",
       "  0.017594024621599536,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  6.256567694852629,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.65311685912974626,\n",
       "  0.14174577338383376,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  20.081719581165824,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0131705286513468,\n",
       "  0.15140352199598039,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  19.867011632433254,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87148753527050904,\n",
       "  0.064283972341825701,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  23.367410094204633,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.74805116283791406,\n",
       "  0.070181513718767352,\n",
       "  2,\n",
       "  0.25,\n",
       "  7.251445731239642,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.398594222751379,\n",
       "  0.20782733217094973,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  30.386763718700482,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.43810239350117386,\n",
       "  0.090160557191925506,\n",
       "  2,\n",
       "  0.25,\n",
       "  10.306777536951415,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.19773602396850531,\n",
       "  0.0039705784304266967,\n",
       "  7,\n",
       "  0.875,\n",
       "  4.832184895584422,\n",
       "  0,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.76505244420193341,\n",
       "  0.047028677385317796,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  12.749839525530122,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.17093296132058466,\n",
       "  0.006916423863740695,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  4.755741425093355,\n",
       "  0,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.25155825277124644,\n",
       "  0.087065761009334053,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  19.866871645915186,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.71240305154795658,\n",
       "  0.053159262402740048,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  14.089156953492829,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.4413534813313968,\n",
       "  0.083934942889390851,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  9.683195102216532,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0318872806734578,\n",
       "  0.14656871232728541,\n",
       "  2,\n",
       "  0.2,\n",
       "  18.141105713554794,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.95652547737918436,\n",
       "  0.044543653099595404,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.225868081317357,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.64814879681899118,\n",
       "  0.081246997161281143,\n",
       "  0,\n",
       "  0.0,\n",
       "  15.104617347264243,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1603693460744022,\n",
       "  0.037877077190663022,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  14.55357746881305,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [1.3540223062929639,\n",
       "  0.28137323375788592,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  22.61263544109119,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.82854727452620647,\n",
       "  0.049036606708954489,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  11.256034215448304,\n",
       "  0,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.36595124366033416,\n",
       "  0.029350005005525426,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  6.992444480812073,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.64779846234971239,\n",
       "  0.013428140507365405,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  6.013052269270911,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.48310762499339499,\n",
       "  0.090495702211077167,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  10.29578547673787,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.50684601760125347,\n",
       "  0.033368630042517333,\n",
       "  6,\n",
       "  0.5,\n",
       "  23.8099974485945,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.7235202467419668,\n",
       "  0.086893823563340855,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  15.141055981015997,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.58662600767081086,\n",
       "  0.096893384506376967,\n",
       "  2,\n",
       "  0.2,\n",
       "  10.486753318962853,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.81032748735789562,\n",
       "  0.10056021044303631,\n",
       "  2,\n",
       "  0.125,\n",
       "  15.4877474709235,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.74191588126789865,\n",
       "  0.045543238023729393,\n",
       "  5,\n",
       "  0.5,\n",
       "  8.09585184442119,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.0720992590090577,\n",
       "  0.17210463371004625,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  10.774005380303578,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.29794270299293346,\n",
       "  0.029419687669662964,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  10.237908105242036,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [1.018620893166917,\n",
       "  0.058244948294621124,\n",
       "  2,\n",
       "  0.25,\n",
       "  6.706293277726238,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.90254911135084992,\n",
       "  0.036503098969401626,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  8.27916919260831,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.89951013486429876,\n",
       "  0.020455432265735984,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.214436548940466,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [1.1082989183047673,\n",
       "  0.038087494365943053,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  10.032607493354956,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.20990609848474606,\n",
       "  0.11499319024431331,\n",
       "  4,\n",
       "  0.5,\n",
       "  10.167015713022405,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.89342586041192507,\n",
       "  0.24881361184907858,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  10.910804460390937,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.27259886538604228,\n",
       "  0.10887027420284845,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  14.70279656441591,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.85938586066017053,\n",
       "  0.38984495725970303,\n",
       "  1,\n",
       "  0.125,\n",
       "  18.298919135273806,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.20009746326632882,\n",
       "  0.054371498325386347,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  7.704966313965004,\n",
       "  1,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.9125509275950584,\n",
       "  0.11045669092169641,\n",
       "  3,\n",
       "  0.3,\n",
       "  15.190683269946259,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.53013707203605653,\n",
       "  0.35979609495161646,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  19.284544934796358,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.7739720723907082,\n",
       "  0.08293032829871172,\n",
       "  3,\n",
       "  0.375,\n",
       "  8.94559595694798,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.45684119717342897,\n",
       "  0.031966505197694617,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  5.278596192918436,\n",
       "  1,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.53384578270520755,\n",
       "  0.043473212457701194,\n",
       "  5,\n",
       "  0.5,\n",
       "  8.158555290685513,\n",
       "  0,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [0.32260160576220431,\n",
       "  0.030735243038105931,\n",
       "  4,\n",
       "  0.8,\n",
       "  4.91965852575603,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.65379812055563891,\n",
       "  0.35586871767910233,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  23.66880722777128,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.4449527801730575,\n",
       "  0.17600071796622341,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  8.861547369556217,\n",
       "  0,\n",
       "  1,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.55165770139944836,\n",
       "  0.012938112731013529,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  4.112925944547299,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.36470319131072249,\n",
       "  0.082420496922435027,\n",
       "  3,\n",
       "  0.5,\n",
       "  8.110733177063578,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.070181080573366472,\n",
       "  0.007238644179722975,\n",
       "  5,\n",
       "  0.8333333333333334,\n",
       "  4.409947981180707,\n",
       "  1,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  0],\n",
       " [0.3487705113137215,\n",
       "  0.013543346158489511,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  8.276342861117604,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [1.0371816236040714,\n",
       "  0.10262805228298977,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  14.099794131666032,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.52453979261211803,\n",
       "  0.029954363718853627,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  9.308559105427912,\n",
       "  0,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  0],\n",
       " [0.19276181477194454,\n",
       "  0.025247162959700309,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  8.809943684107857,\n",
       "  1,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.45875898181519281,\n",
       "  0.035456206979004201,\n",
       "  6,\n",
       "  0.42857142857142855,\n",
       "  26.11722187807961,\n",
       "  0,\n",
       "  3,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.57743245568517332,\n",
       "  0.17787034225031961,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  8.928956851498308,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.58440409651859548,\n",
       "  0.010907060864214557,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  9.627465566904121,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.20595013539009854,\n",
       "  0.082357106708369265,\n",
       "  4,\n",
       "  0.5,\n",
       "  11.38924590547043,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.0023643673670886711,\n",
       "  3.3306690738754696e-16,\n",
       "  3,\n",
       "  1.0,\n",
       "  3.744592594843172e-16,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.96911705365230194,\n",
       "  0.088212158177760402,\n",
       "  3,\n",
       "  0.375,\n",
       "  22.310561575818593,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0331800103228186,\n",
       "  0.12877726148320079,\n",
       "  2,\n",
       "  0.25,\n",
       "  14.157514968388314,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.88481905342161238,\n",
       "  0.047119404105936069,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  7.941013934572856,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.1087334727412651,\n",
       "  0.016530345679431457,\n",
       "  6,\n",
       "  0.6,\n",
       "  7.48843211355486,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.32749964520952735,\n",
       "  0.067398728324792101,\n",
       "  2,\n",
       "  0.2,\n",
       "  10.86716138795535,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.0686419629445671,\n",
       "  0.052504519605516609,\n",
       "  2,\n",
       "  0.25,\n",
       "  9.784894844334156,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.20062125497063665,\n",
       "  0.016213676623345274,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  11.758457867258706,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.86706719955238643,\n",
       "  0.079705866283804938,\n",
       "  2,\n",
       "  0.25,\n",
       "  8.113358058742051,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.93574513977641272,\n",
       "  0.048845744346298403,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  6.983672095453563,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53796688941256465,\n",
       "  0.077822769723647434,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  7.082932676525663,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3157713236686559,\n",
       "  0.050072739651376819,\n",
       "  4,\n",
       "  0.25,\n",
       "  16.124777279480988,\n",
       "  0,\n",
       "  2,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.58525113962619058,\n",
       "  0.09295305139173593,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  9.350942749669597,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3474289701989004,\n",
       "  0.21875109050595287,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  20.262756779182826,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.025545793321234078,\n",
       "  0.085129777501825266,\n",
       "  3,\n",
       "  0.6,\n",
       "  6.948071332819922,\n",
       "  0,\n",
       "  1,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.59521671584560143,\n",
       "  0.064896509127363,\n",
       "  3,\n",
       "  0.3,\n",
       "  19.564008903215967,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.42852187227536187,\n",
       "  0.037098288921767808,\n",
       "  4,\n",
       "  0.8,\n",
       "  5.696251589958961,\n",
       "  1,\n",
       "  2,\n",
       "  0.4,\n",
       "  0],\n",
       " [1.692974706303267,\n",
       "  0.050943349793932002,\n",
       "  0,\n",
       "  0.0,\n",
       "  10.107722379752518,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.66421263085169135,\n",
       "  0.08709225022856304,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  14.596366636566048,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.0869301003568432,\n",
       "  0.048571014850335326,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  6.8195692446831275,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.4890117394366692,\n",
       "  0.052726424710308306,\n",
       "  4,\n",
       "  0.5,\n",
       "  7.602825365497392,\n",
       "  0,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.22532129743854168,\n",
       "  0.032102189662846392,\n",
       "  4,\n",
       "  0.8,\n",
       "  5.016274825992153,\n",
       "  1,\n",
       "  2,\n",
       "  0.4,\n",
       "  0],\n",
       " [1.2527189559308998,\n",
       "  0.12109486743527109,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  12.88538834603486,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2394638565601122,\n",
       "  0.05356483064532791,\n",
       "  2,\n",
       "  0.2,\n",
       "  10.487512369345716,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.99575131441444731,\n",
       "  0.013997242502799767,\n",
       "  6,\n",
       "  0.4,\n",
       "  21.24890438585237,\n",
       "  1,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  0],\n",
       " [0.87587022223142841,\n",
       "  0.035411709480139386,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  8.66536222035005,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.57347598147263168,\n",
       "  0.046033692405223148,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  7.226056832058332,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.27255321199721305,\n",
       "  0.062280791923028489,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  18.094532934556593,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.81446802364611637,\n",
       "  0.086692778574396279,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  11.794840718806096,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3167046892220171,\n",
       "  0.088936832034998026,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  15.695633051163474,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2220592423121655,\n",
       "  0.70649276747539214,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  50.533708050134535,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.73819282446892087,\n",
       "  0.063158060043436826,\n",
       "  4,\n",
       "  0.4,\n",
       "  14.829401360882427,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.11274253115894617,\n",
       "  0.0066636050506519995,\n",
       "  8,\n",
       "  0.7272727272727273,\n",
       "  4.17502605356218,\n",
       "  1,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  0],\n",
       " [0.72951687815995825,\n",
       "  0.11180938747873947,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  9.998950773421543,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.39534337591111179,\n",
       "  0.27621298856888232,\n",
       "  4,\n",
       "  0.26666666666666666,\n",
       "  44.42015983531384,\n",
       "  0,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  0],\n",
       " [1.0088442782391396,\n",
       "  0.017199498841445227,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  8.642390125784006,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.14851205996902372,\n",
       "  0.054191860668270309,\n",
       "  3,\n",
       "  0.6,\n",
       "  7.015765335956198,\n",
       "  0,\n",
       "  1,\n",
       "  0.2,\n",
       "  1],\n",
       " [1.1077359089612635,\n",
       "  0.1485046106085337,\n",
       "  2,\n",
       "  0.2,\n",
       "  13.207694433535387,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.88954850182359169,\n",
       "  0.044724345787429565,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  12.444268712151253,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  1],\n",
       " [0.76394876749796048,\n",
       "  0.053089441340176902,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  21.49779965701512,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.2741593198202863,\n",
       "  0.015049627881372762,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  5.378819535087654,\n",
       "  0,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  0],\n",
       " [0.3688923228443316,\n",
       "  0.050163266367628356,\n",
       "  5,\n",
       "  0.5,\n",
       "  11.397004386601797,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.92970225635280268,\n",
       "  0.034053756149450498,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  8.829314236727145,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.1314114963624602,\n",
       "  0.042403090131895249,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  15.088625674661364,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.8008441610556879,\n",
       "  0.097823275897135131,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  7.207676388506535,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69135419817093602,\n",
       "  0.08895993247242695,\n",
       "  4,\n",
       "  0.4,\n",
       "  15.500413495006509,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.81336256936257945,\n",
       "  0.026233609164692862,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  9.439955784495552,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.70308017664163391,\n",
       "  0.077575506058824129,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  9.471185740412645,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.5301706810229887,\n",
       "  0.049087842551829297,\n",
       "  3,\n",
       "  0.375,\n",
       "  15.50110967167266,\n",
       "  0,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.81075242383653068,\n",
       "  0.027913597078173424,\n",
       "  4,\n",
       "  0.5,\n",
       "  6.947147324817496,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.89724202855011737,\n",
       "  0.068011362680588161,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  15.004407864037113,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.68066082567927944,\n",
       "  0.080978577677829788,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  8.312622350578858,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.4378823118458544,\n",
       "  0.020291890527696754,\n",
       "  3,\n",
       "  0.375,\n",
       "  5.691461142803406,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.49118474658956934,\n",
       "  0.062179299261808874,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.181182214354152,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.74034304924188044,\n",
       "  0.066097485594659555,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  7.252879259845686,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.155568813124042,\n",
       "  0.033874516862025472,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  7.331977129500838,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.98381417972591956,\n",
       "  0.060185664370372538,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  32.11332982719284,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.837565908359866,\n",
       "  0.085603945855110219,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  19.253335932154236,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.97824751731679094,\n",
       "  0.066710919346105513,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  10.408176439518583,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.35414180296615339,\n",
       "  0.013639598738713787,\n",
       "  5,\n",
       "  0.625,\n",
       "  6.834323562958941,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.99342047889100193,\n",
       "  0.022350501311194981,\n",
       "  5,\n",
       "  0.5,\n",
       "  7.777462976835911,\n",
       "  1,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [0.8519885107862768,\n",
       "  0.10529969441560438,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  36.57724362854429,\n",
       "  0,\n",
       "  1,\n",
       "  0.05555555555555555,\n",
       "  0],\n",
       " [0.98780480739723908,\n",
       "  0.032479703931005743,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  11.852728647003905,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.71069824377141377,\n",
       "  0.028168510291213478,\n",
       "  4,\n",
       "  0.4,\n",
       "  6.5898753394213765,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.42080501251641012,\n",
       "  0.01435595494177544,\n",
       "  8,\n",
       "  0.8,\n",
       "  9.549133890347077,\n",
       "  1,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  0],\n",
       " [0.58420102946705343,\n",
       "  0.06306304745856528,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  10.300501940729662,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.51308973385964896,\n",
       "  0.031614206380807586,\n",
       "  4,\n",
       "  0.5,\n",
       "  8.339634871803645,\n",
       "  0,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [1.0109096971188598,\n",
       "  0.13599985488414978,\n",
       "  1,\n",
       "  0.0625,\n",
       "  33.12699646114301,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53597378236792803,\n",
       "  0.059788127587811268,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  21.68200212096753,\n",
       "  1,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  1],\n",
       " [0.94781234054998043,\n",
       "  0.031488250405368512,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  16.602232769888577,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87677697787381847,\n",
       "  0.13486736454365911,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  44.20834484417096,\n",
       "  0,\n",
       "  3,\n",
       "  0.15789473684210525,\n",
       "  0],\n",
       " [0.71038762224238217,\n",
       "  0.026223151680381251,\n",
       "  6,\n",
       "  0.42857142857142855,\n",
       "  14.950349606469537,\n",
       "  1,\n",
       "  3,\n",
       "  0.16666666666666666,\n",
       "  1],\n",
       " [1.1863984804584056,\n",
       "  0.088063953689356045,\n",
       "  2,\n",
       "  0.25,\n",
       "  7.865287108417203,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.32430794283741293,\n",
       "  0.093126468050465316,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  11.533877718170363,\n",
       "  0,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.52424292419231044,\n",
       "  0.010792532189217163,\n",
       "  10,\n",
       "  0.7142857142857143,\n",
       "  9.526586949923024,\n",
       "  1,\n",
       "  7,\n",
       "  0.4375,\n",
       "  0],\n",
       " [0.48032242987027518,\n",
       "  0.042932741743593961,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  7.578036068331707,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.79956898922084751,\n",
       "  0.15664141087763195,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.377667360274685,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.29018636934229292,\n",
       "  0.016808109989367215,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  11.622981312572199,\n",
       "  1,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [0.47252373410337667,\n",
       "  0.27692602898858087,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  15.10512969796744,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.85571352801121603,\n",
       "  0.36934324985750433,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  20.81933278079968,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.35615894994439112,\n",
       "  0.033385821697831553,\n",
       "  5,\n",
       "  0.3333333333333333,\n",
       "  13.924888817164565,\n",
       "  0,\n",
       "  2,\n",
       "  0.125,\n",
       "  1],\n",
       " [1.119430248190399,\n",
       "  0.084018491651740135,\n",
       "  2,\n",
       "  0.2,\n",
       "  18.640882587318107,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.57635502375727254,\n",
       "  0.033688130213279099,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  6.7481105419766685,\n",
       "  1,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.62854558627500101,\n",
       "  0.026357122314500381,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  4.038151230011815,\n",
       "  1,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.698082646765536,\n",
       "  0.022402795902725869,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  13.08870812616677,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.62550106559969987,\n",
       "  0.14693719543532058,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  17.388978564978164,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.94910749752777579,\n",
       "  0.16059480882392063,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  27.02174068755733,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.67376037931244159,\n",
       "  0.028686746914683869,\n",
       "  5,\n",
       "  0.625,\n",
       "  8.6631638286561,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.23394262974447655,\n",
       "  0.054577384956884578,\n",
       "  6,\n",
       "  0.6,\n",
       "  9.441370115024354,\n",
       "  1,\n",
       "  4,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.4650598549387166,\n",
       "  0.057655776426027372,\n",
       "  5,\n",
       "  0.3125,\n",
       "  22.023661796733382,\n",
       "  0,\n",
       "  4,\n",
       "  0.23529411764705882,\n",
       "  0],\n",
       " [0.69462218735557935,\n",
       "  0.033061230758633431,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  8.51344976638459,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.73322630213558049,\n",
       "  0.05230973296118735,\n",
       "  2,\n",
       "  0.125,\n",
       "  11.372899552374891,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [1.0583206075623486,\n",
       "  0.068105832986269488,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  30.52118611857067,\n",
       "  1,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [1.0434401744348587,\n",
       "  0.071674429046381327,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.648240791241811,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.4864150935982625,\n",
       "  0.024084747507187143,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  17.276692283715295,\n",
       "  0,\n",
       "  1,\n",
       "  0.047619047619047616,\n",
       "  0],\n",
       " [0.38783046057893256,\n",
       "  0.04955285431112344,\n",
       "  3,\n",
       "  0.2,\n",
       "  11.564592743227779,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.011076561253547723,\n",
       "  0.02435673484845069,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  5.439987110073999,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.011243991212380844,\n",
       "  0.10934905356044278,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  7.716015104533666,\n",
       "  1,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [1.1066914447499405,\n",
       "  0.23340620496841591,\n",
       "  1,\n",
       "  0.125,\n",
       "  12.568518566186206,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.4926612767422145,\n",
       "  0.03500306443475798,\n",
       "  3,\n",
       "  0.3,\n",
       "  8.37774932512357,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.7986473580242277,\n",
       "  0.011572809402234596,\n",
       "  8,\n",
       "  0.5714285714285714,\n",
       "  6.235576810641249,\n",
       "  1,\n",
       "  4,\n",
       "  0.23529411764705882,\n",
       "  1],\n",
       " [0.20424586823453272,\n",
       "  0.062689331119694214,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  9.935008647609871,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [1.0822165111915329,\n",
       "  0.024594736646635695,\n",
       "  3,\n",
       "  0.3,\n",
       "  7.5163534555908695,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.90302612131594995,\n",
       "  0.0324735388541737,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  7.810019611040573,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.4889357626940436,\n",
       "  0.17691959250318057,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  16.879391487441243,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.84459485577771909,\n",
       "  0.059161513682884803,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  12.598493595188142,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.1381142794082203,\n",
       "  0.017131918136473745,\n",
       "  8,\n",
       "  0.6153846153846154,\n",
       "  7.202952066500552,\n",
       "  1,\n",
       "  6,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.47620529430612502,\n",
       "  0.092870687857441792,\n",
       "  3,\n",
       "  0.3,\n",
       "  11.118570225495539,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.60387455891080133,\n",
       "  0.061373336478236951,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  31.069940565478262,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.097227763441992021,\n",
       "  0.033890772045520601,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  7.143632707645428,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.90978242765318829,\n",
       "  0.15107818588665356,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  14.912684483633189,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0517150534775319,\n",
       "  0.025430855829505883,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  16.535177180911354,\n",
       "  0,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  1],\n",
       " [0.39306637179291737,\n",
       "  0.04023401062649945,\n",
       "  4,\n",
       "  0.4,\n",
       "  6.916636253470091,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.88671983131470922,\n",
       "  0.038623776175878755,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  18.418521304307337,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.90591560369810431,\n",
       "  0.044100849556049204,\n",
       "  4,\n",
       "  0.2222222222222222,\n",
       "  47.45476144835311,\n",
       "  0,\n",
       "  1,\n",
       "  0.047619047619047616,\n",
       "  0],\n",
       " [1.1970147697617834,\n",
       "  0.0513547221177304,\n",
       "  2,\n",
       "  0.125,\n",
       "  15.857264870354477,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80371678718152206,\n",
       "  0.07964479023894,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  10.58267714333154,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1447335013925442,\n",
       "  0.042780155343429938,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  12.782458252946299,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.40065475456222921,\n",
       "  0.030347066464184058,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  13.537743866296848,\n",
       "  1,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  1],\n",
       " [0.3943103842282647,\n",
       "  0.030755280414822339,\n",
       "  5,\n",
       "  0.5,\n",
       "  9.091425200629844,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [1.0789981390622889,\n",
       "  0.044039583682577321,\n",
       "  2,\n",
       "  0.125,\n",
       "  11.987130296155888,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.80616750063886999,\n",
       "  0.041145343005696833,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  9.44928126609778,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.45348834132245119,\n",
       "  0.030977490299189059,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  13.357167136345035,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  1],\n",
       " [1.2451762727561477,\n",
       "  0.31139139957577566,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  21.79618219878028,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.26796005482205287,\n",
       "  0.051309136981608439,\n",
       "  6,\n",
       "  0.4,\n",
       "  14.574747295677719,\n",
       "  0,\n",
       "  4,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.72677566273100425,\n",
       "  0.03556690604742152,\n",
       "  4,\n",
       "  0.5,\n",
       "  13.447383056969418,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.96835476249152352,\n",
       "  0.23581394789537857,\n",
       "  1,\n",
       "  0.1,\n",
       "  24.74355814818072,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.71434548588531999,\n",
       "  0.027928649582947496,\n",
       "  8,\n",
       "  0.6666666666666666,\n",
       "  15.205768785394428,\n",
       "  1,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  0],\n",
       " [0.72255244789803963,\n",
       "  0.047180508809510857,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  11.032052646086392,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.95210588004899843,\n",
       "  0.022571434397419488,\n",
       "  3,\n",
       "  0.25,\n",
       "  8.909254502522133,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.89805632256359647,\n",
       "  0.011621853077850464,\n",
       "  7,\n",
       "  0.7,\n",
       "  12.464314542105354,\n",
       "  1,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [1.0866176466233091,\n",
       "  0.05352075107872134,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  19.038215848237872,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.91881355424178679,\n",
       "  0.076121788152416991,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  11.196657034025929,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.80023086632094564,\n",
       "  0.031195103570139771,\n",
       "  3,\n",
       "  0.375,\n",
       "  6.826748576674162,\n",
       "  1,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.48280968790369694,\n",
       "  0.063789138863857842,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  10.90964952573549,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.1509245601498717,\n",
       "  0.025396406216123935,\n",
       "  3,\n",
       "  0.1875,\n",
       "  27.696881590914163,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.55116763287243264,\n",
       "  0.024356582351472755,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  22.380480385707443,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.92466573032738486,\n",
       "  0.048501595938135078,\n",
       "  2,\n",
       "  0.2,\n",
       "  15.75241535398635,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.38772480352810812,\n",
       "  0.022516704645484054,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  22.40911146796216,\n",
       "  1,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.83566407295207368,\n",
       "  0.07121957839936921,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  13.212286145848097,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [1.1095438489803242,\n",
       "  0.077984885913848334,\n",
       "  3,\n",
       "  0.2,\n",
       "  27.182526832596942,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1457955630485395,\n",
       "  0.07299298203045157,\n",
       "  3,\n",
       "  0.3,\n",
       "  16.726486189721797,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [1.2487796625427374,\n",
       "  0.058891369956187045,\n",
       "  3,\n",
       "  0.2,\n",
       "  12.22570721184361,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.41966260429668167,\n",
       "  0.20400344915218227,\n",
       "  3,\n",
       "  0.5,\n",
       "  10.748494358353453,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.53483154996751714,\n",
       "  0.20482188313418626,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  31.275237552182,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  1],\n",
       " [0.51851337623949612,\n",
       "  0.061366956323219757,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  18.102186410327263,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.79274787090863286,\n",
       "  0.039892553705301537,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  7.279661740360916,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.33340550668478708,\n",
       "  0.039179043967289995,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  7.600677834791827,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69494734226466881,\n",
       "  0.016383723643157877,\n",
       "  8,\n",
       "  0.6666666666666666,\n",
       "  13.569042661957013,\n",
       "  1,\n",
       "  6,\n",
       "  0.46153846153846156,\n",
       "  0],\n",
       " [0.71676004338083332,\n",
       "  0.019534325240791905,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  9.437974686155005,\n",
       "  1,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  0],\n",
       " [0.4571624497761827,\n",
       "  0.010714893815537585,\n",
       "  7,\n",
       "  0.875,\n",
       "  5.321509716503697,\n",
       "  0,\n",
       "  5,\n",
       "  0.625,\n",
       "  0],\n",
       " [1.0095558971065017,\n",
       "  0.054883506498203172,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  14.27810286801822,\n",
       "  1,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.9641773527510813,\n",
       "  0.04552784879800531,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  15.021570699139598,\n",
       "  1,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  0],\n",
       " [0.8058566433138592,\n",
       "  0.10253226556329997,\n",
       "  2,\n",
       "  0.2,\n",
       "  11.302772117788457,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.64862828718927168,\n",
       "  0.053350683846525082,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  13.01669683333019,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.79862686814441197,\n",
       "  0.18134836466073467,\n",
       "  2,\n",
       "  0.25,\n",
       "  9.994626365620189,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.5302826605108506,\n",
       "  0.030747870772796948,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  6.2664299656812,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [1.1582639161627262,\n",
       "  0.036997464455916784,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  28.171809946762234,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.46694535813395921,\n",
       "  0.12089701491000537,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  14.008949716784693,\n",
       "  0,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [0.50399262713891479,\n",
       "  0.01730164440089077,\n",
       "  8,\n",
       "  0.6153846153846154,\n",
       "  11.474199907225298,\n",
       "  1,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  0],\n",
       " [1.2808026362223353,\n",
       "  0.04932666950219855,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  9.305893511201855,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.29521687589549861,\n",
       "  0.04179114300601916,\n",
       "  5,\n",
       "  0.5,\n",
       "  8.837864574714162,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.158867282209177,\n",
       "  0.050707790067145098,\n",
       "  3,\n",
       "  0.25,\n",
       "  9.59265617510486,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.59117619587704595,\n",
       "  0.11243631545890798,\n",
       "  2,\n",
       "  0.2,\n",
       "  7.5664689392914894,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.39585840644479842,\n",
       "  0.037355470550054837,\n",
       "  6,\n",
       "  0.3157894736842105,\n",
       "  29.45206882068754,\n",
       "  0,\n",
       "  4,\n",
       "  0.21052631578947367,\n",
       "  0],\n",
       " [0.35051658381144013,\n",
       "  0.02477831725260371,\n",
       "  6,\n",
       "  0.5,\n",
       "  9.332286531990706,\n",
       "  1,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  0],\n",
       " [0.82955798163356609,\n",
       "  0.072241922933600655,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  10.969923469585185,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.2257471999087899,\n",
       "  0.057530504424811668,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  9.4189310134045,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.79091368729773948,\n",
       "  0.055535641836811256,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  10.809890291435456,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.77668742634788157,\n",
       "  0.031561395555727678,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  12.966018421609592,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.76111353052938036,\n",
       "  0.033983530377212134,\n",
       "  3,\n",
       "  0.375,\n",
       "  13.525284492955722,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.27866989365402484,\n",
       "  0.064763010647476249,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  10.74454410005084,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.84409911700514739,\n",
       "  0.049435177769027616,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  7.348322488966513,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [1.1612095886365252,\n",
       "  0.048987559477368769,\n",
       "  4,\n",
       "  0.4,\n",
       "  11.6751676760813,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.63298068348200442,\n",
       "  0.13896927827096706,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  38.83243788750291,\n",
       "  0,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  1],\n",
       " [0.18248899410250718,\n",
       "  0.044866854917387622,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.166724974442415,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.64991017843780485,\n",
       "  0.10437687039671872,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  22.308285710287855,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.7148477350222417,\n",
       "  0.17710347893052081,\n",
       "  0,\n",
       "  0.0,\n",
       "  14.439244102843206,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.23839809099191078,\n",
       "  0.029741209097099763,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  8.13682065686507,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80338033924852215,\n",
       "  0.041896740503770347,\n",
       "  4,\n",
       "  0.5,\n",
       "  7.473673117741689,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.74859543607154744,\n",
       "  0.028119049641330385,\n",
       "  4,\n",
       "  0.2,\n",
       "  11.182627534525444,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69595359356660613,\n",
       "  0.027164623612988104,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  7.137420010587847,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53653817492741585,\n",
       "  0.18693581389344438,\n",
       "  3,\n",
       "  0.2,\n",
       "  25.26722472371184,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.66739490472823215,\n",
       "  0.033438893058971741,\n",
       "  3,\n",
       "  0.5,\n",
       "  6.114362414252028,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [1.0751308355111981,\n",
       "  0.036461669953547715,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  28.051381084807254,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.51118940146377578,\n",
       "  0.093047428207717497,\n",
       "  3,\n",
       "  0.3,\n",
       "  12.462626996547202,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.99794182521154029,\n",
       "  0.02576020675292523,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  17.592607797030958,\n",
       "  1,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.71325204522519359,\n",
       "  0.04402334331386859,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  12.278527885149272,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.81417980738579687,\n",
       "  0.063812029239606161,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  8.844155351912152,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2136550327615669,\n",
       "  0.056598000831800999,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.079317384957495,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.30040478480718458,\n",
       "  0.022371022726802803,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  6.471146852620277,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.13160335562639081,\n",
       "  0.031615203126675762,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  14.961521750628147,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.8508408619684843,\n",
       "  0.063002008836876899,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  12.512775671761926,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.81192071930353138,\n",
       "  0.16634609811974088,\n",
       "  2,\n",
       "  0.2,\n",
       "  20.553142473060774,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.42120288384461324,\n",
       "  0.054199827830162195,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  6.737732219541375,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.96908896390898391,\n",
       "  0.12010133493276709,\n",
       "  4,\n",
       "  0.25,\n",
       "  45.35621191785732,\n",
       "  0,\n",
       "  2,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.3996632455190379,\n",
       "  0.13686457392131901,\n",
       "  2,\n",
       "  0.2,\n",
       "  12.384142115629343,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.96874451381602344,\n",
       "  0.023665480642382275,\n",
       "  4,\n",
       "  0.5,\n",
       "  10.012993566944406,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.49267409704828113,\n",
       "  0.020171603039882635,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  9.745695158472532,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.28367881859869848,\n",
       "  0.097265195707747387,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  11.442046065246286,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.80213264912552562,\n",
       "  0.11488754077258512,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  10.00401743665469,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.3100895046586882,\n",
       "  0.053240242989861963,\n",
       "  3,\n",
       "  0.25,\n",
       "  9.623266156428098,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.70517518004559165,\n",
       "  0.031234413855779741,\n",
       "  5,\n",
       "  0.625,\n",
       "  12.499286336525898,\n",
       "  0,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  0],\n",
       " [0.64978102377445057,\n",
       "  0.007716478742547217,\n",
       "  10,\n",
       "  0.7692307692307693,\n",
       "  15.666455395148079,\n",
       "  1,\n",
       "  8,\n",
       "  0.5714285714285714,\n",
       "  0],\n",
       " [1.1185448470787904,\n",
       "  0.047285068239801165,\n",
       "  3,\n",
       "  0.25,\n",
       "  10.440827795975293,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.91379078820374804,\n",
       "  0.0488738468696156,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  22.195556873370705,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [1.0191099317479324,\n",
       "  0.13175809043869002,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  16.375613411253045,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1809373472086167,\n",
       "  0.019102723036041991,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  6.803023275251487,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  1],\n",
       " [0.55917163275780779,\n",
       "  0.050033704257950506,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  17.053110354663687,\n",
       "  1,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [1.2065775931134433,\n",
       "  0.021152002677381154,\n",
       "  5,\n",
       "  0.625,\n",
       "  6.508668185601522,\n",
       "  1,\n",
       "  3,\n",
       "  0.375,\n",
       "  0],\n",
       " [0.45493825771442098,\n",
       "  0.10826784501808107,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  17.810776408353686,\n",
       "  0,\n",
       "  2,\n",
       "  0.10526315789473684,\n",
       "  1],\n",
       " [0.50380705180897323,\n",
       "  0.013765370744228456,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  5.8598202392305785,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.93614170086172399,\n",
       "  0.022336909308738218,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  9.462903084607065,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.25520800250697739,\n",
       "  0.48117866069629178,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  24.63705029312825,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.78548037211995847,\n",
       "  0.27209926990725941,\n",
       "  2,\n",
       "  0.2,\n",
       "  18.675544839499068,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.79682389439028634,\n",
       "  0.035120641370809613,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  9.62529350770473,\n",
       "  1,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.86284060340782553,\n",
       "  0.065294693206514709,\n",
       "  3,\n",
       "  0.3,\n",
       "  14.337612308076144,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2436269567309786,\n",
       "  0.010174420678481511,\n",
       "  4,\n",
       "  0.4,\n",
       "  10.336953769728293,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.73356311275185337,\n",
       "  0.043584931947609817,\n",
       "  2,\n",
       "  0.25,\n",
       "  12.033302706296817,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.41134656211769782,\n",
       "  0.1195339624135211,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  13.61524123564176,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87259278805690454,\n",
       "  0.023329117380565645,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  11.685440049722477,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.83190913442199921,\n",
       "  0.010028261452355247,\n",
       "  8,\n",
       "  0.7272727272727273,\n",
       "  5.925163524892995,\n",
       "  1,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  0],\n",
       " [1.2222875213790634,\n",
       "  0.026327271673760255,\n",
       "  5,\n",
       "  0.3125,\n",
       "  9.495242849822516,\n",
       "  0,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [1.2166806075842154,\n",
       "  0.045437124266933471,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  14.82201291510669,\n",
       "  1,\n",
       "  1,\n",
       "  0.05263157894736842,\n",
       "  0],\n",
       " [0.6670243120922219,\n",
       "  0.029664030736371694,\n",
       "  4,\n",
       "  0.4,\n",
       "  9.193683258210353,\n",
       "  0,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.571671866575586,\n",
       "  0.11458076975302178,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  25.900614416539963,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.72417180493342781,\n",
       "  0.022116712927204851,\n",
       "  4,\n",
       "  0.4,\n",
       "  18.616625559829657,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.53062768704980712,\n",
       "  0.25875342207024965,\n",
       "  3,\n",
       "  0.3,\n",
       "  14.656629051290095,\n",
       "  0,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.72654581591232481,\n",
       "  0.13812399362252392,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  17.848147361623,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.65128910092429793,\n",
       "  0.023808769557078113,\n",
       "  4,\n",
       "  0.5,\n",
       "  5.2391868842196345,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.118991572297543,\n",
       "  0.035486439165626438,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  13.900123339580116,\n",
       "  1,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  1],\n",
       " [0.94273998925442815,\n",
       "  0.041827298770848453,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  14.994998538945456,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.79424395968361794,\n",
       "  0.21321913612495302,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  15.328906941741787,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0698269028600695,\n",
       "  0.19587764064192903,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  24.398722794074082,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.26213005634594244,\n",
       "  0.039514049532665463,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  9.399352774061306,\n",
       "  1,\n",
       "  4,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.77760719152888336,\n",
       "  0.024393070529416194,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  11.746561624542785,\n",
       "  0,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.52042763369762413,\n",
       "  0.19764324698137281,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  24.88835786419792,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.2678621857399082,\n",
       "  0.081981843140034316,\n",
       "  1,\n",
       "  0.1,\n",
       "  9.840542709207652,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0284460028324269,\n",
       "  0.11784772854436032,\n",
       "  3,\n",
       "  0.3,\n",
       "  20.308609286669476,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80800516085800056,\n",
       "  0.035900315426275697,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  15.001532308399579,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  1],\n",
       " [0.92484308982075081,\n",
       "  0.25256592741967376,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  17.362817905361904,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.71265240276342301,\n",
       "  0.053939053828450434,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  15.047033916578897,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0656863849836615,\n",
       "  0.031768916697585525,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  9.468666010927734,\n",
       "  0,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.5629770065317139,\n",
       "  0.1764788486706369,\n",
       "  2,\n",
       "  0.25,\n",
       "  14.080573459731413,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.37287181112430434,\n",
       "  0.065792534549935611,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  27.80155035448926,\n",
       "  1,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.73422690428438675,\n",
       "  0.03107303632286651,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  8.50889128422268,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.1740797754483603,\n",
       "  0.060641517473783657,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  11.511583340830366,\n",
       "  1,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.14268858848794286,\n",
       "  0.0044655859693328859,\n",
       "  8,\n",
       "  0.8888888888888888,\n",
       "  5.057550830208772,\n",
       "  1,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  0],\n",
       " [0.52006951416281455,\n",
       "  0.059960795435790581,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  11.472726538754074,\n",
       "  0,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.26983520431162922,\n",
       "  0.028478179091348421,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  8.55703869941775,\n",
       "  1,\n",
       "  4,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.93289476865796539,\n",
       "  0.11520181028297305,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  14.009419003506766,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.37768882084333066,\n",
       "  0.04201775121814233,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  20.771258160664555,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.88311675152714486,\n",
       "  0.074323778513209815,\n",
       "  3,\n",
       "  0.6,\n",
       "  6.214355554527295,\n",
       "  0,\n",
       "  1,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.589008091626291,\n",
       "  0.10217535824633506,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  14.977820446197168,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.68041870375731461,\n",
       "  0.025991154733230659,\n",
       "  4,\n",
       "  0.5,\n",
       "  5.451566245519516,\n",
       "  1,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.59005490992983844,\n",
       "  0.029757336881413243,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  7.0809876818329585,\n",
       "  1,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.44093912201177299,\n",
       "  0.036750701296549892,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  9.189674826177052,\n",
       "  1,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  0],\n",
       " [0.15099941072273249,\n",
       "  0.015259355965759669,\n",
       "  9,\n",
       "  0.6923076923076923,\n",
       "  8.504913251297078,\n",
       "  1,\n",
       "  5,\n",
       "  0.3125,\n",
       "  0],\n",
       " [1.1463008302521387,\n",
       "  0.039170390985667547,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  23.667481153223417,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.94817341780709696,\n",
       "  0.035050700596632067,\n",
       "  4,\n",
       "  0.4,\n",
       "  11.899897092938362,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [1.3779301432592057,\n",
       "  0.2148337760490252,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  15.559529581282078,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.73631415366395458,\n",
       "  0.031388282807243484,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  10.955949162104446,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.90829946447194043,\n",
       "  0.078777355549104033,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  8.469847698562022,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87858888692785442,\n",
       "  0.078661276622776222,\n",
       "  2,\n",
       "  0.125,\n",
       "  36.18688563675422,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [1.1959923975043556,\n",
       "  0.18417611662671474,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  35.476945875808724,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.005673251897782,\n",
       "  0.047200945397765648,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  12.687244264252243,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.3407881006644002,\n",
       "  0.049114369735429042,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  8.6101520980984,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.21840009307378039,\n",
       "  0.042478805628434646,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  5.730117362308124,\n",
       "  1,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.60093570743615055,\n",
       "  0.19446544415339351,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  22.892025150974288,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.95883847238133257,\n",
       "  0.15049671952795063,\n",
       "  0,\n",
       "  0.0,\n",
       "  7.898127333782396,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0045985615427355,\n",
       "  0.071959906942947627,\n",
       "  4,\n",
       "  0.5,\n",
       "  17.144181860172857,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [1.2323292415732787,\n",
       "  0.13717404503940933,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  26.465545932780294,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.78248356261699936,\n",
       "  0.060506995230947269,\n",
       "  3,\n",
       "  0.25,\n",
       "  11.272160114983356,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.36622653014374618,\n",
       "  0.010011726880191807,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  9.746347994008165,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [0.42199137359543781,\n",
       "  0.012146647485197004,\n",
       "  9,\n",
       "  0.5625,\n",
       "  7.183271162712385,\n",
       "  1,\n",
       "  5,\n",
       "  0.2631578947368421,\n",
       "  0],\n",
       " [0.48621581292428573,\n",
       "  0.055931810040041108,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  8.807946017673343,\n",
       "  1,\n",
       "  3,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.64798037558009569,\n",
       "  0.03618475271111421,\n",
       "  6,\n",
       "  0.6,\n",
       "  8.05292643446313,\n",
       "  1,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [0.86585174301133228,\n",
       "  0.032222126220470826,\n",
       "  5,\n",
       "  0.625,\n",
       "  6.477448638851364,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.74327720714191681,\n",
       "  0.015635170636843165,\n",
       "  10,\n",
       "  0.6666666666666666,\n",
       "  7.850479268660423,\n",
       "  1,\n",
       "  6,\n",
       "  0.3333333333333333,\n",
       "  1],\n",
       " [0.64863701886775149,\n",
       "  0.035867928331904553,\n",
       "  4,\n",
       "  0.4,\n",
       "  11.414382847462509,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.47098874328702156,\n",
       "  0.046328937914945856,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  6.698219553655831,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.2739557893711428,\n",
       "  0.085802422183059002,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  13.041953340778024,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.50852087175268257,\n",
       "  0.048784794277064836,\n",
       "  4,\n",
       "  0.4,\n",
       "  8.41158966641185,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.74201430771803079,\n",
       "  0.22332404548425278,\n",
       "  5,\n",
       "  0.22727272727272727,\n",
       "  43.65763841545126,\n",
       "  0,\n",
       "  2,\n",
       "  0.08695652173913043,\n",
       "  0],\n",
       " [0.53487558828289306,\n",
       "  0.061937315810189464,\n",
       "  5,\n",
       "  0.5,\n",
       "  8.334751055461757,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.36980494106942152,\n",
       "  0.055597470123118442,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  15.254606593934785,\n",
       "  0,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [1.79896623747275,\n",
       "  0.049226323720078624,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  8.565166153976296,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.0738095532763809,\n",
       "  0.058798695731085204,\n",
       "  4,\n",
       "  0.5,\n",
       "  9.15271782194502,\n",
       "  1,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.35670772559136632,\n",
       "  0.023055760369977252,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  9.464652105377622,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.24758226188187726,\n",
       "  0.0094274032546944886,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  5.006460579759787,\n",
       "  1,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [1.1284216476364244,\n",
       "  0.042097450438721595,\n",
       "  4,\n",
       "  0.4,\n",
       "  12.799546093306368,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.3444581799166955,\n",
       "  0.043181903931057808,\n",
       "  6,\n",
       "  0.46153846153846156,\n",
       "  21.390384705607136,\n",
       "  1,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  0],\n",
       " [0.79394630512401965,\n",
       "  0.036587782862142038,\n",
       "  7,\n",
       "  0.5384615384615384,\n",
       "  9.304390773185894,\n",
       "  1,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.5595287941949767,\n",
       "  0.0068806898582132181,\n",
       "  8,\n",
       "  0.6666666666666666,\n",
       "  3.397339612700767,\n",
       "  1,\n",
       "  6,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.87983306842901055,\n",
       "  0.12357932256529625,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  13.79847115976522,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.44407349871195689,\n",
       "  0.018725610298443218,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  19.35530797356711,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.62809275018674926,\n",
       "  0.039804424000627003,\n",
       "  6,\n",
       "  0.5,\n",
       "  9.206207450677956,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [1.1632741279974612,\n",
       "  0.059895423122521607,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  10.273863521057747,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.56133763589085162,\n",
       "  0.067230851575902473,\n",
       "  2,\n",
       "  0.2,\n",
       "  9.570000540711574,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.78735383936624104,\n",
       "  0.016882934915892767,\n",
       "  6,\n",
       "  0.75,\n",
       "  6.548059108482086,\n",
       "  1,\n",
       "  5,\n",
       "  0.625,\n",
       "  0],\n",
       " [0.96960705121295243,\n",
       "  0.029600831982294351,\n",
       "  4,\n",
       "  0.4,\n",
       "  10.506399936487922,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.65457464883104677,\n",
       "  0.0078243876470833751,\n",
       "  6,\n",
       "  0.6,\n",
       "  11.020588712526752,\n",
       "  1,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  0],\n",
       " [0.87713298533586992,\n",
       "  0.049658530439114279,\n",
       "  3,\n",
       "  0.375,\n",
       "  15.916210807441185,\n",
       "  0,\n",
       "  1,\n",
       "  0.125,\n",
       "  0],\n",
       " [0.75270975673716678,\n",
       "  0.10597229154797505,\n",
       "  6,\n",
       "  0.46153846153846156,\n",
       "  18.589894198209567,\n",
       "  0,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  0],\n",
       " [0.6683081482617601,\n",
       "  0.070443750239468361,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  14.871635373002889,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.44269441974042101,\n",
       "  0.0086230321551143962,\n",
       "  8,\n",
       "  0.8,\n",
       "  5.118542680923156,\n",
       "  1,\n",
       "  6,\n",
       "  0.6,\n",
       "  0],\n",
       " [0.9353755720943252,\n",
       "  0.084079281163178643,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.118980083341853,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.57697064621786465,\n",
       "  0.26728374133063049,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  18.649141335899387,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.29649121839040882,\n",
       "  0.011172418338183721,\n",
       "  9,\n",
       "  0.6428571428571429,\n",
       "  15.758322334541067,\n",
       "  1,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  0],\n",
       " [1.3838410383763766,\n",
       "  0.092216505142943994,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  17.352540784916126,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.48272812737461968,\n",
       "  0.029931773038697651,\n",
       "  5,\n",
       "  0.625,\n",
       "  6.901614849389529,\n",
       "  0,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  0],\n",
       " [0.034992921848680081,\n",
       "  0.01289606470583593,\n",
       "  7,\n",
       "  0.875,\n",
       "  5.833549122462243,\n",
       "  1,\n",
       "  6,\n",
       "  0.8571428571428571,\n",
       "  0],\n",
       " [0.58522856542617263,\n",
       "  0.0060638880604372281,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  5.876008125304974,\n",
       "  0,\n",
       "  5,\n",
       "  0.625,\n",
       "  0],\n",
       " [1.0885414973370415,\n",
       "  0.063129028985789648,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  14.919148670309012,\n",
       "  0,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [0.0059694808756801665,\n",
       "  0.013046064890736564,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  5.211819798070227,\n",
       "  1,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [1.5074512034165188,\n",
       "  0.071871544390762709,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  15.519294814244061,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.66390117045395025,\n",
       "  0.098494387497766067,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  38.84525225637416,\n",
       "  0,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [1.5949634353368127,\n",
       "  0.13365534054756789,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  20.603338879398255,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.93223170235266994,\n",
       "  0.015978177044917108,\n",
       "  8,\n",
       "  0.6153846153846154,\n",
       "  13.595681108642292,\n",
       "  1,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  0],\n",
       " [0.41444707513607049,\n",
       "  0.069403209547984845,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  15.077915665715883,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69394607813390397,\n",
       "  0.028376801419030651,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  13.354141202597312,\n",
       "  1,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  1],\n",
       " [0.66893803288954001,\n",
       "  0.01485641376651492,\n",
       "  7,\n",
       "  0.7,\n",
       "  9.47593787364944,\n",
       "  1,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  0],\n",
       " [0.607598905858308,\n",
       "  0.022328248938686568,\n",
       "  5,\n",
       "  0.5,\n",
       "  7.742511240695821,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.84182123538627374,\n",
       "  0.033653866711503388,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  10.538803134747416,\n",
       "  0,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [1.1005760379188978,\n",
       "  0.048922393514415075,\n",
       "  2,\n",
       "  0.2,\n",
       "  14.847837281449204,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.83692288720404284,\n",
       "  0.059314945806097197,\n",
       "  3,\n",
       "  0.25,\n",
       "  9.109837700082029,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.340620844097935,\n",
       "  0.099158650523779257,\n",
       "  2,\n",
       "  0.2,\n",
       "  25.2226847269219,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.18761221517647808,\n",
       "  0.086201839106589406,\n",
       "  6,\n",
       "  0.4,\n",
       "  15.262386439393639,\n",
       "  0,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  0],\n",
       " [0.45656586251351128,\n",
       "  0.042312094475108752,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  11.452823149441752,\n",
       "  1,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [1.2204320281093242,\n",
       "  0.029659179134011837,\n",
       "  3,\n",
       "  0.25,\n",
       "  10.458336291437943,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [1.0743332251140512,\n",
       "  0.046057401794128427,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  9.988552673667236,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.78025016175606199,\n",
       "  0.035275348849581611,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  17.44639824948621,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.98846070713237155,\n",
       "  0.097606146869198107,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  15.798246257883559,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.70622436896151797,\n",
       "  0.046352127200857685,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  11.192698070659244,\n",
       "  1,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.38080929624305393,\n",
       "  0.043406139546351974,\n",
       "  3,\n",
       "  0.5,\n",
       "  7.332041707031122,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.6032607888860217,\n",
       "  0.098885450121241392,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  9.778300355276427,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.64898413388959197,\n",
       "  0.0052611880753823392,\n",
       "  8,\n",
       "  0.7272727272727273,\n",
       "  5.321694919866417,\n",
       "  1,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  0],\n",
       " [1.0305971010394956,\n",
       "  0.03493510941446254,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  22.16047514065258,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.95400154613809518,\n",
       "  0.046998848147979055,\n",
       "  7,\n",
       "  0.5,\n",
       "  22.01856240785706,\n",
       "  1,\n",
       "  4,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.9561052789468476,\n",
       "  0.01622563476938399,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  4.164445233594608,\n",
       "  1,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.60396770257664523,\n",
       "  0.13479612206565639,\n",
       "  2,\n",
       "  0.125,\n",
       "  36.087219878742516,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.65741900289904975,\n",
       "  0.20646023915794054,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  11.913038406646567,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  1],\n",
       " [0.73626612879089759,\n",
       "  0.043099111187778094,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  5.857085530238018,\n",
       "  0,\n",
       "  1,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [1.2274479072641493,\n",
       "  0.042106156066059275,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  11.141368235304276,\n",
       "  1,\n",
       "  1,\n",
       "  0.07142857142857142,\n",
       "  0],\n",
       " [0.55141177577153799,\n",
       "  0.037340455002572437,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  9.866230332558047,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [1.2475772109397807,\n",
       "  0.023781037990209142,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  20.180148649912887,\n",
       "  1,\n",
       "  4,\n",
       "  0.3076923076923077,\n",
       "  0],\n",
       " [0.41171334696060735,\n",
       "  0.01416350916468978,\n",
       "  8,\n",
       "  0.7272727272727273,\n",
       "  5.671759072737401,\n",
       "  1,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  0],\n",
       " [0.59576580628055775,\n",
       "  0.083419131615968589,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  10.351466197058734,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.53297001883094464,\n",
       "  0.23504218732468984,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  26.883527171834068,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  1],\n",
       " [0.11220543305412722,\n",
       "  0.0091362522071544161,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  6.250794554182136,\n",
       "  1,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.29083667420963666,\n",
       "  0.064417836935507511,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  11.41939127789652,\n",
       "  0,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.3071935485697359,\n",
       "  0.078148203374888703,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  13.103584446762314,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.84651961534334941,\n",
       "  0.096914191113047221,\n",
       "  3,\n",
       "  0.1875,\n",
       "  20.787737930587888,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.90089651544797,\n",
       "  0.038184319728579497,\n",
       "  7,\n",
       "  0.7,\n",
       "  12.976375595786504,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.30065138478443243,\n",
       "  0.019989094582534062,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  6.701256710116031,\n",
       "  1,\n",
       "  3,\n",
       "  0.3,\n",
       "  0],\n",
       " [0.22891535062000545,\n",
       "  0.027565897006846529,\n",
       "  4,\n",
       "  0.6666666666666666,\n",
       "  3.6007003063261136,\n",
       "  0,\n",
       "  3,\n",
       "  0.6,\n",
       "  0],\n",
       " [0.43443670649046406,\n",
       "  0.036902992464048046,\n",
       "  6,\n",
       "  0.46153846153846156,\n",
       "  18.5217857213112,\n",
       "  1,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  0],\n",
       " [0.88171310648608925,\n",
       "  0.043682636633649241,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  12.248029745907012,\n",
       "  0,\n",
       "  2,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.80757120917066227,\n",
       "  0.097756396426914471,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  9.879498346509834,\n",
       "  0,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [1.1325975318535204,\n",
       "  0.027742162821072114,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  6.943870348597019,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.70944367357067706,\n",
       "  0.010914126413821945,\n",
       "  9,\n",
       "  0.75,\n",
       "  6.106288506256016,\n",
       "  1,\n",
       "  7,\n",
       "  0.5384615384615384,\n",
       "  0],\n",
       " [1.167728941776083,\n",
       "  0.089951558697320633,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  11.908070251803014,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69846033524977647,\n",
       "  0.16465487362610665,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  12.939794379140075,\n",
       "  1,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.79878927243719644,\n",
       "  0.23340246601103909,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  23.265705200718575,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1563852190748001,\n",
       "  0.071336726687781793,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  13.11244355806613,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.29189646402077063,\n",
       "  0.032541374312376359,\n",
       "  3,\n",
       "  0.375,\n",
       "  7.792287353971604,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.4005415333870902,\n",
       "  0.12706156310396588,\n",
       "  4,\n",
       "  0.5,\n",
       "  10.244018244152178,\n",
       "  1,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.26094447649096542,\n",
       "  0.015282683180509737,\n",
       "  4,\n",
       "  0.5,\n",
       "  6.134711618641294,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.65470285887089019,\n",
       "  0.020943762223723406,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  6.153786756241347,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.67658501809760208,\n",
       "  0.014395258767744945,\n",
       "  8,\n",
       "  0.5333333333333333,\n",
       "  8.573776499431496,\n",
       "  1,\n",
       "  4,\n",
       "  0.23529411764705882,\n",
       "  0],\n",
       " [1.3079173629068892,\n",
       "  0.14134651588916347,\n",
       "  3,\n",
       "  0.3,\n",
       "  20.38875923780978,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.54197836080561301,\n",
       "  0.016276742678178024,\n",
       "  4,\n",
       "  0.5,\n",
       "  4.627634499068728,\n",
       "  0,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.24146169187919952,\n",
       "  0.016472400206605675,\n",
       "  8,\n",
       "  0.8,\n",
       "  9.560646260589294,\n",
       "  1,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  0],\n",
       " [0.89069802692729128,\n",
       "  0.05773571421395618,\n",
       "  4,\n",
       "  0.4,\n",
       "  8.448909590301112,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.3328104847250009,\n",
       "  0.3091713042260642,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  20.640732647044622,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.96980802951367284,\n",
       "  0.01458309118292167,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  5.1325713521706104,\n",
       "  1,\n",
       "  2,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.63466307662204091,\n",
       "  0.086262706479503182,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  11.030572481122174,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.54793539448795803,\n",
       "  0.0062021577647011217,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  5.6888864207878616,\n",
       "  1,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [0.4271026298978523,\n",
       "  0.0291995817424342,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  7.114484578355163,\n",
       "  1,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.76184550390556516,\n",
       "  0.029841290721905445,\n",
       "  6,\n",
       "  0.4,\n",
       "  13.81687110598628,\n",
       "  0,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [0.50963208346776945,\n",
       "  0.052985395586813921,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  17.229407872591445,\n",
       "  1,\n",
       "  1,\n",
       "  0.1,\n",
       "  1],\n",
       " [0.43006955317770035,\n",
       "  0.066002331671447423,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  31.68833176140042,\n",
       "  1,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  0],\n",
       " [0.73010338402338337,\n",
       "  0.032464119118669754,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  6.5856200222925585,\n",
       "  1,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [1.2441516916572417,\n",
       "  0.033039175394613873,\n",
       "  6,\n",
       "  0.5,\n",
       "  11.695725889669964,\n",
       "  1,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  0],\n",
       " [0.29866366132226863,\n",
       "  0.099237913812532663,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  10.167516000238594,\n",
       "  0,\n",
       "  1,\n",
       "  0.09090909090909091,\n",
       "  0],\n",
       " [0.66145629513597548,\n",
       "  0.013620011496306317,\n",
       "  5,\n",
       "  0.625,\n",
       "  5.852704764660102,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.93455142064039642,\n",
       "  0.032715312751844028,\n",
       "  4,\n",
       "  0.5714285714285714,\n",
       "  8.080259744275581,\n",
       "  1,\n",
       "  2,\n",
       "  0.2857142857142857,\n",
       "  0],\n",
       " [0.73640651996329987,\n",
       "  0.010329261375383259,\n",
       "  7,\n",
       "  0.5,\n",
       "  32.09107155964151,\n",
       "  1,\n",
       "  5,\n",
       "  0.3125,\n",
       "  0],\n",
       " [0.7759017374812831,\n",
       "  0.084518833426545648,\n",
       "  4,\n",
       "  0.2857142857142857,\n",
       "  18.637180355715223,\n",
       "  0,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  0],\n",
       " [0.89327005177469909,\n",
       "  0.033304993120135684,\n",
       "  6,\n",
       "  0.4,\n",
       "  24.705311825007364,\n",
       "  1,\n",
       "  3,\n",
       "  0.1875,\n",
       "  0],\n",
       " [1.1812533315128328,\n",
       "  0.14487634102487723,\n",
       "  1,\n",
       "  0.1,\n",
       "  18.5413520265129,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.42526968085478134,\n",
       "  0.022724012626564094,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  6.967723689649956,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.76565373836868722,\n",
       "  0.07482577932659773,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  19.331941538685967,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.83908512470440322,\n",
       "  0.14608207297914622,\n",
       "  2,\n",
       "  0.125,\n",
       "  33.427939576202995,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.43766740397294135,\n",
       "  0.0045424974359351555,\n",
       "  9,\n",
       "  0.9,\n",
       "  5.37139098640585,\n",
       "  1,\n",
       "  7,\n",
       "  0.7,\n",
       "  0],\n",
       " [0.83783713747954924,\n",
       "  0.16240676572943202,\n",
       "  5,\n",
       "  0.35714285714285715,\n",
       "  31.314127928438815,\n",
       "  0,\n",
       "  2,\n",
       "  0.13333333333333333,\n",
       "  0],\n",
       " [0.54832334411868766,\n",
       "  0.092133685244343067,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  17.16143297664571,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.39044017015710886,\n",
       "  0.030975526673478138,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  9.29995810413091,\n",
       "  1,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  0],\n",
       " [0.60650274456086684,\n",
       "  0.24615577152192969,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  28.040072831001012,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.80271149427342725,\n",
       "  0.033905734822842426,\n",
       "  4,\n",
       "  0.5,\n",
       "  7.6076076453559605,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.82185273986105767,\n",
       "  0.051760367213122938,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  10.909513828612637,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.82779426194042305,\n",
       "  0.070867162589156929,\n",
       "  1,\n",
       "  0.1,\n",
       "  20.494057444884035,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53252207792745265,\n",
       "  0.013129168795565915,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  10.829852416245528,\n",
       "  1,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  0],\n",
       " [0.44636053015140498,\n",
       "  0.071318190659719227,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  12.083036854971208,\n",
       "  0,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  0],\n",
       " [0.19290085396411438,\n",
       "  0.017008159071715045,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  4.8677316240309505,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.67565678387018191,\n",
       "  0.023399985791878186,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  12.010933363683447,\n",
       "  0,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  0],\n",
       " [0.45396350467774282,\n",
       "  0.12333599191501099,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  25.213067361284818,\n",
       "  0,\n",
       "  1,\n",
       "  0.1,\n",
       "  0],\n",
       " [0.68144186426615749,\n",
       "  0.016042767345105902,\n",
       "  8,\n",
       "  0.5,\n",
       "  9.11897032021807,\n",
       "  1,\n",
       "  5,\n",
       "  0.29411764705882354,\n",
       "  1],\n",
       " [1.2218567214395641,\n",
       "  0.18432782237593814,\n",
       "  2,\n",
       "  0.14285714285714285,\n",
       "  25.672444149972133,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [1.1094945482667071,\n",
       "  0.017978273455984128,\n",
       "  6,\n",
       "  0.6,\n",
       "  9.447220852927117,\n",
       "  1,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  0],\n",
       " [0.7009778235577846,\n",
       "  0.01216157092491954,\n",
       "  5,\n",
       "  0.5,\n",
       "  5.962289053306257,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.0552961035145685,\n",
       "  0.027378478916388116,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  13.288701122046808,\n",
       "  0,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.76989707171018451,\n",
       "  0.081408273815231191,\n",
       "  3,\n",
       "  0.25,\n",
       "  17.396252937067462,\n",
       "  0,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.68076523182702975,\n",
       "  0.012932119823647836,\n",
       "  10,\n",
       "  0.6666666666666666,\n",
       "  10.09297457096021,\n",
       "  1,\n",
       "  6,\n",
       "  0.3333333333333333,\n",
       "  1],\n",
       " [0.88675130766747867,\n",
       "  0.016552100103902401,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  21.946372830794182,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [1.0093424868128142,\n",
       "  0.16354621334331898,\n",
       "  1,\n",
       "  0.07692307692307693,\n",
       "  17.254332450229192,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1428496671698936,\n",
       "  0.016398440094332001,\n",
       "  3,\n",
       "  0.3,\n",
       "  6.844282379638739,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.3276311497912519,\n",
       "  0.056469344723243498,\n",
       "  6,\n",
       "  0.6,\n",
       "  18.670003226317835,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [1.050093503139617,\n",
       "  0.031302276000274465,\n",
       "  7,\n",
       "  0.5833333333333334,\n",
       "  8.824784365572102,\n",
       "  1,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  0],\n",
       " [1.5458543154550721,\n",
       "  0.069875848533972107,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.223067462498705,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [0.84529146891089535,\n",
       "  0.02890453360090417,\n",
       "  6,\n",
       "  0.4,\n",
       "  16.390653281866452,\n",
       "  1,\n",
       "  3,\n",
       "  0.17647058823529413,\n",
       "  0],\n",
       " [0.71654123566845551,\n",
       "  0.10972295093339712,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  20.748595285863587,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.53875057493475098,\n",
       "  0.013866759780104099,\n",
       "  7,\n",
       "  0.6363636363636364,\n",
       "  6.846918381454385,\n",
       "  1,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  0],\n",
       " [1.3586311826129036,\n",
       "  0.12776525663102356,\n",
       "  0,\n",
       "  0.0,\n",
       "  11.17789000174984,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.87959502098332698,\n",
       "  0.094983301237103857,\n",
       "  4,\n",
       "  0.4,\n",
       "  21.523790830820293,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.50794971127745758,\n",
       "  0.0132485849307864,\n",
       "  7,\n",
       "  0.7777777777777778,\n",
       "  7.224759646598871,\n",
       "  1,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  0],\n",
       " [1.115368493886159,\n",
       "  0.022246044993990632,\n",
       "  7,\n",
       "  0.5384615384615384,\n",
       "  8.14402923553326,\n",
       "  1,\n",
       "  3,\n",
       "  0.2,\n",
       "  0],\n",
       " [1.3956989374256925,\n",
       "  0.18090481068212594,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  17.048165005417147,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.92220208062461484,\n",
       "  0.029707210930785211,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  10.777479111119147,\n",
       "  0,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  1],\n",
       " [0.69360566441426563,\n",
       "  0.036349518286078131,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  6.155207198795193,\n",
       "  1,\n",
       "  1,\n",
       "  0.1111111111111111,\n",
       "  0],\n",
       " [0.94191388690416467,\n",
       "  0.023509699166747944,\n",
       "  7,\n",
       "  0.4375,\n",
       "  16.44309188643865,\n",
       "  1,\n",
       "  3,\n",
       "  0.15,\n",
       "  0],\n",
       " [0.59236872925038053,\n",
       "  0.027686328672209326,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  5.735371599347514,\n",
       "  0,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [1.1598143694178478,\n",
       "  0.11109719522897665,\n",
       "  1,\n",
       "  0.1,\n",
       "  13.475104373663203,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2968895969191889,\n",
       "  0.037462389528214679,\n",
       "  5,\n",
       "  0.5,\n",
       "  10.82854218168519,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.20410679809407994,\n",
       "  0.14678282870994441,\n",
       "  0,\n",
       "  0.0,\n",
       "  11.532426144047113,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.43342450230754215,\n",
       "  0.008645419741994731,\n",
       "  6,\n",
       "  0.5454545454545454,\n",
       "  6.9712213416969115,\n",
       "  0,\n",
       "  5,\n",
       "  0.4166666666666667,\n",
       "  0],\n",
       " [0.099338006115701205,\n",
       "  0.0065310995679681572,\n",
       "  8,\n",
       "  0.6666666666666666,\n",
       "  4.559206879453916,\n",
       "  1,\n",
       "  6,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [1.0819265916644611,\n",
       "  0.075915438281880743,\n",
       "  1,\n",
       "  0.06666666666666667,\n",
       "  11.11071272062832,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.85670462731995367,\n",
       "  0.024015248811714018,\n",
       "  8,\n",
       "  0.47058823529411764,\n",
       "  10.577518704588167,\n",
       "  1,\n",
       "  4,\n",
       "  0.21052631578947367,\n",
       "  0],\n",
       " [0.66389958135840044,\n",
       "  0.070301504610626342,\n",
       "  4,\n",
       "  0.36363636363636365,\n",
       "  14.927948477991453,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.85921310074005375,\n",
       "  0.070285438221377672,\n",
       "  5,\n",
       "  0.5,\n",
       "  10.117797513384252,\n",
       "  1,\n",
       "  1,\n",
       "  0.08333333333333333,\n",
       "  0],\n",
       " [0.46010761245386145,\n",
       "  0.030487785439249793,\n",
       "  7,\n",
       "  0.7,\n",
       "  13.0395528316496,\n",
       "  1,\n",
       "  5,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.73957127656433352,\n",
       "  0.016934361261721653,\n",
       "  6,\n",
       "  0.6,\n",
       "  5.507830137681354,\n",
       "  1,\n",
       "  4,\n",
       "  0.4,\n",
       "  0],\n",
       " [0.28970432890201547,\n",
       "  0.033300838361723817,\n",
       "  4,\n",
       "  0.4,\n",
       "  10.344070065095373,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.28640545074890134,\n",
       "  0.030097664527671597,\n",
       "  3,\n",
       "  0.3,\n",
       "  10.002245471327756,\n",
       "  1,\n",
       "  2,\n",
       "  0.2,\n",
       "  0],\n",
       " [0.97565130284214452,\n",
       "  0.037923329818183471,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  10.70612863967956,\n",
       "  0,\n",
       "  2,\n",
       "  0.15384615384615385,\n",
       "  0],\n",
       " [0.43808184294238484,\n",
       "  0.13703466932058561,\n",
       "  3,\n",
       "  0.2727272727272727,\n",
       "  14.186728759560136,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  1],\n",
       " [0.98367932787918777,\n",
       "  0.04555357520888903,\n",
       "  4,\n",
       "  0.25,\n",
       "  11.834965496991044,\n",
       "  1,\n",
       "  1,\n",
       "  0.058823529411764705,\n",
       "  0],\n",
       " [0.2113199657499778,\n",
       "  0.019398685770997237,\n",
       "  8,\n",
       "  0.6666666666666666,\n",
       "  9.2886283435741,\n",
       "  1,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  0],\n",
       " [0.47165226864669652,\n",
       "  0.065292532887056831,\n",
       "  4,\n",
       "  0.4444444444444444,\n",
       "  8.862537602110844,\n",
       "  1,\n",
       "  2,\n",
       "  0.2222222222222222,\n",
       "  0],\n",
       " [1.1464283323681028,\n",
       "  0.0756027968001588,\n",
       "  2,\n",
       "  0.2,\n",
       "  8.880238417426968,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.69553796723819161,\n",
       "  0.029612427870929214,\n",
       "  6,\n",
       "  0.42857142857142855,\n",
       "  28.40025510506003,\n",
       "  1,\n",
       "  2,\n",
       "  0.11764705882352941,\n",
       "  0],\n",
       " [1.2386747348475302,\n",
       "  0.042988091137207696,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  12.924969612399279,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.2710423731866176,\n",
       "  0.029118820628994735,\n",
       "  3,\n",
       "  0.3,\n",
       "  8.948981742786076,\n",
       "  1,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [1.1184259854712726,\n",
       "  0.010646836273408233,\n",
       "  6,\n",
       "  0.6666666666666666,\n",
       "  5.723017356184585,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [1.0601013574417821,\n",
       "  0.039807149696027944,\n",
       "  5,\n",
       "  0.38461538461538464,\n",
       "  19.07525838528204,\n",
       "  1,\n",
       "  3,\n",
       "  0.23076923076923078,\n",
       "  0],\n",
       " [1.3139621430012729,\n",
       "  0.058379682205605521,\n",
       "  3,\n",
       "  0.21428571428571427,\n",
       "  14.818328081367097,\n",
       "  0,\n",
       "  1,\n",
       "  0.0625,\n",
       "  0],\n",
       " [0.76209500141506403,\n",
       "  0.018654167430294244,\n",
       "  6,\n",
       "  0.5,\n",
       "  12.059468658502555,\n",
       "  1,\n",
       "  4,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [1.1168667336318847,\n",
       "  0.044249455560674944,\n",
       "  1,\n",
       "  0.16666666666666666,\n",
       "  5.6114386318301275,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.18233370477462607,\n",
       "  0.0051428279267015098,\n",
       "  5,\n",
       "  0.7142857142857143,\n",
       "  2.750202175973526,\n",
       "  1,\n",
       "  3,\n",
       "  0.42857142857142855,\n",
       "  0],\n",
       " [0.41888046136035173,\n",
       "  0.075796312000708133,\n",
       "  5,\n",
       "  0.45454545454545453,\n",
       "  21.51616698819091,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " [0.28704168538344688,\n",
       "  0.0046054301137737896,\n",
       "  6,\n",
       "  0.75,\n",
       "  3.0965932096066484,\n",
       "  1,\n",
       "  4,\n",
       "  0.5,\n",
       "  0],\n",
       " [0.30893808969564329,\n",
       "  0.069057985013851297,\n",
       "  5,\n",
       "  0.5,\n",
       "  12.38167861007174,\n",
       "  1,\n",
       "  2,\n",
       "  0.18181818181818182,\n",
       "  0],\n",
       " [0.37578590582083282,\n",
       "  0.037111234355929223,\n",
       "  6,\n",
       "  0.6,\n",
       "  15.04343007388362,\n",
       "  1,\n",
       "  3,\n",
       "  0.25,\n",
       "  0],\n",
       " [0.63145020183073874,\n",
       "  0.035970133798289949,\n",
       "  5,\n",
       "  0.5555555555555556,\n",
       "  10.724740240427373,\n",
       "  1,\n",
       "  3,\n",
       "  0.3333333333333333,\n",
       "  0],\n",
       " [0.48790621312420124,\n",
       "  0.19849262729835104,\n",
       "  1,\n",
       "  0.1,\n",
       "  14.976328358206967,\n",
       "  0,\n",
       "  0,\n",
       "  0.0,\n",
       "  0],\n",
       " [0.44688061297324799,\n",
       "  0.042143677654457479,\n",
       "  6,\n",
       "  0.6,\n",
       "  11.598944818820113,\n",
       "  1,\n",
       "  2,\n",
       "  0.16666666666666666,\n",
       "  0],\n",
       " ...]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "3\n",
      "4\n",
      "9\n",
      "4\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "7\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "9\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "7\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "3\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "7\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "8\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "8\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "3\n",
      "12\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "7\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "7\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "9\n",
      "7\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "8\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "7\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "7\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "8\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "7\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "8\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for t in test:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing features: 0\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.551\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.546     0.874     0.672      1560\n",
      "          1      0.576     0.191     0.287      1401\n",
      "\n",
      "avg / total      0.560     0.551     0.490      2961\n",
      "\n",
      "Testing features: 1\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.654\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.633     0.817     0.713      1560\n",
      "          1      0.699     0.473     0.564      1401\n",
      "\n",
      "avg / total      0.664     0.654     0.643      2961\n",
      "\n",
      "Testing features: 2\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.613     0.969     0.751      1560\n",
      "          1      0.902     0.320     0.473      1401\n",
      "\n",
      "avg / total      0.750     0.662     0.620      2961\n",
      "\n",
      "Testing features: 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.627     0.885     0.734      1560\n",
      "          1      0.764     0.413     0.536      1401\n",
      "\n",
      "avg / total      0.692     0.662     0.640      2961\n",
      "\n",
      "Testing features: 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.929     0.797      1560\n",
      "          1      0.876     0.552     0.677      1401\n",
      "\n",
      "avg / total      0.782     0.751     0.741      2961\n",
      "\n",
      "Testing features: 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.597     0.900     0.718      1560\n",
      "          1      0.745     0.325     0.452      1401\n",
      "\n",
      "avg / total      0.667     0.628     0.592      2961\n",
      "\n",
      "Testing features: 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.620     0.829     0.709      1560\n",
      "          1      0.695     0.433     0.534      1401\n",
      "\n",
      "avg / total      0.655     0.642     0.626      2961\n",
      "\n",
      "Testing features: 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.559\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.253     0.377      1560\n",
      "          1      0.519     0.899     0.658      1401\n",
      "\n",
      "avg / total      0.633     0.559     0.510      2961\n",
      "\n",
      "Testing features: 0, 1\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.655\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.634     0.815     0.713      1560\n",
      "          1      0.698     0.476     0.566      1401\n",
      "\n",
      "avg / total      0.664     0.655     0.643      2961\n",
      "\n",
      "Testing features: 0, 2\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.674\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.630     0.923     0.749      1560\n",
      "          1      0.822     0.396     0.535      1401\n",
      "\n",
      "avg / total      0.721     0.674     0.648      2961\n",
      "\n",
      "Testing features: 0, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.664\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.629     0.879     0.734      1560\n",
      "          1      0.759     0.423     0.544      1401\n",
      "\n",
      "avg / total      0.691     0.664     0.644      2961\n",
      "\n",
      "Testing features: 0, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.918     0.795      1560\n",
      "          1      0.861     0.565     0.682      1401\n",
      "\n",
      "avg / total      0.777     0.751     0.742      2961\n",
      "\n",
      "Testing features: 0, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.641\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.610     0.883     0.722      1560\n",
      "          1      0.740     0.372     0.495      1401\n",
      "\n",
      "avg / total      0.672     0.641     0.614      2961\n",
      "\n",
      "Testing features: 0, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.641\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.613     0.866     0.718      1560\n",
      "          1      0.724     0.391     0.508      1401\n",
      "\n",
      "avg / total      0.665     0.641     0.619      2961\n",
      "\n",
      "Testing features: 0, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.563\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.557     0.837     0.669      1560\n",
      "          1      0.587     0.259     0.360      1401\n",
      "\n",
      "avg / total      0.571     0.563     0.522      2961\n",
      "\n",
      "Testing features: 1, 2\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.852     0.785      1560\n",
      "          1      0.797     0.647     0.714      1401\n",
      "\n",
      "avg / total      0.761     0.755     0.752      2961\n",
      "\n",
      "Testing features: 1, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.647     0.852     0.736      1560\n",
      "          1      0.746     0.483     0.586      1401\n",
      "\n",
      "avg / total      0.694     0.677     0.665      2961\n",
      "\n",
      "Testing features: 1, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.869     0.832      1560\n",
      "          1      0.838     0.754     0.794      1401\n",
      "\n",
      "avg / total      0.817     0.815     0.814      2961\n",
      "\n",
      "Testing features: 1, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.808     0.754      1560\n",
      "          1      0.746     0.625     0.680      1401\n",
      "\n",
      "avg / total      0.725     0.722     0.719      2961\n",
      "\n",
      "Testing features: 1, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.697\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.671     0.837     0.744      1560\n",
      "          1      0.749     0.542     0.629      1401\n",
      "\n",
      "avg / total      0.708     0.697     0.690      2961\n",
      "\n",
      "Testing features: 1, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.651\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.631     0.814     0.711      1560\n",
      "          1      0.694     0.469     0.560      1401\n",
      "\n",
      "avg / total      0.660     0.651     0.639      2961\n",
      "\n",
      "Testing features: 2, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.684     0.913     0.782      1560\n",
      "          1      0.845     0.531     0.652      1401\n",
      "\n",
      "avg / total      0.761     0.732     0.721      2961\n",
      "\n",
      "Testing features: 2, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.776\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.901     0.809      1560\n",
      "          1      0.852     0.638     0.730      1401\n",
      "\n",
      "avg / total      0.790     0.776     0.772      2961\n",
      "\n",
      "Testing features: 2, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.680\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.919     0.751      1560\n",
      "          1      0.820     0.413     0.550      1401\n",
      "\n",
      "avg / total      0.723     0.680     0.656      2961\n",
      "\n",
      "Testing features: 2, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.694\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.650     0.908     0.758      1560\n",
      "          1      0.817     0.455     0.584      1401\n",
      "\n",
      "avg / total      0.729     0.694     0.675      2961\n",
      "\n",
      "Testing features: 2, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.674\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.671     0.747     0.707      1560\n",
      "          1      0.678     0.592     0.632      1401\n",
      "\n",
      "avg / total      0.674     0.674     0.672      2961\n",
      "\n",
      "Testing features: 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.764     0.903     0.828      1560\n",
      "          1      0.864     0.690     0.767      1401\n",
      "\n",
      "avg / total      0.811     0.802     0.799      2961\n",
      "\n",
      "Testing features: 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.708\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.677     0.853     0.755      1560\n",
      "          1      0.769     0.547     0.640      1401\n",
      "\n",
      "avg / total      0.721     0.708     0.700      2961\n",
      "\n",
      "Testing features: 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.661     0.867     0.750      1560\n",
      "          1      0.773     0.504     0.610      1401\n",
      "\n",
      "avg / total      0.714     0.695     0.684      2961\n",
      "\n",
      "Testing features: 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.663\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.850     0.727      1560\n",
      "          1      0.732     0.455     0.561      1401\n",
      "\n",
      "avg / total      0.681     0.663     0.649      2961\n",
      "\n",
      "Testing features: 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.924     0.801      1560\n",
      "          1      0.872     0.573     0.692      1401\n",
      "\n",
      "avg / total      0.785     0.758     0.749      2961\n",
      "\n",
      "Testing features: 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.915     0.805      1560\n",
      "          1      0.863     0.600     0.708      1401\n",
      "\n",
      "avg / total      0.787     0.766     0.759      2961\n",
      "\n",
      "Testing features: 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.929     0.797      1560\n",
      "          1      0.876     0.552     0.677      1401\n",
      "\n",
      "avg / total      0.782     0.751     0.741      2961\n",
      "\n",
      "Testing features: 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.638     0.886     0.742      1560\n",
      "          1      0.776     0.440     0.562      1401\n",
      "\n",
      "avg / total      0.703     0.675     0.657      2961\n",
      "\n",
      "Testing features: 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.605     0.851     0.707      1560\n",
      "          1      0.696     0.380     0.492      1401\n",
      "\n",
      "avg / total      0.648     0.628     0.605      2961\n",
      "\n",
      "Testing features: 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.646\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.626     0.815     0.708      1560\n",
      "          1      0.690     0.458     0.551      1401\n",
      "\n",
      "avg / total      0.657     0.646     0.634      2961\n",
      "\n",
      "Testing features: 0, 1, 2\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.846     0.779      1560\n",
      "          1      0.788     0.637     0.704      1401\n",
      "\n",
      "avg / total      0.753     0.747     0.744      2961\n",
      "\n",
      "Testing features: 0, 1, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.655     0.847     0.739      1560\n",
      "          1      0.747     0.503     0.601      1401\n",
      "\n",
      "avg / total      0.698     0.684     0.674      2961\n",
      "\n",
      "Testing features: 0, 1, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.768     0.879     0.820      1560\n",
      "          1      0.839     0.705     0.766      1401\n",
      "\n",
      "avg / total      0.802     0.797     0.795      2961\n",
      "\n",
      "Testing features: 0, 1, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.717\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.797     0.748      1560\n",
      "          1      0.735     0.628     0.677      1401\n",
      "\n",
      "avg / total      0.719     0.717     0.715      2961\n",
      "\n",
      "Testing features: 0, 1, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.699\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.677     0.823     0.743      1560\n",
      "          1      0.740     0.562     0.639      1401\n",
      "\n",
      "avg / total      0.707     0.699     0.693      2961\n",
      "\n",
      "Testing features: 0, 1, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.639     0.797     0.709      1560\n",
      "          1      0.688     0.499     0.578      1401\n",
      "\n",
      "avg / total      0.662     0.656     0.647      2961\n",
      "\n",
      "Testing features: 0, 2, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.687     0.915     0.785      1560\n",
      "          1      0.851     0.536     0.658      1401\n",
      "\n",
      "avg / total      0.764     0.736     0.725      2961\n",
      "\n",
      "Testing features: 0, 2, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.782\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.893     0.812      1560\n",
      "          1      0.847     0.658     0.741      1401\n",
      "\n",
      "avg / total      0.793     0.782     0.778      2961\n",
      "\n",
      "Testing features: 0, 2, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.647     0.887     0.748      1560\n",
      "          1      0.786     0.460     0.581      1401\n",
      "\n",
      "avg / total      0.712     0.685     0.669      2961\n",
      "\n",
      "Testing features: 0, 2, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.652     0.903     0.758      1560\n",
      "          1      0.811     0.464     0.590      1401\n",
      "\n",
      "avg / total      0.728     0.695     0.678      2961\n",
      "\n",
      "Testing features: 0, 2, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.681\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.640     0.900     0.748      1560\n",
      "          1      0.797     0.437     0.564      1401\n",
      "\n",
      "avg / total      0.714     0.681     0.661      2961\n",
      "\n",
      "Testing features: 0, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.899     0.831      1560\n",
      "          1      0.863     0.704     0.775      1401\n",
      "\n",
      "avg / total      0.815     0.807     0.804      2961\n",
      "\n",
      "Testing features: 0, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.707\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.677     0.850     0.754      1560\n",
      "          1      0.766     0.548     0.639      1401\n",
      "\n",
      "avg / total      0.719     0.707     0.699      2961\n",
      "\n",
      "Testing features: 0, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.700\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.666     0.864     0.752      1560\n",
      "          1      0.774     0.517     0.620      1401\n",
      "\n",
      "avg / total      0.717     0.700     0.689      2961\n",
      "\n",
      "Testing features: 0, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.636     0.861     0.731      1560\n",
      "          1      0.744     0.451     0.562      1401\n",
      "\n",
      "avg / total      0.687     0.667     0.651      2961\n",
      "\n",
      "Testing features: 0, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.717     0.905     0.800      1560\n",
      "          1      0.851     0.602     0.705      1401\n",
      "\n",
      "avg / total      0.780     0.762     0.755      2961\n",
      "\n",
      "Testing features: 0, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.909     0.803      1560\n",
      "          1      0.857     0.606     0.710      1401\n",
      "\n",
      "avg / total      0.785     0.766     0.759      2961\n",
      "\n",
      "Testing features: 0, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.909     0.795      1560\n",
      "          1      0.851     0.578     0.688      1401\n",
      "\n",
      "avg / total      0.774     0.752     0.744      2961\n",
      "\n",
      "Testing features: 0, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.689\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.656     0.860     0.744      1560\n",
      "          1      0.761     0.498     0.602      1401\n",
      "\n",
      "avg / total      0.706     0.689     0.677      2961\n",
      "\n",
      "Testing features: 0, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.618     0.872     0.724      1560\n",
      "          1      0.738     0.400     0.519      1401\n",
      "\n",
      "avg / total      0.675     0.649     0.627      2961\n",
      "\n",
      "Testing features: 0, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.651\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.629     0.827     0.714      1560\n",
      "          1      0.703     0.456     0.553      1401\n",
      "\n",
      "avg / total      0.664     0.651     0.638      2961\n",
      "\n",
      "Testing features: 1, 2, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.730     0.890     0.802      1560\n",
      "          1      0.838     0.633     0.721      1401\n",
      "\n",
      "avg / total      0.781     0.768     0.764      2961\n",
      "\n",
      "Testing features: 1, 2, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.879     0.829      1560\n",
      "          1      0.845     0.729     0.783      1401\n",
      "\n",
      "avg / total      0.812     0.809     0.807      2961\n",
      "\n",
      "Testing features: 1, 2, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.750\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.721     0.856     0.783      1560\n",
      "          1      0.798     0.631     0.705      1401\n",
      "\n",
      "avg / total      0.757     0.750     0.746      2961\n",
      "\n",
      "Testing features: 1, 2, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.869     0.789      1560\n",
      "          1      0.811     0.629     0.708      1401\n",
      "\n",
      "avg / total      0.765     0.755     0.751      2961\n",
      "\n",
      "Testing features: 1, 2, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.738     0.838     0.785      1560\n",
      "          1      0.788     0.668     0.723      1401\n",
      "\n",
      "avg / total      0.761     0.758     0.756      2961\n",
      "\n",
      "Testing features: 1, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.883     0.831      1560\n",
      "          1      0.849     0.730     0.785      1401\n",
      "\n",
      "avg / total      0.815     0.811     0.809      2961\n",
      "\n",
      "Testing features: 1, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.840     0.766      1560\n",
      "          1      0.773     0.605     0.679      1401\n",
      "\n",
      "avg / total      0.736     0.729     0.725      2961\n",
      "\n",
      "Testing features: 1, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.710\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.681     0.847     0.755      1560\n",
      "          1      0.766     0.558     0.646      1401\n",
      "\n",
      "avg / total      0.721     0.710     0.703      2961\n",
      "\n",
      "Testing features: 1, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.664     0.847     0.744      1560\n",
      "          1      0.754     0.522     0.617      1401\n",
      "\n",
      "avg / total      0.707     0.693     0.684      2961\n",
      "\n",
      "Testing features: 1, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.765     0.880     0.819      1560\n",
      "          1      0.840     0.700     0.763      1401\n",
      "\n",
      "avg / total      0.801     0.795     0.792      2961\n",
      "\n",
      "Testing features: 1, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.875     0.820      1560\n",
      "          1      0.836     0.712     0.769      1401\n",
      "\n",
      "avg / total      0.802     0.798     0.796      2961\n",
      "\n",
      "Testing features: 1, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.863     0.834      1560\n",
      "          1      0.835     0.768     0.800      1401\n",
      "\n",
      "avg / total      0.819     0.818     0.818      2961\n",
      "\n",
      "Testing features: 1, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.803     0.767      1560\n",
      "          1      0.755     0.677     0.714      1401\n",
      "\n",
      "avg / total      0.744     0.743     0.742      2961\n",
      "\n",
      "Testing features: 1, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.730\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.801     0.757      1560\n",
      "          1      0.746     0.651     0.695      1401\n",
      "\n",
      "avg / total      0.731     0.730     0.728      2961\n",
      "\n",
      "Testing features: 1, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.715\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.694     0.820     0.752      1560\n",
      "          1      0.749     0.598     0.665      1401\n",
      "\n",
      "avg / total      0.720     0.715     0.711      2961\n",
      "\n",
      "Testing features: 2, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.896     0.832      1560\n",
      "          1      0.860     0.712     0.779      1401\n",
      "\n",
      "avg / total      0.816     0.809     0.807      2961\n",
      "\n",
      "Testing features: 2, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.917     0.789      1560\n",
      "          1      0.855     0.547     0.667      1401\n",
      "\n",
      "avg / total      0.769     0.742     0.731      2961\n",
      "\n",
      "Testing features: 2, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.690     0.920     0.788      1560\n",
      "          1      0.858     0.540     0.663      1401\n",
      "\n",
      "avg / total      0.769     0.740     0.729      2961\n",
      "\n",
      "Testing features: 2, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.922     0.791      1560\n",
      "          1      0.863     0.542     0.666      1401\n",
      "\n",
      "avg / total      0.773     0.743     0.732      2961\n",
      "\n",
      "Testing features: 2, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.908     0.811      1560\n",
      "          1      0.860     0.633     0.729      1401\n",
      "\n",
      "avg / total      0.794     0.778     0.773      2961\n",
      "\n",
      "Testing features: 2, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.896     0.812      1560\n",
      "          1      0.849     0.653     0.738      1401\n",
      "\n",
      "avg / total      0.792     0.781     0.777      2961\n",
      "\n",
      "Testing features: 2, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.907     0.811      1560\n",
      "          1      0.859     0.632     0.728      1401\n",
      "\n",
      "avg / total      0.793     0.777     0.772      2961\n",
      "\n",
      "Testing features: 2, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.694     0.888     0.779      1560\n",
      "          1      0.820     0.564     0.668      1401\n",
      "\n",
      "avg / total      0.753     0.735     0.727      2961\n",
      "\n",
      "Testing features: 2, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.690\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.655     0.872     0.748      1560\n",
      "          1      0.774     0.488     0.599      1401\n",
      "\n",
      "avg / total      0.711     0.690     0.677      2961\n",
      "\n",
      "Testing features: 2, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.702\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.662     0.887     0.758      1560\n",
      "          1      0.797     0.496     0.612      1401\n",
      "\n",
      "avg / total      0.726     0.702     0.689      2961\n",
      "\n",
      "Testing features: 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.890     0.831      1560\n",
      "          1      0.854     0.719     0.781      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.894     0.831      1560\n",
      "          1      0.857     0.712     0.778      1401\n",
      "\n",
      "avg / total      0.814     0.808     0.806      2961\n",
      "\n",
      "Testing features: 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.769     0.895     0.827      1560\n",
      "          1      0.857     0.700     0.771      1401\n",
      "\n",
      "avg / total      0.810     0.803     0.800      2961\n",
      "\n",
      "Testing features: 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.696     0.873     0.775      1560\n",
      "          1      0.803     0.576     0.671      1401\n",
      "\n",
      "avg / total      0.747     0.733     0.726      2961\n",
      "\n",
      "Testing features: 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.675     0.872     0.761      1560\n",
      "          1      0.789     0.532     0.635      1401\n",
      "\n",
      "avg / total      0.729     0.711     0.702      2961\n",
      "\n",
      "Testing features: 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.710\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.678     0.855     0.756      1560\n",
      "          1      0.773     0.548     0.641      1401\n",
      "\n",
      "avg / total      0.723     0.710     0.702      2961\n",
      "\n",
      "Testing features: 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.917     0.805      1560\n",
      "          1      0.866     0.598     0.707      1401\n",
      "\n",
      "avg / total      0.788     0.766     0.759      2961\n",
      "\n",
      "Testing features: 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.853     0.784      1560\n",
      "          1      0.796     0.642     0.711      1401\n",
      "\n",
      "avg / total      0.759     0.753     0.749      2961\n",
      "\n",
      "Testing features: 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.910     0.804      1560\n",
      "          1      0.859     0.607     0.711      1401\n",
      "\n",
      "avg / total      0.786     0.767     0.760      2961\n",
      "\n",
      "Testing features: 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.655     0.846     0.738      1560\n",
      "          1      0.746     0.504     0.601      1401\n",
      "\n",
      "avg / total      0.698     0.684     0.673      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.738     0.885     0.805      1560\n",
      "          1      0.836     0.650     0.731      1401\n",
      "\n",
      "avg / total      0.784     0.774     0.770      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.879     0.825      1560\n",
      "          1      0.842     0.721     0.777      1401\n",
      "\n",
      "avg / total      0.809     0.804     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.841     0.776      1560\n",
      "          1      0.782     0.636     0.702      1401\n",
      "\n",
      "avg / total      0.750     0.744     0.741      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.856     0.777      1560\n",
      "          1      0.793     0.612     0.691      1401\n",
      "\n",
      "avg / total      0.750     0.741     0.736      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.847     0.791      1560\n",
      "          1      0.798     0.670     0.728      1401\n",
      "\n",
      "avg / total      0.768     0.764     0.761      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.890     0.834      1560\n",
      "          1      0.857     0.729     0.788      1401\n",
      "\n",
      "avg / total      0.819     0.814     0.812      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.821     0.761      1560\n",
      "          1      0.758     0.625     0.685      1401\n",
      "\n",
      "avg / total      0.732     0.728     0.725      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.835     0.755      1560\n",
      "          1      0.759     0.580     0.657      1401\n",
      "\n",
      "avg / total      0.722     0.714     0.709      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.692\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.835     0.741      1560\n",
      "          1      0.744     0.532     0.620      1401\n",
      "\n",
      "avg / total      0.702     0.692     0.684      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.872     0.821      1560\n",
      "          1      0.835     0.719     0.773      1401\n",
      "\n",
      "avg / total      0.804     0.800     0.798      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.877     0.821      1560\n",
      "          1      0.839     0.712     0.770      1401\n",
      "\n",
      "avg / total      0.803     0.799     0.797      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.867     0.827      1560\n",
      "          1      0.834     0.742     0.785      1401\n",
      "\n",
      "avg / total      0.810     0.808     0.807      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.798     0.769      1560\n",
      "          1      0.754     0.690     0.720      1401\n",
      "\n",
      "avg / total      0.747     0.747     0.746      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.802     0.757      1560\n",
      "          1      0.746     0.649     0.694      1401\n",
      "\n",
      "avg / total      0.731     0.729     0.728      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.816     0.747      1560\n",
      "          1      0.742     0.590     0.658      1401\n",
      "\n",
      "avg / total      0.714     0.709     0.705      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.896     0.836      1560\n",
      "          1      0.862     0.726     0.788      1401\n",
      "\n",
      "avg / total      0.821     0.815     0.813      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.901     0.790      1560\n",
      "          1      0.840     0.576     0.683      1401\n",
      "\n",
      "avg / total      0.768     0.747     0.739      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.910     0.790      1560\n",
      "          1      0.849     0.561     0.676      1401\n",
      "\n",
      "avg / total      0.769     0.745     0.736      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.694     0.913     0.789      1560\n",
      "          1      0.851     0.552     0.670      1401\n",
      "\n",
      "avg / total      0.769     0.743     0.733      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.893     0.808      1560\n",
      "          1      0.845     0.648     0.733      1401\n",
      "\n",
      "avg / total      0.789     0.777     0.773      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.783\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.908     0.815      1560\n",
      "          1      0.863     0.642     0.736      1401\n",
      "\n",
      "avg / total      0.798     0.783     0.778      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.896     0.810      1560\n",
      "          1      0.848     0.647     0.734      1401\n",
      "\n",
      "avg / total      0.791     0.778     0.774      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.695     0.883     0.778      1560\n",
      "          1      0.813     0.570     0.670      1401\n",
      "\n",
      "avg / total      0.751     0.735     0.727      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.662     0.863     0.749      1560\n",
      "          1      0.769     0.510     0.613      1401\n",
      "\n",
      "avg / total      0.713     0.696     0.685      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.699\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.660     0.883     0.756      1560\n",
      "          1      0.791     0.495     0.609      1401\n",
      "\n",
      "avg / total      0.722     0.699     0.686      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.883     0.830      1560\n",
      "          1      0.848     0.726     0.782      1401\n",
      "\n",
      "avg / total      0.813     0.809     0.807      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.896     0.833      1560\n",
      "          1      0.860     0.716     0.781      1401\n",
      "\n",
      "avg / total      0.817     0.811     0.809      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.771     0.895     0.828      1560\n",
      "          1      0.857     0.704     0.773      1401\n",
      "\n",
      "avg / total      0.812     0.804     0.802      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.704     0.871     0.779      1560\n",
      "          1      0.805     0.592     0.682      1401\n",
      "\n",
      "avg / total      0.752     0.739     0.733      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.708\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.675     0.858     0.756      1560\n",
      "          1      0.774     0.540     0.636      1401\n",
      "\n",
      "avg / total      0.722     0.708     0.699      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.675     0.854     0.754      1560\n",
      "          1      0.770     0.541     0.635      1401\n",
      "\n",
      "avg / total      0.720     0.706     0.698      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.908     0.805      1560\n",
      "          1      0.856     0.612     0.714      1401\n",
      "\n",
      "avg / total      0.786     0.768     0.762      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.886     0.797      1560\n",
      "          1      0.831     0.624     0.713      1401\n",
      "\n",
      "avg / total      0.774     0.762     0.757      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.905     0.804      1560\n",
      "          1      0.853     0.613     0.713      1401\n",
      "\n",
      "avg / total      0.784     0.767     0.761      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.853     0.747      1560\n",
      "          1      0.760     0.521     0.618      1401\n",
      "\n",
      "avg / total      0.710     0.696     0.686      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.872     0.834      1560\n",
      "          1      0.842     0.754     0.796      1401\n",
      "\n",
      "avg / total      0.819     0.817     0.816      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.889     0.801      1560\n",
      "          1      0.836     0.632     0.720      1401\n",
      "\n",
      "avg / total      0.780     0.767     0.763      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.895     0.810      1560\n",
      "          1      0.847     0.649     0.735      1401\n",
      "\n",
      "avg / total      0.790     0.778     0.774      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.776\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.874     0.804      1560\n",
      "          1      0.826     0.667     0.738      1401\n",
      "\n",
      "avg / total      0.783     0.776     0.773      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.878     0.825      1560\n",
      "          1      0.842     0.722     0.777      1401\n",
      "\n",
      "avg / total      0.808     0.804     0.802      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.883     0.828      1560\n",
      "          1      0.847     0.721     0.779      1401\n",
      "\n",
      "avg / total      0.811     0.806     0.805      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.880     0.832      1560\n",
      "          1      0.847     0.737     0.788      1401\n",
      "\n",
      "avg / total      0.816     0.813     0.811      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.871     0.792      1560\n",
      "          1      0.815     0.633     0.713      1401\n",
      "\n",
      "avg / total      0.768     0.759     0.754      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.752     0.835     0.792      1560\n",
      "          1      0.791     0.694     0.739      1401\n",
      "\n",
      "avg / total      0.771     0.768     0.767      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.847     0.792      1560\n",
      "          1      0.799     0.675     0.731      1401\n",
      "\n",
      "avg / total      0.770     0.766     0.763      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.880     0.836      1560\n",
      "          1      0.849     0.748     0.795      1401\n",
      "\n",
      "avg / total      0.821     0.818     0.816      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.884     0.837      1560\n",
      "          1      0.852     0.744     0.795      1401\n",
      "\n",
      "avg / total      0.821     0.818     0.817      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.882     0.833      1560\n",
      "          1      0.849     0.739     0.790      1401\n",
      "\n",
      "avg / total      0.818     0.814     0.813      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.841     0.790      1560\n",
      "          1      0.794     0.680     0.733      1401\n",
      "\n",
      "avg / total      0.768     0.765     0.763      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.847     0.770      1560\n",
      "          1      0.781     0.605     0.682      1401\n",
      "\n",
      "avg / total      0.741     0.733     0.728      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.724\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.860     0.767      1560\n",
      "          1      0.786     0.573     0.663      1401\n",
      "\n",
      "avg / total      0.736     0.724     0.717      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.863     0.827      1560\n",
      "          1      0.831     0.752     0.789      1401\n",
      "\n",
      "avg / total      0.812     0.810     0.809      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.874     0.828      1560\n",
      "          1      0.839     0.735     0.784      1401\n",
      "\n",
      "avg / total      0.811     0.808     0.807      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.870     0.832      1560\n",
      "          1      0.839     0.754     0.794      1401\n",
      "\n",
      "avg / total      0.817     0.815     0.814      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.811     0.776      1560\n",
      "          1      0.766     0.690     0.726      1401\n",
      "\n",
      "avg / total      0.755     0.754     0.753      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.891     0.833      1560\n",
      "          1      0.856     0.724     0.785      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.810      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.897     0.835      1560\n",
      "          1      0.862     0.719     0.784      1401\n",
      "\n",
      "avg / total      0.819     0.813     0.811      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.892     0.830      1560\n",
      "          1      0.855     0.713     0.778      1401\n",
      "\n",
      "avg / total      0.813     0.807     0.805      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.910     0.793      1560\n",
      "          1      0.851     0.570     0.683      1401\n",
      "\n",
      "avg / total      0.773     0.749     0.741      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.905     0.788      1560\n",
      "          1      0.842     0.562     0.674      1401\n",
      "\n",
      "avg / total      0.765     0.743     0.734      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.691     0.920     0.789      1560\n",
      "          1      0.859     0.542     0.664      1401\n",
      "\n",
      "avg / total      0.770     0.741     0.730      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.896     0.833      1560\n",
      "          1      0.861     0.714     0.781      1401\n",
      "\n",
      "avg / total      0.817     0.810     0.808      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.913     0.814      1560\n",
      "          1      0.867     0.632     0.731      1401\n",
      "\n",
      "avg / total      0.797     0.780     0.774      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.747     0.894     0.814      1560\n",
      "          1      0.848     0.663     0.744      1401\n",
      "\n",
      "avg / total      0.795     0.785     0.781      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.896     0.784      1560\n",
      "          1      0.829     0.566     0.673      1401\n",
      "\n",
      "avg / total      0.760     0.740     0.731      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.893     0.833      1560\n",
      "          1      0.858     0.721     0.784      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.810      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.890     0.831      1560\n",
      "          1      0.855     0.718     0.780      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.893     0.832      1560\n",
      "          1      0.858     0.718     0.782      1401\n",
      "\n",
      "avg / total      0.816     0.810     0.808      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.885     0.784      1560\n",
      "          1      0.820     0.585     0.682      1401\n",
      "\n",
      "avg / total      0.758     0.743     0.736      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.906     0.805      1560\n",
      "          1      0.854     0.616     0.716      1401\n",
      "\n",
      "avg / total      0.786     0.769     0.763      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.814     0.876     0.844      1560\n",
      "          1      0.849     0.777     0.811      1401\n",
      "\n",
      "avg / total      0.831     0.829     0.828      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.877     0.801      1560\n",
      "          1      0.826     0.651     0.728      1401\n",
      "\n",
      "avg / total      0.779     0.770     0.766      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.883     0.805      1560\n",
      "          1      0.833     0.654     0.733      1401\n",
      "\n",
      "avg / total      0.784     0.774     0.771      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.746     0.868     0.802      1560\n",
      "          1      0.820     0.671     0.738      1401\n",
      "\n",
      "avg / total      0.781     0.775     0.772      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.874     0.824      1560\n",
      "          1      0.838     0.724     0.777      1401\n",
      "\n",
      "avg / total      0.807     0.803     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.878     0.825      1560\n",
      "          1      0.841     0.722     0.777      1401\n",
      "\n",
      "avg / total      0.808     0.804     0.803      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.879     0.829      1560\n",
      "          1      0.844     0.732     0.784      1401\n",
      "\n",
      "avg / total      0.813     0.810     0.808      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.851     0.788      1560\n",
      "          1      0.798     0.655     0.720      1401\n",
      "\n",
      "avg / total      0.764     0.759     0.756      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.826     0.781      1560\n",
      "          1      0.778     0.677     0.724      1401\n",
      "\n",
      "avg / total      0.758     0.756     0.754      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.834     0.781      1560\n",
      "          1      0.782     0.663     0.718      1401\n",
      "\n",
      "avg / total      0.757     0.753     0.751      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.825\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.883     0.842      1560\n",
      "          1      0.854     0.762     0.805      1401\n",
      "\n",
      "avg / total      0.828     0.825     0.824      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.882     0.840      1560\n",
      "          1      0.852     0.757     0.802      1401\n",
      "\n",
      "avg / total      0.825     0.823     0.822      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.882     0.833      1560\n",
      "          1      0.849     0.739     0.790      1401\n",
      "\n",
      "avg / total      0.818     0.814     0.813      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.838     0.790      1560\n",
      "          1      0.791     0.685     0.735      1401\n",
      "\n",
      "avg / total      0.768     0.766     0.764      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.820     0.762      1560\n",
      "          1      0.758     0.629     0.687      1401\n",
      "\n",
      "avg / total      0.733     0.729     0.726      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.730\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.847     0.768      1560\n",
      "          1      0.779     0.601     0.678      1401\n",
      "\n",
      "avg / total      0.739     0.730     0.726      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.799     0.864     0.830      1560\n",
      "          1      0.833     0.757     0.794      1401\n",
      "\n",
      "avg / total      0.815     0.814     0.813      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.871     0.823      1560\n",
      "          1      0.834     0.726     0.776      1401\n",
      "\n",
      "avg / total      0.805     0.802     0.801      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.863     0.826      1560\n",
      "          1      0.831     0.747     0.786      1401\n",
      "\n",
      "avg / total      0.810     0.808     0.807      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.799     0.773      1560\n",
      "          1      0.758     0.700     0.728      1401\n",
      "\n",
      "avg / total      0.753     0.752     0.752      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.886     0.833      1560\n",
      "          1      0.852     0.730     0.786      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.811      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.895     0.838      1560\n",
      "          1      0.862     0.732     0.792      1401\n",
      "\n",
      "avg / total      0.823     0.818     0.816      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.896     0.834      1560\n",
      "          1      0.861     0.720     0.784      1401\n",
      "\n",
      "avg / total      0.819     0.813     0.811      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.894     0.791      1560\n",
      "          1      0.834     0.591     0.692      1401\n",
      "\n",
      "avg / total      0.768     0.751     0.744      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.716     0.888     0.793      1560\n",
      "          1      0.829     0.607     0.701      1401\n",
      "\n",
      "avg / total      0.770     0.755     0.749      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.899     0.793      1560\n",
      "          1      0.840     0.592     0.694      1401\n",
      "\n",
      "avg / total      0.772     0.753     0.747      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.878     0.822      1560\n",
      "          1      0.840     0.713     0.771      1401\n",
      "\n",
      "avg / total      0.805     0.800     0.798      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.888     0.807      1560\n",
      "          1      0.839     0.653     0.735      1401\n",
      "\n",
      "avg / total      0.787     0.777     0.773      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.899     0.812      1560\n",
      "          1      0.853     0.648     0.736      1401\n",
      "\n",
      "avg / total      0.793     0.780     0.776      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.865     0.779      1560\n",
      "          1      0.801     0.605     0.689      1401\n",
      "\n",
      "avg / total      0.752     0.742     0.736      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.890     0.833      1560\n",
      "          1      0.855     0.725     0.785      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.810      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.879     0.828      1560\n",
      "          1      0.844     0.729     0.783      1401\n",
      "\n",
      "avg / total      0.812     0.808     0.807      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.894     0.835      1560\n",
      "          1      0.860     0.727     0.788      1401\n",
      "\n",
      "avg / total      0.820     0.815     0.813      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.864     0.780      1560\n",
      "          1      0.801     0.609     0.692      1401\n",
      "\n",
      "avg / total      0.754     0.743     0.738      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.885     0.802      1560\n",
      "          1      0.833     0.642     0.725      1401\n",
      "\n",
      "avg / total      0.781     0.770     0.765      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.881     0.842      1560\n",
      "          1      0.853     0.764     0.806      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.879     0.844      1560\n",
      "          1      0.852     0.772     0.810      1401\n",
      "\n",
      "avg / total      0.830     0.828     0.828      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.876     0.840      1560\n",
      "          1      0.848     0.766     0.805      1401\n",
      "\n",
      "avg / total      0.826     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.888     0.804      1560\n",
      "          1      0.838     0.641     0.726      1401\n",
      "\n",
      "avg / total      0.783     0.771     0.767      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.887     0.808      1560\n",
      "          1      0.839     0.656     0.736      1401\n",
      "\n",
      "avg / total      0.788     0.778     0.774      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.896     0.815      1560\n",
      "          1      0.851     0.665     0.746      1401\n",
      "\n",
      "avg / total      0.797     0.786     0.783      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.883     0.841      1560\n",
      "          1      0.854     0.757     0.803      1401\n",
      "\n",
      "avg / total      0.826     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.882     0.837      1560\n",
      "          1      0.851     0.750     0.797      1401\n",
      "\n",
      "avg / total      0.823     0.820     0.819      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.883     0.832      1560\n",
      "          1      0.849     0.734     0.787      1401\n",
      "\n",
      "avg / total      0.816     0.812     0.811      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.874     0.802      1560\n",
      "          1      0.825     0.660     0.734      1401\n",
      "\n",
      "avg / total      0.781     0.773     0.770      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.825\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.881     0.841      1560\n",
      "          1      0.852     0.762     0.805      1401\n",
      "\n",
      "avg / total      0.827     0.825     0.824      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.885     0.840      1560\n",
      "          1      0.854     0.754     0.801      1401\n",
      "\n",
      "avg / total      0.826     0.823     0.822      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.799     0.887     0.841      1560\n",
      "          1      0.857     0.751     0.800      1401\n",
      "\n",
      "avg / total      0.826     0.823     0.822      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.849     0.793      1560\n",
      "          1      0.800     0.674     0.731      1401\n",
      "\n",
      "avg / total      0.770     0.766     0.764      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.810     0.859     0.834      1560\n",
      "          1      0.832     0.775     0.802      1401\n",
      "\n",
      "avg / total      0.820     0.819     0.819      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.891     0.834      1560\n",
      "          1      0.857     0.727     0.787      1401\n",
      "\n",
      "avg / total      0.819     0.814     0.812      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.882     0.836      1560\n",
      "          1      0.850     0.747     0.795      1401\n",
      "\n",
      "avg / total      0.821     0.818     0.817      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.898     0.838      1560\n",
      "          1      0.865     0.727     0.790      1401\n",
      "\n",
      "avg / total      0.823     0.817     0.815      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.894     0.792      1560\n",
      "          1      0.834     0.595     0.695      1401\n",
      "\n",
      "avg / total      0.769     0.752     0.746      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.892     0.833      1560\n",
      "          1      0.858     0.722     0.784      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.810      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.888     0.834      1560\n",
      "          1      0.854     0.732     0.789      1401\n",
      "\n",
      "avg / total      0.819     0.814     0.813      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.870     0.836      1560\n",
      "          1      0.841     0.764     0.800      1401\n",
      "\n",
      "avg / total      0.821     0.820     0.819      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.813     0.876     0.843      1560\n",
      "          1      0.848     0.775     0.810      1401\n",
      "\n",
      "avg / total      0.830     0.828     0.827      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.879     0.844      1560\n",
      "          1      0.852     0.772     0.810      1401\n",
      "\n",
      "avg / total      0.831     0.829     0.828      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.871     0.801      1560\n",
      "          1      0.821     0.660     0.732      1401\n",
      "\n",
      "avg / total      0.779     0.771     0.768      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.854     0.795      1560\n",
      "          1      0.805     0.671     0.732      1401\n",
      "\n",
      "avg / total      0.772     0.767     0.765      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.784\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.756     0.872     0.810      1560\n",
      "          1      0.828     0.686     0.750      1401\n",
      "\n",
      "avg / total      0.790     0.784     0.782      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.870     0.836      1560\n",
      "          1      0.841     0.764     0.800      1401\n",
      "\n",
      "avg / total      0.821     0.820     0.819      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.869     0.825      1560\n",
      "          1      0.835     0.734     0.781      1401\n",
      "\n",
      "avg / total      0.808     0.805     0.804      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.878     0.829      1560\n",
      "          1      0.843     0.731     0.783      1401\n",
      "\n",
      "avg / total      0.812     0.809     0.807      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.836     0.785      1560\n",
      "          1      0.786     0.672     0.725      1401\n",
      "\n",
      "avg / total      0.762     0.759     0.756      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.881     0.844      1560\n",
      "          1      0.853     0.771     0.810      1401\n",
      "\n",
      "avg / total      0.831     0.829     0.828      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.875     0.839      1560\n",
      "          1      0.846     0.767     0.804      1401\n",
      "\n",
      "avg / total      0.825     0.824     0.823      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.884     0.843      1560\n",
      "          1      0.855     0.762     0.806      1401\n",
      "\n",
      "avg / total      0.829     0.826     0.825      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.838     0.790      1560\n",
      "          1      0.792     0.686     0.735      1401\n",
      "\n",
      "avg / total      0.769     0.766     0.764      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.803     0.865     0.833      1560\n",
      "          1      0.836     0.763     0.798      1401\n",
      "\n",
      "avg / total      0.818     0.817     0.816      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.888     0.829      1560\n",
      "          1      0.852     0.717     0.779      1401\n",
      "\n",
      "avg / total      0.813     0.807     0.805      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.876     0.833      1560\n",
      "          1      0.844     0.747     0.792      1401\n",
      "\n",
      "avg / total      0.818     0.815     0.814      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.892     0.840      1560\n",
      "          1      0.860     0.744     0.798      1401\n",
      "\n",
      "avg / total      0.826     0.822     0.820      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.881     0.791      1560\n",
      "          1      0.823     0.615     0.704      1401\n",
      "\n",
      "avg / total      0.768     0.755     0.750      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.874     0.823      1560\n",
      "          1      0.838     0.722     0.775      1401\n",
      "\n",
      "avg / total      0.806     0.802     0.801      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.880     0.833      1560\n",
      "          1      0.847     0.741     0.791      1401\n",
      "\n",
      "avg / total      0.818     0.814     0.813      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.827\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.808     0.883     0.843      1560\n",
      "          1      0.854     0.766     0.808      1401\n",
      "\n",
      "avg / total      0.830     0.827     0.827      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.883     0.844      1560\n",
      "          1      0.855     0.767     0.809      1401\n",
      "\n",
      "avg / total      0.831     0.828     0.828      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.815     0.885     0.848      1560\n",
      "          1      0.858     0.776     0.815      1401\n",
      "\n",
      "avg / total      0.835     0.833     0.832      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.885     0.808      1560\n",
      "          1      0.837     0.662     0.739      1401\n",
      "\n",
      "avg / total      0.788     0.779     0.776      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.810     0.879     0.844      1560\n",
      "          1      0.852     0.771     0.809      1401\n",
      "\n",
      "avg / total      0.830     0.828     0.827      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.827\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.808     0.881     0.843      1560\n",
      "          1      0.853     0.767     0.808      1401\n",
      "\n",
      "avg / total      0.830     0.827     0.827      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.886     0.834      1560\n",
      "          1      0.852     0.733     0.788      1401\n",
      "\n",
      "avg / total      0.818     0.814     0.812      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 7\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.871     0.834      1560\n",
      "          1      0.840     0.759     0.797      1401\n",
      "\n",
      "avg / total      0.819     0.818     0.817      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.871     0.839      1560\n",
      "          1      0.842     0.771     0.805      1401\n",
      "\n",
      "avg / total      0.825     0.823     0.823      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.827\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.813     0.874     0.842      1560\n",
      "          1      0.847     0.776     0.810      1401\n",
      "\n",
      "avg / total      0.829     0.827     0.827      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.854     0.798      1560\n",
      "          1      0.807     0.682     0.739      1401\n",
      "\n",
      "avg / total      0.777     0.772     0.770      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.803     0.863     0.832      1560\n",
      "          1      0.833     0.764     0.797      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.827\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.814     0.872     0.842      1560\n",
      "          1      0.845     0.777     0.810      1401\n",
      "\n",
      "avg / total      0.829     0.827     0.827      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.881     0.832      1560\n",
      "          1      0.847     0.737     0.789      1401\n",
      "\n",
      "avg / total      0.817     0.813     0.812      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.831\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.812     0.884     0.846      1560\n",
      "          1      0.857     0.772     0.812      1401\n",
      "\n",
      "avg / total      0.833     0.831     0.830      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 7, 8\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.869     0.836      1560\n",
      "          1      0.840     0.765     0.801      1401\n",
      "\n",
      "avg / total      0.821     0.820     0.819      2961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = y_neg + y_pos\n",
    "for s in subsets:\n",
    "    indices = list(s)\n",
    "    print \"Testing features: \" + str(indices).strip('[]')\n",
    "    #build X\n",
    "\n",
    "    X_neg = [[] for i in range(0, len(X_neg_all_features[0]))]\n",
    "    X_pos = [[] for i in range(0, len(X_pos_all_features[0]))]\n",
    "    for i in indices:\n",
    "        neg_features = X_neg_all_features[i]\n",
    "        j = 0\n",
    "        for feature in neg_features:\n",
    "            X_neg[j].append(feature)\n",
    "            j += 1\n",
    "        \n",
    "        pos_features = X_pos_all_features[i]\n",
    "        j = 0\n",
    "        for feature in pos_features:\n",
    "            X_pos[j].append(feature)\n",
    "            j += 1\n",
    "    \n",
    "    X = X_neg + X_pos\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    #train test accuracy\n",
    "    clf = train(X_train, y_train, 1000, 2)\n",
    "    predictions = test(clf, X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import relevant dependency trees\n",
    "\n",
    "import pickle\n",
    "f1 = open('tree_roots_neg.txt', 'rb')\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "neg_roots = pickle.load(f1)\n",
    "f1.close()\n",
    "\n",
    "f2 = open('tree_depth_neg.txt', 'rb')\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "neg_depth = pickle.load(f2)\n",
    "f2.close()\n",
    "\n",
    "f3 = open('tree_roots_pos.txt', 'rb')\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "pos_roots = pickle.load(f3)\n",
    "f3.close()\n",
    "\n",
    "f4 = open('tree_depth_pos.txt', 'rb')\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "pos_depth = pickle.load(f4)\n",
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_neg_roots = []\n",
    "for root in neg_roots:\n",
    "    new_root = root.encode('ascii','ignore')\n",
    "    new_neg_roots.append(new_root)\n",
    "\n",
    "new_pos_roots = []\n",
    "for root in pos_roots:\n",
    "    new_root = root.encode('ascii','ignore')\n",
    "    new_pos_roots.append(new_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_roots_features = []\n",
    "for i in range(0, len(neg_roots), 2):\n",
    "    root_dist = distance_between_words(new_neg_roots[i], new_neg_roots[i+1])\n",
    "    neg_roots_features.append(root_dist)\n",
    "    \n",
    "pos_roots_features = []\n",
    "for i in range(0, len(pos_roots), 2):\n",
    "    root_dist = distance_between_words(new_pos_roots[i], new_pos_roots[i+1])\n",
    "    pos_roots_features.append(root_dist)\n",
    "    \n",
    "neg_depth_features = []\n",
    "for i in range(0, len(neg_depth), 2):\n",
    "    depth_diff = abs(neg_depth[i]-neg_depth[i+1])\n",
    "    neg_depth_features.append(depth_diff)\n",
    "\n",
    "pos_depth_features = []\n",
    "for i in range(0, len(pos_depth), 2):\n",
    "    depth_diff = abs(pos_depth[i]-pos_depth[i+1])\n",
    "    pos_depth_features.append(depth_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "X_neg_all_features.append(neg_roots_features)\n",
    "X_pos_all_features.append(pos_roots_features)\n",
    "X_neg_all_features.append(neg_depth_features)\n",
    "X_pos_all_features.append(pos_depth_features)\n",
    "\n",
    "print len(X_neg_all_features)\n",
    "print len(X_pos_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets2 = []\n",
    "\n",
    "feature_indices2 = [0,1,2,3,5,6,7,8,9,10]\n",
    "for L in range(0, len(feature_indices)+1):\n",
    "    for subset in itertools.combinations(feature_indices2, L):\n",
    "        subsets2.append(subset)\n",
    "\n",
    "new_indices = []\n",
    "for subset in subsets2:\n",
    "    if subset not in subsets:\n",
    "        new_indices.append(subset)\n",
    "\n",
    "        \n",
    "new_indices.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing features: 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.561\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.569     0.684     0.621      1560\n",
      "          1      0.546     0.424     0.477      1401\n",
      "\n",
      "avg / total      0.559     0.561     0.553      2961\n",
      "\n",
      "Testing features: 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.534\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.558     0.558     0.558      1560\n",
      "          1      0.508     0.507     0.507      1401\n",
      "\n",
      "avg / total      0.534     0.534     0.534      2961\n",
      "\n",
      "Testing features: 0, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.571\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.587     0.631     0.608      1560\n",
      "          1      0.551     0.505     0.527      1401\n",
      "\n",
      "avg / total      0.570     0.571     0.570      2961\n",
      "\n",
      "Testing features: 0, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.572\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.562     0.842     0.675      1560\n",
      "          1      0.606     0.271     0.374      1401\n",
      "\n",
      "avg / total      0.583     0.572     0.532      2961\n",
      "\n",
      "Testing features: 1, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.655\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.638     0.799     0.710      1560\n",
      "          1      0.689     0.495     0.576      1401\n",
      "\n",
      "avg / total      0.662     0.655     0.646      2961\n",
      "\n",
      "Testing features: 1, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.648     0.783     0.709      1560\n",
      "          1      0.685     0.527     0.596      1401\n",
      "\n",
      "avg / total      0.666     0.662     0.655      2961\n",
      "\n",
      "Testing features: 2, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.666\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.859     0.730      1560\n",
      "          1      0.742     0.451     0.561      1401\n",
      "\n",
      "avg / total      0.686     0.666     0.650      2961\n",
      "\n",
      "Testing features: 2, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.664\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.619     0.944     0.747      1560\n",
      "          1      0.849     0.353     0.498      1401\n",
      "\n",
      "avg / total      0.728     0.664     0.630      2961\n",
      "\n",
      "Testing features: 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.671\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.633     0.894     0.741      1560\n",
      "          1      0.782     0.423     0.549      1401\n",
      "\n",
      "avg / total      0.703     0.671     0.650      2961\n",
      "\n",
      "Testing features: 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.664\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.854     0.728      1560\n",
      "          1      0.736     0.453     0.561      1401\n",
      "\n",
      "avg / total      0.683     0.664     0.649      2961\n",
      "\n",
      "Testing features: 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.929     0.797      1560\n",
      "          1      0.876     0.552     0.677      1401\n",
      "\n",
      "avg / total      0.782     0.751     0.741      2961\n",
      "\n",
      "Testing features: 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.699     0.929     0.798      1560\n",
      "          1      0.876     0.554     0.679      1401\n",
      "\n",
      "avg / total      0.783     0.752     0.741      2961\n",
      "\n",
      "Testing features: 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.650\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.627     0.831     0.715      1560\n",
      "          1      0.705     0.449     0.549      1401\n",
      "\n",
      "avg / total      0.664     0.650     0.636      2961\n",
      "\n",
      "Testing features: 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.606     0.864     0.713      1560\n",
      "          1      0.713     0.375     0.492      1401\n",
      "\n",
      "avg / total      0.657     0.633     0.608      2961\n",
      "\n",
      "Testing features: 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.643\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.615     0.863     0.718      1560\n",
      "          1      0.722     0.398     0.513      1401\n",
      "\n",
      "avg / total      0.666     0.643     0.621      2961\n",
      "\n",
      "Testing features: 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.651\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.628     0.831     0.715      1560\n",
      "          1      0.705     0.451     0.550      1401\n",
      "\n",
      "avg / total      0.664     0.651     0.637      2961\n",
      "\n",
      "Testing features: 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.610\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.640     0.594     0.616      1560\n",
      "          1      0.581     0.627     0.603      1401\n",
      "\n",
      "avg / total      0.612     0.610     0.610      2961\n",
      "\n",
      "Testing features: 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.572\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.583     0.665     0.621      1560\n",
      "          1      0.557     0.470     0.510      1401\n",
      "\n",
      "avg / total      0.571     0.572     0.568      2961\n",
      "\n",
      "Testing features: 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.577     0.743     0.649      1560\n",
      "          1      0.579     0.393     0.468      1401\n",
      "\n",
      "avg / total      0.578     0.578     0.564      2961\n",
      "\n",
      "Testing features: 0, 1, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.645\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.770     0.696      1560\n",
      "          1      0.664     0.507     0.575      1401\n",
      "\n",
      "avg / total      0.649     0.645     0.639      2961\n",
      "\n",
      "Testing features: 0, 1, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.658\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.648     0.766     0.702      1560\n",
      "          1      0.674     0.537     0.598      1401\n",
      "\n",
      "avg / total      0.660     0.658     0.653      2961\n",
      "\n",
      "Testing features: 0, 2, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.633     0.910     0.747      1560\n",
      "          1      0.805     0.413     0.546      1401\n",
      "\n",
      "avg / total      0.715     0.675     0.652      2961\n",
      "\n",
      "Testing features: 0, 2, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.678\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.636     0.908     0.748      1560\n",
      "          1      0.804     0.422     0.553      1401\n",
      "\n",
      "avg / total      0.716     0.678     0.656      2961\n",
      "\n",
      "Testing features: 0, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.638     0.876     0.739      1560\n",
      "          1      0.765     0.448     0.565      1401\n",
      "\n",
      "avg / total      0.698     0.673     0.656      2961\n",
      "\n",
      "Testing features: 0, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.665\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.634     0.860     0.730      1560\n",
      "          1      0.742     0.447     0.558      1401\n",
      "\n",
      "avg / total      0.685     0.665     0.648      2961\n",
      "\n",
      "Testing features: 0, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.918     0.796      1560\n",
      "          1      0.861     0.567     0.684      1401\n",
      "\n",
      "avg / total      0.777     0.752     0.743      2961\n",
      "\n",
      "Testing features: 0, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.917     0.796      1560\n",
      "          1      0.860     0.569     0.685      1401\n",
      "\n",
      "avg / total      0.777     0.752     0.743      2961\n",
      "\n",
      "Testing features: 0, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.659\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.638     0.815     0.716      1560\n",
      "          1      0.702     0.485     0.574      1401\n",
      "\n",
      "avg / total      0.668     0.659     0.649      2961\n",
      "\n",
      "Testing features: 0, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.615     0.841     0.711      1560\n",
      "          1      0.700     0.414     0.520      1401\n",
      "\n",
      "avg / total      0.655     0.639     0.621      2961\n",
      "\n",
      "Testing features: 0, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.643\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.616     0.852     0.715      1560\n",
      "          1      0.713     0.410     0.520      1401\n",
      "\n",
      "avg / total      0.662     0.643     0.623      2961\n",
      "\n",
      "Testing features: 0, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.631     0.836     0.719      1560\n",
      "          1      0.713     0.455     0.555      1401\n",
      "\n",
      "avg / total      0.670     0.656     0.641      2961\n",
      "\n",
      "Testing features: 0, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.659     0.568     0.610      1560\n",
      "          1      0.583     0.673     0.625      1401\n",
      "\n",
      "avg / total      0.623     0.618     0.617      2961\n",
      "\n",
      "Testing features: 0, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.585\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.606     0.604     0.605      1560\n",
      "          1      0.561     0.562     0.562      1401\n",
      "\n",
      "avg / total      0.585     0.585     0.585      2961\n",
      "\n",
      "Testing features: 0, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.589\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.583     0.771     0.664      1560\n",
      "          1      0.602     0.385     0.470      1401\n",
      "\n",
      "avg / total      0.592     0.589     0.572      2961\n",
      "\n",
      "Testing features: 1, 2, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.731     0.838     0.781      1560\n",
      "          1      0.784     0.657     0.715      1401\n",
      "\n",
      "avg / total      0.757     0.752     0.750      2961\n",
      "\n",
      "Testing features: 1, 2, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.837     0.776      1560\n",
      "          1      0.780     0.644     0.705      1401\n",
      "\n",
      "avg / total      0.750     0.745     0.742      2961\n",
      "\n",
      "Testing features: 1, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.832     0.739      1560\n",
      "          1      0.740     0.533     0.620      1401\n",
      "\n",
      "avg / total      0.701     0.691     0.683      2961\n",
      "\n",
      "Testing features: 1, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.678\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.650     0.846     0.735      1560\n",
      "          1      0.741     0.493     0.592      1401\n",
      "\n",
      "avg / total      0.693     0.678     0.667      2961\n",
      "\n",
      "Testing features: 1, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.799     0.862     0.829      1560\n",
      "          1      0.831     0.759     0.793      1401\n",
      "\n",
      "avg / total      0.814     0.813     0.812      2961\n",
      "\n",
      "Testing features: 1, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.866     0.829      1560\n",
      "          1      0.834     0.752     0.791      1401\n",
      "\n",
      "avg / total      0.814     0.812     0.811      2961\n",
      "\n",
      "Testing features: 1, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.712     0.791     0.750      1560\n",
      "          1      0.735     0.645     0.687      1401\n",
      "\n",
      "avg / total      0.723     0.722     0.720      2961\n",
      "\n",
      "Testing features: 1, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.794     0.742      1560\n",
      "          1      0.728     0.615     0.667      1401\n",
      "\n",
      "avg / total      0.712     0.709     0.706      2961\n",
      "\n",
      "Testing features: 1, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.683     0.822     0.746      1560\n",
      "          1      0.744     0.575     0.649      1401\n",
      "\n",
      "avg / total      0.712     0.706     0.700      2961\n",
      "\n",
      "Testing features: 1, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.701\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.678     0.824     0.744      1560\n",
      "          1      0.743     0.565     0.642      1401\n",
      "\n",
      "avg / total      0.709     0.701     0.696      2961\n",
      "\n",
      "Testing features: 1, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.660\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.712     0.688      1560\n",
      "          1      0.652     0.601     0.626      1401\n",
      "\n",
      "avg / total      0.659     0.660     0.658      2961\n",
      "\n",
      "Testing features: 1, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.649     0.800     0.717      1560\n",
      "          1      0.699     0.518     0.595      1401\n",
      "\n",
      "avg / total      0.673     0.667     0.659      2961\n",
      "\n",
      "Testing features: 1, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.671\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.672     0.735     0.702      1560\n",
      "          1      0.670     0.600     0.633      1401\n",
      "\n",
      "avg / total      0.671     0.671     0.669      2961\n",
      "\n",
      "Testing features: 2, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.685     0.917     0.784      1560\n",
      "          1      0.852     0.530     0.653      1401\n",
      "\n",
      "avg / total      0.764     0.734     0.722      2961\n",
      "\n",
      "Testing features: 2, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.906     0.783      1560\n",
      "          1      0.838     0.545     0.660      1401\n",
      "\n",
      "avg / total      0.760     0.735     0.725      2961\n",
      "\n",
      "Testing features: 2, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.893     0.807      1560\n",
      "          1      0.844     0.644     0.730      1401\n",
      "\n",
      "avg / total      0.787     0.775     0.771      2961\n",
      "\n",
      "Testing features: 2, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.901     0.810      1560\n",
      "          1      0.852     0.639     0.730      1401\n",
      "\n",
      "avg / total      0.791     0.777     0.772      2961\n",
      "\n",
      "Testing features: 2, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.671\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.638     0.869     0.736      1560\n",
      "          1      0.756     0.451     0.565      1401\n",
      "\n",
      "avg / total      0.694     0.671     0.655      2961\n",
      "\n",
      "Testing features: 2, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.682\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.641     0.901     0.749      1560\n",
      "          1      0.799     0.439     0.567      1401\n",
      "\n",
      "avg / total      0.716     0.682     0.663      2961\n",
      "\n",
      "Testing features: 2, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.649     0.916     0.760      1560\n",
      "          1      0.827     0.448     0.581      1401\n",
      "\n",
      "avg / total      0.733     0.695     0.675      2961\n",
      "\n",
      "Testing features: 2, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.648     0.895     0.752      1560\n",
      "          1      0.797     0.458     0.582      1401\n",
      "\n",
      "avg / total      0.718     0.688     0.671      2961\n",
      "\n",
      "Testing features: 2, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.637     0.891     0.743      1560\n",
      "          1      0.782     0.435     0.559      1401\n",
      "\n",
      "avg / total      0.706     0.675     0.656      2961\n",
      "\n",
      "Testing features: 2, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.665\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.639     0.837     0.725      1560\n",
      "          1      0.723     0.474     0.572      1401\n",
      "\n",
      "avg / total      0.679     0.665     0.653      2961\n",
      "\n",
      "Testing features: 2, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.629     0.899     0.740      1560\n",
      "          1      0.784     0.410     0.538      1401\n",
      "\n",
      "avg / total      0.702     0.667     0.645      2961\n",
      "\n",
      "Testing features: 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.888     0.830      1560\n",
      "          1      0.853     0.720     0.781      1401\n",
      "\n",
      "avg / total      0.814     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.770     0.893     0.827      1560\n",
      "          1      0.855     0.702     0.771      1401\n",
      "\n",
      "avg / total      0.810     0.803     0.800      2961\n",
      "\n",
      "Testing features: 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.680     0.866     0.762      1560\n",
      "          1      0.785     0.545     0.644      1401\n",
      "\n",
      "avg / total      0.730     0.714     0.706      2961\n",
      "\n",
      "Testing features: 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.683     0.844     0.755      1560\n",
      "          1      0.765     0.564     0.649      1401\n",
      "\n",
      "avg / total      0.722     0.712     0.705      2961\n",
      "\n",
      "Testing features: 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.680     0.853     0.757      1560\n",
      "          1      0.771     0.553     0.644      1401\n",
      "\n",
      "avg / total      0.723     0.711     0.703      2961\n",
      "\n",
      "Testing features: 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.678     0.840     0.751      1560\n",
      "          1      0.757     0.557     0.642      1401\n",
      "\n",
      "avg / total      0.716     0.706     0.699      2961\n",
      "\n",
      "Testing features: 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.668\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.870     0.734      1560\n",
      "          1      0.754     0.443     0.558      1401\n",
      "\n",
      "avg / total      0.691     0.668     0.651      2961\n",
      "\n",
      "Testing features: 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.670\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.640     0.852     0.731      1560\n",
      "          1      0.739     0.468     0.573      1401\n",
      "\n",
      "avg / total      0.687     0.670     0.656      2961\n",
      "\n",
      "Testing features: 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.670\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.639     0.860     0.733      1560\n",
      "          1      0.747     0.459     0.569      1401\n",
      "\n",
      "avg / total      0.690     0.670     0.655      2961\n",
      "\n",
      "Testing features: 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.913     0.799      1560\n",
      "          1      0.858     0.585     0.695      1401\n",
      "\n",
      "avg / total      0.780     0.758     0.750      2961\n",
      "\n",
      "Testing features: 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.888     0.791      1560\n",
      "          1      0.829     0.602     0.697      1401\n",
      "\n",
      "avg / total      0.768     0.753     0.747      2961\n",
      "\n",
      "Testing features: 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.911     0.806      1560\n",
      "          1      0.860     0.610     0.713      1401\n",
      "\n",
      "avg / total      0.787     0.768     0.762      2961\n",
      "\n",
      "Testing features: 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.908     0.803      1560\n",
      "          1      0.856     0.607     0.710      1401\n",
      "\n",
      "avg / total      0.784     0.766     0.759      2961\n",
      "\n",
      "Testing features: 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.929     0.797      1560\n",
      "          1      0.876     0.552     0.677      1401\n",
      "\n",
      "avg / total      0.782     0.751     0.741      2961\n",
      "\n",
      "Testing features: 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.699     0.929     0.797      1560\n",
      "          1      0.875     0.554     0.678      1401\n",
      "\n",
      "avg / total      0.782     0.751     0.741      2961\n",
      "\n",
      "Testing features: 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.929     0.799      1560\n",
      "          1      0.876     0.557     0.681      1401\n",
      "\n",
      "avg / total      0.783     0.753     0.743      2961\n",
      "\n",
      "Testing features: 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.683\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.651     0.856     0.740      1560\n",
      "          1      0.753     0.490     0.593      1401\n",
      "\n",
      "avg / total      0.699     0.683     0.670      2961\n",
      "\n",
      "Testing features: 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.649     0.846     0.734      1560\n",
      "          1      0.741     0.490     0.590      1401\n",
      "\n",
      "avg / total      0.692     0.677     0.666      2961\n",
      "\n",
      "Testing features: 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.651\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.636     0.790     0.705      1560\n",
      "          1      0.680     0.496     0.574      1401\n",
      "\n",
      "avg / total      0.657     0.651     0.643      2961\n",
      "\n",
      "Testing features: 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.636\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.613     0.838     0.708      1560\n",
      "          1      0.695     0.410     0.516      1401\n",
      "\n",
      "avg / total      0.652     0.636     0.617      2961\n",
      "\n",
      "Testing features: 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.618     0.817     0.704      1560\n",
      "          1      0.682     0.438     0.534      1401\n",
      "\n",
      "avg / total      0.648     0.638     0.623      2961\n",
      "\n",
      "Testing features: 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.651\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.629     0.826     0.714      1560\n",
      "          1      0.703     0.457     0.554      1401\n",
      "\n",
      "avg / total      0.664     0.651     0.638      2961\n",
      "\n",
      "Testing features: 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.631     0.803     0.707      1560\n",
      "          1      0.685     0.478     0.563      1401\n",
      "\n",
      "avg / total      0.657     0.649     0.639      2961\n",
      "\n",
      "Testing features: 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.628     0.833     0.716      1560\n",
      "          1      0.708     0.450     0.550      1401\n",
      "\n",
      "avg / total      0.666     0.652     0.637      2961\n",
      "\n",
      "Testing features: 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.608\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.625     0.640     0.632      1560\n",
      "          1      0.588     0.573     0.581      1401\n",
      "\n",
      "avg / total      0.608     0.608     0.608      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.842     0.776      1560\n",
      "          1      0.783     0.634     0.701      1401\n",
      "\n",
      "avg / total      0.749     0.744     0.740      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.847     0.777      1560\n",
      "          1      0.787     0.630     0.700      1401\n",
      "\n",
      "avg / total      0.751     0.744     0.741      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.669     0.833     0.742      1560\n",
      "          1      0.744     0.540     0.626      1401\n",
      "\n",
      "avg / total      0.705     0.695     0.687      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.661     0.822     0.733      1560\n",
      "          1      0.729     0.531     0.614      1401\n",
      "\n",
      "avg / total      0.693     0.685     0.677      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.862     0.818      1560\n",
      "          1      0.825     0.728     0.774      1401\n",
      "\n",
      "avg / total      0.801     0.798     0.797      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.769     0.872     0.817      1560\n",
      "          1      0.832     0.709     0.766      1401\n",
      "\n",
      "avg / total      0.799     0.795     0.793      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.783     0.744      1560\n",
      "          1      0.726     0.640     0.681      1401\n",
      "\n",
      "avg / total      0.717     0.716     0.714      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.710\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.781     0.739      1560\n",
      "          1      0.721     0.631     0.673      1401\n",
      "\n",
      "avg / total      0.711     0.710     0.708      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.797     0.739      1560\n",
      "          1      0.726     0.599     0.656      1401\n",
      "\n",
      "avg / total      0.707     0.703     0.700      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.674     0.801     0.732      1560\n",
      "          1      0.719     0.569     0.635      1401\n",
      "\n",
      "avg / total      0.695     0.691     0.686      2961\n",
      "\n",
      "Testing features: 0, 1, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.721     0.692      1560\n",
      "          1      0.657     0.597     0.626      1401\n",
      "\n",
      "avg / total      0.662     0.662     0.660      2961\n",
      "\n",
      "Testing features: 0, 1, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.767     0.712      1560\n",
      "          1      0.687     0.570     0.623      1401\n",
      "\n",
      "avg / total      0.675     0.673     0.670      2961\n",
      "\n",
      "Testing features: 0, 1, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.658\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.657     0.736     0.694      1560\n",
      "          1      0.660     0.572     0.613      1401\n",
      "\n",
      "avg / total      0.658     0.658     0.656      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.892     0.786      1560\n",
      "          1      0.828     0.581     0.683      1401\n",
      "\n",
      "avg / total      0.762     0.745     0.737      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.699     0.904     0.788      1560\n",
      "          1      0.841     0.567     0.677      1401\n",
      "\n",
      "avg / total      0.766     0.744     0.736      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.776\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.896     0.808      1560\n",
      "          1      0.847     0.644     0.732      1401\n",
      "\n",
      "avg / total      0.789     0.776     0.772      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.888     0.809      1560\n",
      "          1      0.841     0.660     0.739      1401\n",
      "\n",
      "avg / total      0.790     0.780     0.776      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.651     0.860     0.741      1560\n",
      "          1      0.758     0.487     0.593      1401\n",
      "\n",
      "avg / total      0.702     0.684     0.671      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.650     0.868     0.743      1560\n",
      "          1      0.765     0.479     0.589      1401\n",
      "\n",
      "avg / total      0.704     0.684     0.670      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.689\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.651     0.881     0.749      1560\n",
      "          1      0.782     0.475     0.591      1401\n",
      "\n",
      "avg / total      0.713     0.689     0.674      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.654     0.903     0.759      1560\n",
      "          1      0.813     0.469     0.595      1401\n",
      "\n",
      "avg / total      0.730     0.698     0.681      2961\n",
      "\n",
      "Testing features: 0, 2, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.655     0.847     0.739      1560\n",
      "          1      0.747     0.504     0.602      1401\n",
      "\n",
      "avg / total      0.699     0.685     0.674      2961\n",
      "\n",
      "Testing features: 0, 2, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.682\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.641     0.897     0.748      1560\n",
      "          1      0.794     0.441     0.567      1401\n",
      "\n",
      "avg / total      0.714     0.682     0.663      2961\n",
      "\n",
      "Testing features: 0, 2, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.642     0.872     0.740      1560\n",
      "          1      0.764     0.459     0.573      1401\n",
      "\n",
      "avg / total      0.700     0.677     0.661      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.889     0.832      1560\n",
      "          1      0.854     0.722     0.783      1401\n",
      "\n",
      "avg / total      0.816     0.810     0.808      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.894     0.830      1560\n",
      "          1      0.858     0.710     0.777      1401\n",
      "\n",
      "avg / total      0.814     0.807     0.805      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.841     0.758      1560\n",
      "          1      0.765     0.577     0.658      1401\n",
      "\n",
      "avg / total      0.725     0.716     0.711      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.682     0.847     0.756      1560\n",
      "          1      0.767     0.560     0.647      1401\n",
      "\n",
      "avg / total      0.722     0.711     0.704      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.682     0.840     0.753      1560\n",
      "          1      0.760     0.563     0.647      1401\n",
      "\n",
      "avg / total      0.719     0.709     0.703      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.681     0.850     0.756      1560\n",
      "          1      0.769     0.557     0.646      1401\n",
      "\n",
      "avg / total      0.723     0.711     0.704      2961\n",
      "\n",
      "Testing features: 0, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.671\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.640     0.858     0.733      1560\n",
      "          1      0.745     0.463     0.571      1401\n",
      "\n",
      "avg / total      0.690     0.671     0.656      2961\n",
      "\n",
      "Testing features: 0, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.648     0.848     0.735      1560\n",
      "          1      0.742     0.487     0.588      1401\n",
      "\n",
      "avg / total      0.692     0.677     0.665      2961\n",
      "\n",
      "Testing features: 0, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.646     0.848     0.733      1560\n",
      "          1      0.740     0.483     0.584      1401\n",
      "\n",
      "avg / total      0.691     0.675     0.663      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.894     0.798      1560\n",
      "          1      0.839     0.613     0.708      1401\n",
      "\n",
      "avg / total      0.776     0.761     0.756      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.881     0.794      1560\n",
      "          1      0.825     0.624     0.710      1401\n",
      "\n",
      "avg / total      0.771     0.759     0.754      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.908     0.804      1560\n",
      "          1      0.856     0.610     0.713      1401\n",
      "\n",
      "avg / total      0.785     0.767     0.761      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.907     0.806      1560\n",
      "          1      0.856     0.617     0.717      1401\n",
      "\n",
      "avg / total      0.787     0.770     0.764      2961\n",
      "\n",
      "Testing features: 0, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.912     0.796      1560\n",
      "          1      0.854     0.577     0.689      1401\n",
      "\n",
      "avg / total      0.776     0.753     0.745      2961\n",
      "\n",
      "Testing features: 0, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.909     0.795      1560\n",
      "          1      0.851     0.580     0.690      1401\n",
      "\n",
      "avg / total      0.775     0.753     0.746      2961\n",
      "\n",
      "Testing features: 0, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.917     0.797      1560\n",
      "          1      0.861     0.573     0.688      1401\n",
      "\n",
      "avg / total      0.779     0.754     0.746      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.656     0.844     0.738      1560\n",
      "          1      0.745     0.508     0.604      1401\n",
      "\n",
      "avg / total      0.698     0.685     0.675      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.661     0.831     0.737      1560\n",
      "          1      0.737     0.525     0.613      1401\n",
      "\n",
      "avg / total      0.697     0.687     0.678      2961\n",
      "\n",
      "Testing features: 0, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.645     0.818     0.721      1560\n",
      "          1      0.711     0.499     0.586      1401\n",
      "\n",
      "avg / total      0.676     0.667     0.657      2961\n",
      "\n",
      "Testing features: 0, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.644\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.619     0.848     0.715      1560\n",
      "          1      0.712     0.418     0.526      1401\n",
      "\n",
      "avg / total      0.663     0.644     0.626      2961\n",
      "\n",
      "Testing features: 0, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.653\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.632     0.819     0.713      1560\n",
      "          1      0.699     0.468     0.561      1401\n",
      "\n",
      "avg / total      0.663     0.653     0.641      2961\n",
      "\n",
      "Testing features: 0, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.634     0.821     0.716      1560\n",
      "          1      0.704     0.473     0.565      1401\n",
      "\n",
      "avg / total      0.667     0.656     0.645      2961\n",
      "\n",
      "Testing features: 0, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.653\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.631     0.823     0.715      1560\n",
      "          1      0.702     0.465     0.559      1401\n",
      "\n",
      "avg / total      0.665     0.653     0.641      2961\n",
      "\n",
      "Testing features: 0, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.658\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.632     0.840     0.721      1560\n",
      "          1      0.718     0.455     0.557      1401\n",
      "\n",
      "avg / total      0.673     0.658     0.644      2961\n",
      "\n",
      "Testing features: 0, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.610\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.624     0.656     0.640      1560\n",
      "          1      0.594     0.559     0.576      1401\n",
      "\n",
      "avg / total      0.609     0.610     0.609      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.865     0.798      1560\n",
      "          1      0.815     0.662     0.730      1401\n",
      "\n",
      "avg / total      0.776     0.769     0.766      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.882     0.802      1560\n",
      "          1      0.831     0.647     0.728      1401\n",
      "\n",
      "avg / total      0.781     0.771     0.767      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.878     0.826      1560\n",
      "          1      0.842     0.723     0.778      1401\n",
      "\n",
      "avg / total      0.809     0.805     0.803      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.877     0.824      1560\n",
      "          1      0.840     0.719     0.775      1401\n",
      "\n",
      "avg / total      0.807     0.802     0.801      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.841     0.767      1560\n",
      "          1      0.775     0.610     0.682      1401\n",
      "\n",
      "avg / total      0.738     0.732     0.727      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.730\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.704     0.842     0.767      1560\n",
      "          1      0.775     0.605     0.680      1401\n",
      "\n",
      "avg / total      0.738     0.730     0.726      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.851     0.780      1560\n",
      "          1      0.792     0.632     0.703      1401\n",
      "\n",
      "avg / total      0.754     0.747     0.744      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.873     0.785      1560\n",
      "          1      0.812     0.610     0.696      1401\n",
      "\n",
      "avg / total      0.760     0.748     0.743      2961\n",
      "\n",
      "Testing features: 1, 2, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.831     0.788      1560\n",
      "          1      0.786     0.690     0.735      1401\n",
      "\n",
      "avg / total      0.767     0.765     0.763      2961\n",
      "\n",
      "Testing features: 1, 2, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.831     0.785      1560\n",
      "          1      0.783     0.681     0.729      1401\n",
      "\n",
      "avg / total      0.762     0.760     0.758      2961\n",
      "\n",
      "Testing features: 1, 2, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.848     0.775      1560\n",
      "          1      0.786     0.622     0.694      1401\n",
      "\n",
      "avg / total      0.748     0.741     0.737      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.881     0.833      1560\n",
      "          1      0.848     0.739     0.790      1401\n",
      "\n",
      "avg / total      0.817     0.814     0.813      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.886     0.833      1560\n",
      "          1      0.852     0.732     0.788      1401\n",
      "\n",
      "avg / total      0.818     0.813     0.812      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.814     0.763      1560\n",
      "          1      0.757     0.645     0.697      1401\n",
      "\n",
      "avg / total      0.737     0.734     0.732      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.726\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.704     0.829     0.761      1560\n",
      "          1      0.763     0.611     0.679      1401\n",
      "\n",
      "avg / total      0.732     0.726     0.722      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.701     0.844     0.766      1560\n",
      "          1      0.775     0.600     0.677      1401\n",
      "\n",
      "avg / total      0.736     0.728     0.724      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.717\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.688     0.848     0.760      1560\n",
      "          1      0.772     0.572     0.657      1401\n",
      "\n",
      "avg / total      0.728     0.717     0.711      2961\n",
      "\n",
      "Testing features: 1, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.707\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.684     0.824     0.747      1560\n",
      "          1      0.746     0.576     0.650      1401\n",
      "\n",
      "avg / total      0.713     0.707     0.701      2961\n",
      "\n",
      "Testing features: 1, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.674     0.820     0.740      1560\n",
      "          1      0.736     0.559     0.635      1401\n",
      "\n",
      "avg / total      0.703     0.696     0.690      2961\n",
      "\n",
      "Testing features: 1, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.668     0.823     0.738      1560\n",
      "          1      0.734     0.545     0.625      1401\n",
      "\n",
      "avg / total      0.699     0.691     0.684      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.867     0.822      1560\n",
      "          1      0.831     0.729     0.777      1401\n",
      "\n",
      "avg / total      0.805     0.802     0.800      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.865     0.818      1560\n",
      "          1      0.828     0.720     0.770      1401\n",
      "\n",
      "avg / total      0.800     0.797     0.795      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.873     0.824      1560\n",
      "          1      0.837     0.727     0.778      1401\n",
      "\n",
      "avg / total      0.807     0.804     0.802      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.769     0.879     0.820      1560\n",
      "          1      0.840     0.706     0.767      1401\n",
      "\n",
      "avg / total      0.802     0.797     0.795      2961\n",
      "\n",
      "Testing features: 1, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.863     0.833      1560\n",
      "          1      0.834     0.769     0.800      1401\n",
      "\n",
      "avg / total      0.819     0.818     0.818      2961\n",
      "\n",
      "Testing features: 1, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.859     0.831      1560\n",
      "          1      0.830     0.767     0.797      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.853     0.826      1560\n",
      "          1      0.823     0.765     0.793      1401\n",
      "\n",
      "avg / total      0.812     0.811     0.811      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.747     0.788     0.767      1560\n",
      "          1      0.749     0.703     0.725      1401\n",
      "\n",
      "avg / total      0.748     0.748     0.747      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.790     0.764      1560\n",
      "          1      0.747     0.692     0.718      1401\n",
      "\n",
      "avg / total      0.744     0.743     0.743      2961\n",
      "\n",
      "Testing features: 1, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.781     0.751      1560\n",
      "          1      0.733     0.667     0.699      1401\n",
      "\n",
      "avg / total      0.728     0.727     0.726      2961\n",
      "\n",
      "Testing features: 1, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.792     0.750      1560\n",
      "          1      0.736     0.645     0.687      1401\n",
      "\n",
      "avg / total      0.724     0.722     0.721      2961\n",
      "\n",
      "Testing features: 1, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.702\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.783     0.735      1560\n",
      "          1      0.717     0.612     0.661      1401\n",
      "\n",
      "avg / total      0.704     0.702     0.700      2961\n",
      "\n",
      "Testing features: 1, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.717\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.804     0.750      1560\n",
      "          1      0.740     0.621     0.675      1401\n",
      "\n",
      "avg / total      0.720     0.717     0.714      2961\n",
      "\n",
      "Testing features: 1, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.804     0.747      1560\n",
      "          1      0.737     0.610     0.668      1401\n",
      "\n",
      "avg / total      0.716     0.713     0.709      2961\n",
      "\n",
      "Testing features: 1, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.701\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.682     0.810     0.741      1560\n",
      "          1      0.733     0.580     0.647      1401\n",
      "\n",
      "avg / total      0.706     0.701     0.696      2961\n",
      "\n",
      "Testing features: 1, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.677     0.724     0.700      1560\n",
      "          1      0.667     0.615     0.640      1401\n",
      "\n",
      "avg / total      0.672     0.673     0.672      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.892     0.831      1560\n",
      "          1      0.856     0.716     0.780      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.807      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.894     0.831      1560\n",
      "          1      0.858     0.714     0.780      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.807      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.884     0.781      1560\n",
      "          1      0.817     0.578     0.677      1401\n",
      "\n",
      "avg / total      0.756     0.739     0.732      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.695     0.894     0.782      1560\n",
      "          1      0.827     0.564     0.671      1401\n",
      "\n",
      "avg / total      0.758     0.738     0.730      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.890     0.787      1560\n",
      "          1      0.828     0.587     0.687      1401\n",
      "\n",
      "avg / total      0.764     0.747     0.740      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.695     0.914     0.790      1560\n",
      "          1      0.853     0.554     0.672      1401\n",
      "\n",
      "avg / total      0.770     0.744     0.734      2961\n",
      "\n",
      "Testing features: 2, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.897     0.790      1560\n",
      "          1      0.835     0.582     0.686      1401\n",
      "\n",
      "avg / total      0.767     0.748     0.741      2961\n",
      "\n",
      "Testing features: 2, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.690     0.917     0.788      1560\n",
      "          1      0.854     0.542     0.663      1401\n",
      "\n",
      "avg / total      0.768     0.740     0.729      2961\n",
      "\n",
      "Testing features: 2, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.897     0.785      1560\n",
      "          1      0.832     0.567     0.674      1401\n",
      "\n",
      "avg / total      0.761     0.741     0.733      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.776\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.902     0.809      1560\n",
      "          1      0.853     0.636     0.729      1401\n",
      "\n",
      "avg / total      0.790     0.776     0.771      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.908     0.811      1560\n",
      "          1      0.860     0.633     0.729      1401\n",
      "\n",
      "avg / total      0.794     0.778     0.773      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.910     0.814      1560\n",
      "          1      0.864     0.638     0.734      1401\n",
      "\n",
      "avg / total      0.797     0.781     0.776      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.893     0.809      1560\n",
      "          1      0.845     0.651     0.735      1401\n",
      "\n",
      "avg / total      0.790     0.778     0.774      2961\n",
      "\n",
      "Testing features: 2, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.899     0.808      1560\n",
      "          1      0.850     0.638     0.729      1401\n",
      "\n",
      "avg / total      0.789     0.775     0.771      2961\n",
      "\n",
      "Testing features: 2, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.903     0.809      1560\n",
      "          1      0.854     0.633     0.727      1401\n",
      "\n",
      "avg / total      0.790     0.775     0.770      2961\n",
      "\n",
      "Testing features: 2, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.885     0.804      1560\n",
      "          1      0.835     0.647     0.729      1401\n",
      "\n",
      "avg / total      0.783     0.772     0.768      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.693     0.872     0.772      1560\n",
      "          1      0.800     0.570     0.665      1401\n",
      "\n",
      "avg / total      0.743     0.729     0.722      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.874     0.775      1560\n",
      "          1      0.804     0.577     0.672      1401\n",
      "\n",
      "avg / total      0.748     0.733     0.726      2961\n",
      "\n",
      "Testing features: 2, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.817     0.733      1560\n",
      "          1      0.727     0.541     0.620      1401\n",
      "\n",
      "avg / total      0.694     0.687     0.680      2961\n",
      "\n",
      "Testing features: 2, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.654     0.877     0.749      1560\n",
      "          1      0.779     0.484     0.597      1401\n",
      "\n",
      "avg / total      0.713     0.691     0.677      2961\n",
      "\n",
      "Testing features: 2, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.679\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.645     0.871     0.741      1560\n",
      "          1      0.764     0.466     0.579      1401\n",
      "\n",
      "avg / total      0.701     0.679     0.664      2961\n",
      "\n",
      "Testing features: 2, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.661     0.876     0.754      1560\n",
      "          1      0.784     0.500     0.610      1401\n",
      "\n",
      "avg / total      0.719     0.698     0.686      2961\n",
      "\n",
      "Testing features: 2, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.656     0.884     0.753      1560\n",
      "          1      0.789     0.484     0.600      1401\n",
      "\n",
      "avg / total      0.719     0.695     0.681      2961\n",
      "\n",
      "Testing features: 2, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.656     0.890     0.755      1560\n",
      "          1      0.796     0.480     0.599      1401\n",
      "\n",
      "avg / total      0.722     0.696     0.681      2961\n",
      "\n",
      "Testing features: 2, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.643     0.872     0.740      1560\n",
      "          1      0.764     0.460     0.574      1401\n",
      "\n",
      "avg / total      0.700     0.677     0.661      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.887     0.829      1560\n",
      "          1      0.851     0.719     0.779      1401\n",
      "\n",
      "avg / total      0.813     0.807     0.806      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.892     0.830      1560\n",
      "          1      0.855     0.714     0.778      1401\n",
      "\n",
      "avg / total      0.814     0.807     0.805      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.892     0.833      1560\n",
      "          1      0.858     0.722     0.784      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.810      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.894     0.831      1560\n",
      "          1      0.858     0.714     0.779      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.806      2961\n",
      "\n",
      "Testing features: 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.893     0.831      1560\n",
      "          1      0.857     0.715     0.780      1401\n",
      "\n",
      "avg / total      0.815     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.768     0.896     0.827      1560\n",
      "          1      0.858     0.699     0.770      1401\n",
      "\n",
      "avg / total      0.811     0.803     0.800      2961\n",
      "\n",
      "Testing features: 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.888     0.830      1560\n",
      "          1      0.852     0.721     0.781      1401\n",
      "\n",
      "avg / total      0.814     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.850     0.772      1560\n",
      "          1      0.785     0.609     0.686      1401\n",
      "\n",
      "avg / total      0.744     0.736     0.731      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.853     0.768      1560\n",
      "          1      0.783     0.588     0.672      1401\n",
      "\n",
      "avg / total      0.738     0.728     0.722      2961\n",
      "\n",
      "Testing features: 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.684     0.866     0.764      1560\n",
      "          1      0.788     0.555     0.651      1401\n",
      "\n",
      "avg / total      0.733     0.719     0.711      2961\n",
      "\n",
      "Testing features: 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.683     0.842     0.754      1560\n",
      "          1      0.762     0.565     0.649      1401\n",
      "\n",
      "avg / total      0.720     0.711     0.704      2961\n",
      "\n",
      "Testing features: 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.687     0.831     0.752      1560\n",
      "          1      0.754     0.579     0.655      1401\n",
      "\n",
      "avg / total      0.719     0.712     0.706      2961\n",
      "\n",
      "Testing features: 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.681     0.860     0.760      1560\n",
      "          1      0.779     0.552     0.647      1401\n",
      "\n",
      "avg / total      0.728     0.714     0.706      2961\n",
      "\n",
      "Testing features: 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.708\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.679     0.847     0.754      1560\n",
      "          1      0.765     0.553     0.642      1401\n",
      "\n",
      "avg / total      0.720     0.708     0.701      2961\n",
      "\n",
      "Testing features: 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.705\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.680     0.832     0.748      1560\n",
      "          1      0.751     0.564     0.644      1401\n",
      "\n",
      "avg / total      0.714     0.705     0.699      2961\n",
      "\n",
      "Testing features: 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.674\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.649     0.830     0.729      1560\n",
      "          1      0.726     0.501     0.593      1401\n",
      "\n",
      "avg / total      0.686     0.674     0.664      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.899     0.802      1560\n",
      "          1      0.847     0.619     0.715      1401\n",
      "\n",
      "avg / total      0.782     0.767     0.761      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.889     0.797      1560\n",
      "          1      0.834     0.620     0.711      1401\n",
      "\n",
      "avg / total      0.775     0.762     0.756      2961\n",
      "\n",
      "Testing features: 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.860     0.787      1560\n",
      "          1      0.803     0.638     0.711      1401\n",
      "\n",
      "avg / total      0.762     0.755     0.751      2961\n",
      "\n",
      "Testing features: 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.869     0.788      1560\n",
      "          1      0.811     0.624     0.705      1401\n",
      "\n",
      "avg / total      0.763     0.753     0.749      2961\n",
      "\n",
      "Testing features: 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.896     0.798      1560\n",
      "          1      0.841     0.611     0.708      1401\n",
      "\n",
      "avg / total      0.777     0.761     0.755      2961\n",
      "\n",
      "Testing features: 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.909     0.806      1560\n",
      "          1      0.859     0.615     0.717      1401\n",
      "\n",
      "avg / total      0.788     0.770     0.764      2961\n",
      "\n",
      "Testing features: 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.910     0.804      1560\n",
      "          1      0.858     0.606     0.710      1401\n",
      "\n",
      "avg / total      0.785     0.766     0.759      2961\n",
      "\n",
      "Testing features: 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.909     0.806      1560\n",
      "          1      0.859     0.615     0.717      1401\n",
      "\n",
      "avg / total      0.788     0.770     0.764      2961\n",
      "\n",
      "Testing features: 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.927     0.798      1560\n",
      "          1      0.873     0.557     0.680      1401\n",
      "\n",
      "avg / total      0.782     0.752     0.742      2961\n",
      "\n",
      "Testing features: 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.663     0.847     0.744      1560\n",
      "          1      0.754     0.520     0.616      1401\n",
      "\n",
      "avg / total      0.706     0.693     0.683      2961\n",
      "\n",
      "Testing features: 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.667     0.841     0.744      1560\n",
      "          1      0.750     0.532     0.622      1401\n",
      "\n",
      "avg / total      0.706     0.695     0.686      2961\n",
      "\n",
      "Testing features: 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.689\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.663     0.836     0.739      1560\n",
      "          1      0.742     0.526     0.616      1401\n",
      "\n",
      "avg / total      0.700     0.689     0.681      2961\n",
      "\n",
      "Testing features: 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.639     0.781     0.703      1560\n",
      "          1      0.676     0.507     0.580      1401\n",
      "\n",
      "avg / total      0.656     0.652     0.645      2961\n",
      "\n",
      "Testing features: 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.653\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.633     0.813     0.712      1560\n",
      "          1      0.695     0.475     0.565      1401\n",
      "\n",
      "avg / total      0.662     0.653     0.642      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.753     0.862     0.804      1560\n",
      "          1      0.817     0.685     0.745      1401\n",
      "\n",
      "avg / total      0.783     0.778     0.776      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.873     0.801      1560\n",
      "          1      0.824     0.660     0.732      1401\n",
      "\n",
      "avg / total      0.780     0.772     0.769      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.879     0.824      1560\n",
      "          1      0.842     0.716     0.774      1401\n",
      "\n",
      "avg / total      0.807     0.802     0.800      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.872     0.824      1560\n",
      "          1      0.837     0.727     0.778      1401\n",
      "\n",
      "avg / total      0.807     0.804     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.723\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.823     0.758      1560\n",
      "          1      0.756     0.611     0.676      1401\n",
      "\n",
      "avg / total      0.728     0.723     0.719      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.723\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.699     0.833     0.760      1560\n",
      "          1      0.763     0.601     0.673      1401\n",
      "\n",
      "avg / total      0.730     0.723     0.719      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.833     0.769      1560\n",
      "          1      0.772     0.627     0.692      1401\n",
      "\n",
      "avg / total      0.741     0.736     0.732      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.865     0.779      1560\n",
      "          1      0.801     0.605     0.689      1401\n",
      "\n",
      "avg / total      0.753     0.742     0.737      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.746     0.827     0.784      1560\n",
      "          1      0.781     0.686     0.730      1401\n",
      "\n",
      "avg / total      0.762     0.760     0.759      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.731     0.829     0.777      1560\n",
      "          1      0.777     0.660     0.714      1401\n",
      "\n",
      "avg / total      0.753     0.749     0.747      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.840     0.769      1560\n",
      "          1      0.776     0.618     0.688      1401\n",
      "\n",
      "avg / total      0.741     0.735     0.731      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.878     0.834      1560\n",
      "          1      0.846     0.747     0.793      1401\n",
      "\n",
      "avg / total      0.819     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.885     0.833      1560\n",
      "          1      0.851     0.734     0.789      1401\n",
      "\n",
      "avg / total      0.818     0.814     0.812      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.717     0.804     0.758      1560\n",
      "          1      0.748     0.646     0.693      1401\n",
      "\n",
      "avg / total      0.731     0.729     0.727      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.822     0.756      1560\n",
      "          1      0.754     0.607     0.673      1401\n",
      "\n",
      "avg / total      0.726     0.721     0.717      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.726\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.824     0.760      1560\n",
      "          1      0.759     0.618     0.681      1401\n",
      "\n",
      "avg / total      0.731     0.726     0.723      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.695     0.840     0.761      1560\n",
      "          1      0.768     0.590     0.667      1401\n",
      "\n",
      "avg / total      0.730     0.722     0.717      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.686     0.825     0.749      1560\n",
      "          1      0.748     0.580     0.653      1401\n",
      "\n",
      "avg / total      0.716     0.709     0.704      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.671     0.818     0.737      1560\n",
      "          1      0.732     0.554     0.631      1401\n",
      "\n",
      "avg / total      0.700     0.693     0.687      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.667     0.814     0.733      1560\n",
      "          1      0.726     0.547     0.624      1401\n",
      "\n",
      "avg / total      0.695     0.688     0.682      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.859     0.818      1560\n",
      "          1      0.823     0.731     0.774      1401\n",
      "\n",
      "avg / total      0.801     0.798     0.797      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.864     0.817      1560\n",
      "          1      0.826     0.720     0.770      1401\n",
      "\n",
      "avg / total      0.799     0.796     0.795      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.862     0.822      1560\n",
      "          1      0.828     0.737     0.779      1401\n",
      "\n",
      "avg / total      0.805     0.803     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.870     0.818      1560\n",
      "          1      0.831     0.714     0.769      1401\n",
      "\n",
      "avg / total      0.800     0.796     0.795      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.853     0.826      1560\n",
      "          1      0.823     0.763     0.792      1401\n",
      "\n",
      "avg / total      0.811     0.810     0.810      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.862     0.824      1560\n",
      "          1      0.829     0.744     0.785      1401\n",
      "\n",
      "avg / total      0.808     0.806     0.806      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.854     0.816      1560\n",
      "          1      0.819     0.732     0.773      1401\n",
      "\n",
      "avg / total      0.799     0.797     0.796      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.775     0.759      1560\n",
      "          1      0.737     0.702     0.719      1401\n",
      "\n",
      "avg / total      0.740     0.741     0.740      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.783     0.761      1560\n",
      "          1      0.742     0.693     0.717      1401\n",
      "\n",
      "avg / total      0.741     0.741     0.740      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.715     0.776     0.744      1560\n",
      "          1      0.724     0.655     0.688      1401\n",
      "\n",
      "avg / total      0.719     0.719     0.717      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.779     0.741      1560\n",
      "          1      0.722     0.637     0.677      1401\n",
      "\n",
      "avg / total      0.713     0.712     0.710      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.701\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.780     0.733      1560\n",
      "          1      0.715     0.613     0.660      1401\n",
      "\n",
      "avg / total      0.703     0.701     0.699      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.701     0.789     0.743      1560\n",
      "          1      0.727     0.626     0.673      1401\n",
      "\n",
      "avg / total      0.714     0.712     0.710      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.690     0.794     0.738      1560\n",
      "          1      0.724     0.602     0.658      1401\n",
      "\n",
      "avg / total      0.706     0.703     0.700      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.689     0.794     0.738      1560\n",
      "          1      0.724     0.602     0.657      1401\n",
      "\n",
      "avg / total      0.706     0.703     0.700      2961\n",
      "\n",
      "Testing features: 0, 1, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.666\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.663     0.743     0.701      1560\n",
      "          1      0.669     0.580     0.621      1401\n",
      "\n",
      "avg / total      0.666     0.666     0.663      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.890     0.834      1560\n",
      "          1      0.856     0.727     0.786      1401\n",
      "\n",
      "avg / total      0.818     0.813     0.811      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.888     0.833      1560\n",
      "          1      0.854     0.729     0.787      1401\n",
      "\n",
      "avg / total      0.818     0.813     0.811      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.862     0.776      1560\n",
      "          1      0.796     0.599     0.684      1401\n",
      "\n",
      "avg / total      0.748     0.738     0.732      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.882     0.784      1560\n",
      "          1      0.818     0.592     0.687      1401\n",
      "\n",
      "avg / total      0.759     0.745     0.738      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.871     0.782      1560\n",
      "          1      0.808     0.604     0.691      1401\n",
      "\n",
      "avg / total      0.756     0.745     0.739      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.902     0.792      1560\n",
      "          1      0.842     0.583     0.689      1401\n",
      "\n",
      "avg / total      0.771     0.751     0.744      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.877     0.786      1560\n",
      "          1      0.815     0.604     0.694      1401\n",
      "\n",
      "avg / total      0.760     0.748     0.742      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.701     0.890     0.784      1560\n",
      "          1      0.825     0.577     0.679      1401\n",
      "\n",
      "avg / total      0.760     0.742     0.734      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.871     0.779      1560\n",
      "          1      0.805     0.595     0.684      1401\n",
      "\n",
      "avg / total      0.753     0.740     0.734      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.886     0.802      1560\n",
      "          1      0.835     0.641     0.725      1401\n",
      "\n",
      "avg / total      0.781     0.770     0.766      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.738     0.871     0.799      1560\n",
      "          1      0.820     0.656     0.729      1401\n",
      "\n",
      "avg / total      0.777     0.769     0.766      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.890     0.804      1560\n",
      "          1      0.840     0.640     0.726      1401\n",
      "\n",
      "avg / total      0.784     0.772     0.767      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.782\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.901     0.813      1560\n",
      "          1      0.855     0.650     0.739      1401\n",
      "\n",
      "avg / total      0.795     0.782     0.778      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.892     0.811      1560\n",
      "          1      0.845     0.657     0.739      1401\n",
      "\n",
      "avg / total      0.791     0.780     0.777      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.885     0.808      1560\n",
      "          1      0.837     0.658     0.737      1401\n",
      "\n",
      "avg / total      0.787     0.778     0.774      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.738     0.882     0.804      1560\n",
      "          1      0.832     0.651     0.730      1401\n",
      "\n",
      "avg / total      0.782     0.773     0.769      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.699     0.846     0.765      1560\n",
      "          1      0.776     0.594     0.673      1401\n",
      "\n",
      "avg / total      0.735     0.727     0.722      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.697     0.862     0.771      1560\n",
      "          1      0.791     0.582     0.671      1401\n",
      "\n",
      "avg / total      0.741     0.729     0.723      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.669     0.798     0.728      1560\n",
      "          1      0.713     0.560     0.627      1401\n",
      "\n",
      "avg / total      0.690     0.685     0.680      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.692\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.662     0.848     0.743      1560\n",
      "          1      0.754     0.517     0.614      1401\n",
      "\n",
      "avg / total      0.705     0.692     0.682      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.679\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.654     0.830     0.731      1560\n",
      "          1      0.730     0.510     0.601      1401\n",
      "\n",
      "avg / total      0.690     0.679     0.670      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.661     0.867     0.750      1560\n",
      "          1      0.774     0.505     0.611      1401\n",
      "\n",
      "avg / total      0.714     0.696     0.684      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.700\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.664     0.872     0.754      1560\n",
      "          1      0.781     0.507     0.615      1401\n",
      "\n",
      "avg / total      0.719     0.700     0.688      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.654     0.862     0.744      1560\n",
      "          1      0.762     0.491     0.597      1401\n",
      "\n",
      "avg / total      0.705     0.687     0.674      2961\n",
      "\n",
      "Testing features: 0, 2, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.682\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.653     0.846     0.737      1560\n",
      "          1      0.744     0.500     0.598      1401\n",
      "\n",
      "avg / total      0.696     0.682     0.671      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.879     0.829      1560\n",
      "          1      0.844     0.731     0.783      1401\n",
      "\n",
      "avg / total      0.813     0.809     0.807      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.887     0.834      1560\n",
      "          1      0.854     0.734     0.789      1401\n",
      "\n",
      "avg / total      0.819     0.815     0.813      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.890     0.837      1560\n",
      "          1      0.858     0.736     0.792      1401\n",
      "\n",
      "avg / total      0.822     0.817     0.816      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.890     0.835      1560\n",
      "          1      0.857     0.730     0.788      1401\n",
      "\n",
      "avg / total      0.820     0.815     0.813      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.890     0.830      1560\n",
      "          1      0.854     0.714     0.778      1401\n",
      "\n",
      "avg / total      0.813     0.807     0.805      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.894     0.829      1560\n",
      "          1      0.857     0.707     0.775      1401\n",
      "\n",
      "avg / total      0.812     0.805     0.803      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.884     0.828      1560\n",
      "          1      0.848     0.720     0.779      1401\n",
      "\n",
      "avg / total      0.811     0.806     0.805      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.849     0.773      1560\n",
      "          1      0.784     0.612     0.688      1401\n",
      "\n",
      "avg / total      0.745     0.737     0.733      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.858     0.775      1560\n",
      "          1      0.793     0.605     0.686      1401\n",
      "\n",
      "avg / total      0.748     0.738     0.733      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.692     0.817     0.750      1560\n",
      "          1      0.745     0.595     0.662      1401\n",
      "\n",
      "avg / total      0.717     0.712     0.708      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.681     0.821     0.745      1560\n",
      "          1      0.742     0.572     0.646      1401\n",
      "\n",
      "avg / total      0.710     0.703     0.698      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.682     0.819     0.744      1560\n",
      "          1      0.740     0.574     0.647      1401\n",
      "\n",
      "avg / total      0.709     0.703     0.698      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.687     0.837     0.754      1560\n",
      "          1      0.760     0.575     0.655      1401\n",
      "\n",
      "avg / total      0.721     0.713     0.707      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.677     0.844     0.752      1560\n",
      "          1      0.761     0.552     0.640      1401\n",
      "\n",
      "avg / total      0.717     0.706     0.699      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.681     0.821     0.744      1560\n",
      "          1      0.741     0.571     0.645      1401\n",
      "\n",
      "avg / total      0.709     0.703     0.697      2961\n",
      "\n",
      "Testing features: 0, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.680\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.653     0.835     0.733      1560\n",
      "          1      0.734     0.506     0.599      1401\n",
      "\n",
      "avg / total      0.691     0.680     0.670      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.890     0.800      1560\n",
      "          1      0.837     0.627     0.717      1401\n",
      "\n",
      "avg / total      0.779     0.766     0.761      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.877     0.794      1560\n",
      "          1      0.821     0.630     0.713      1401\n",
      "\n",
      "avg / total      0.771     0.760     0.755      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.885     0.797      1560\n",
      "          1      0.830     0.627     0.714      1401\n",
      "\n",
      "avg / total      0.775     0.763     0.758      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.757\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.875     0.792      1560\n",
      "          1      0.818     0.626     0.709      1401\n",
      "\n",
      "avg / total      0.768     0.757     0.753      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.878     0.795      1560\n",
      "          1      0.822     0.631     0.714      1401\n",
      "\n",
      "avg / total      0.772     0.761     0.756      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.903     0.804      1560\n",
      "          1      0.852     0.618     0.716      1401\n",
      "\n",
      "avg / total      0.785     0.768     0.763      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.898     0.805      1560\n",
      "          1      0.847     0.628     0.721      1401\n",
      "\n",
      "avg / total      0.785     0.770     0.765      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.894     0.801      1560\n",
      "          1      0.841     0.624     0.716      1401\n",
      "\n",
      "avg / total      0.780     0.766     0.761      2961\n",
      "\n",
      "Testing features: 0, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.907     0.795      1560\n",
      "          1      0.849     0.582     0.690      1401\n",
      "\n",
      "avg / total      0.774     0.753     0.745      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.665     0.821     0.735      1560\n",
      "          1      0.730     0.540     0.621      1401\n",
      "\n",
      "avg / total      0.696     0.688     0.681      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.668     0.823     0.737      1560\n",
      "          1      0.734     0.544     0.625      1401\n",
      "\n",
      "avg / total      0.699     0.691     0.684      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.689\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.668     0.814     0.734      1560\n",
      "          1      0.727     0.550     0.626      1401\n",
      "\n",
      "avg / total      0.696     0.689     0.683      2961\n",
      "\n",
      "Testing features: 0, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.647\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.635     0.779     0.699      1560\n",
      "          1      0.670     0.501     0.574      1401\n",
      "\n",
      "avg / total      0.652     0.647     0.640      2961\n",
      "\n",
      "Testing features: 0, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.641     0.785     0.706      1560\n",
      "          1      0.681     0.511     0.584      1401\n",
      "\n",
      "avg / total      0.660     0.656     0.648      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.879     0.834      1560\n",
      "          1      0.847     0.747     0.794      1401\n",
      "\n",
      "avg / total      0.819     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.864     0.834      1560\n",
      "          1      0.836     0.769     0.801      1401\n",
      "\n",
      "avg / total      0.820     0.819     0.819      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.849     0.788      1560\n",
      "          1      0.797     0.657     0.720      1401\n",
      "\n",
      "avg / total      0.764     0.759     0.756      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.867     0.792      1560\n",
      "          1      0.812     0.640     0.716      1401\n",
      "\n",
      "avg / total      0.768     0.760     0.756      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.856     0.795      1560\n",
      "          1      0.807     0.670     0.732      1401\n",
      "\n",
      "avg / total      0.773     0.768     0.765      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.881     0.805      1560\n",
      "          1      0.833     0.657     0.735      1401\n",
      "\n",
      "avg / total      0.785     0.775     0.772      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.787\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.763     0.864     0.810      1560\n",
      "          1      0.822     0.701     0.757      1401\n",
      "\n",
      "avg / total      0.791     0.787     0.785      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.747     0.867     0.803      1560\n",
      "          1      0.819     0.674     0.740      1401\n",
      "\n",
      "avg / total      0.781     0.775     0.773      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.868     0.800      1560\n",
      "          1      0.818     0.662     0.732      1401\n",
      "\n",
      "avg / total      0.778     0.771     0.768      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.875     0.828      1560\n",
      "          1      0.841     0.735     0.784      1401\n",
      "\n",
      "avg / total      0.812     0.809     0.808      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.879     0.828      1560\n",
      "          1      0.844     0.727     0.781      1401\n",
      "\n",
      "avg / total      0.811     0.807     0.806      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.873     0.822      1560\n",
      "          1      0.836     0.719     0.773      1401\n",
      "\n",
      "avg / total      0.804     0.800     0.799      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.878     0.826      1560\n",
      "          1      0.842     0.724     0.779      1401\n",
      "\n",
      "avg / total      0.810     0.805     0.804      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.878     0.830      1560\n",
      "          1      0.844     0.736     0.786      1401\n",
      "\n",
      "avg / total      0.814     0.811     0.810      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.871     0.828      1560\n",
      "          1      0.837     0.740     0.786      1401\n",
      "\n",
      "avg / total      0.811     0.809     0.808      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.871     0.823      1560\n",
      "          1      0.834     0.726     0.776      1401\n",
      "\n",
      "avg / total      0.805     0.802     0.801      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.730     0.842     0.782      1560\n",
      "          1      0.788     0.654     0.715      1401\n",
      "\n",
      "avg / total      0.758     0.753     0.750      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.725     0.853     0.784      1560\n",
      "          1      0.796     0.640     0.710      1401\n",
      "\n",
      "avg / total      0.759     0.752     0.749      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.732     0.820     0.773      1560\n",
      "          1      0.768     0.665     0.713      1401\n",
      "\n",
      "avg / total      0.749     0.747     0.745      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.728     0.825     0.773      1560\n",
      "          1      0.771     0.656     0.709      1401\n",
      "\n",
      "avg / total      0.748     0.745     0.743      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.701     0.812     0.753      1560\n",
      "          1      0.746     0.615     0.674      1401\n",
      "\n",
      "avg / total      0.722     0.719     0.715      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.851     0.793      1560\n",
      "          1      0.802     0.672     0.731      1401\n",
      "\n",
      "avg / total      0.770     0.766     0.764      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.837     0.788      1560\n",
      "          1      0.789     0.679     0.730      1401\n",
      "\n",
      "avg / total      0.765     0.762     0.760      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.853     0.773      1560\n",
      "          1      0.787     0.607     0.686      1401\n",
      "\n",
      "avg / total      0.745     0.737     0.732      2961\n",
      "\n",
      "Testing features: 1, 2, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.829     0.784      1560\n",
      "          1      0.782     0.682     0.729      1401\n",
      "\n",
      "avg / total      0.762     0.760     0.758      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.803     0.877     0.838      1560\n",
      "          1      0.847     0.761     0.802      1401\n",
      "\n",
      "avg / total      0.824     0.822     0.821      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.883     0.840      1560\n",
      "          1      0.853     0.756     0.802      1401\n",
      "\n",
      "avg / total      0.826     0.823     0.822      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.878     0.838      1560\n",
      "          1      0.847     0.757     0.800      1401\n",
      "\n",
      "avg / total      0.823     0.821     0.820      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.884     0.837      1560\n",
      "          1      0.852     0.745     0.795      1401\n",
      "\n",
      "avg / total      0.822     0.818     0.817      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.885     0.838      1560\n",
      "          1      0.853     0.747     0.796      1401\n",
      "\n",
      "avg / total      0.823     0.819     0.818      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.880     0.831      1560\n",
      "          1      0.846     0.736     0.787      1401\n",
      "\n",
      "avg / total      0.816     0.812     0.811      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.792     0.875     0.831      1560\n",
      "          1      0.842     0.744     0.790      1401\n",
      "\n",
      "avg / total      0.816     0.813     0.812      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.752     0.816     0.783      1560\n",
      "          1      0.774     0.701     0.736      1401\n",
      "\n",
      "avg / total      0.763     0.762     0.761      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.757\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.821     0.781      1560\n",
      "          1      0.775     0.685     0.727      1401\n",
      "\n",
      "avg / total      0.759     0.757     0.755      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.829     0.776      1560\n",
      "          1      0.775     0.657     0.711      1401\n",
      "\n",
      "avg / total      0.751     0.747     0.745      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.830     0.766      1560\n",
      "          1      0.767     0.623     0.688      1401\n",
      "\n",
      "avg / total      0.737     0.732     0.729      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.717     0.810     0.761      1560\n",
      "          1      0.753     0.643     0.694      1401\n",
      "\n",
      "avg / total      0.734     0.731     0.729      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.842     0.769      1560\n",
      "          1      0.777     0.611     0.684      1401\n",
      "\n",
      "avg / total      0.740     0.733     0.729      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.724\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.694     0.852     0.765      1560\n",
      "          1      0.779     0.582     0.666      1401\n",
      "\n",
      "avg / total      0.734     0.724     0.718      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.822     0.756      1560\n",
      "          1      0.754     0.607     0.673      1401\n",
      "\n",
      "avg / total      0.726     0.721     0.717      2961\n",
      "\n",
      "Testing features: 1, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.687     0.812     0.744      1560\n",
      "          1      0.737     0.588     0.654      1401\n",
      "\n",
      "avg / total      0.711     0.706     0.702      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.849     0.827      1560\n",
      "          1      0.821     0.771     0.795      1401\n",
      "\n",
      "avg / total      0.813     0.812     0.812      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.860     0.826      1560\n",
      "          1      0.828     0.752     0.788      1401\n",
      "\n",
      "avg / total      0.810     0.809     0.808      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.864     0.829      1560\n",
      "          1      0.833     0.754     0.792      1401\n",
      "\n",
      "avg / total      0.814     0.812     0.811      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.856     0.812      1560\n",
      "          1      0.818     0.720     0.766      1401\n",
      "\n",
      "avg / total      0.794     0.792     0.790      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.774     0.860     0.815      1560\n",
      "          1      0.822     0.721     0.768      1401\n",
      "\n",
      "avg / total      0.797     0.794     0.793      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.799     0.860     0.828      1560\n",
      "          1      0.830     0.759     0.793      1401\n",
      "\n",
      "avg / total      0.813     0.812     0.812      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.865     0.826      1560\n",
      "          1      0.832     0.745     0.786      1401\n",
      "\n",
      "avg / total      0.810     0.808     0.807      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.873     0.825      1560\n",
      "          1      0.838     0.729     0.779      1401\n",
      "\n",
      "avg / total      0.808     0.805     0.803      2961\n",
      "\n",
      "Testing features: 1, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.843     0.825      1560\n",
      "          1      0.816     0.778     0.797      1401\n",
      "\n",
      "avg / total      0.812     0.812     0.812      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.754     0.792     0.772      1560\n",
      "          1      0.754     0.712     0.733      1401\n",
      "\n",
      "avg / total      0.754     0.754     0.754      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.746     0.792     0.769      1560\n",
      "          1      0.752     0.700     0.725      1401\n",
      "\n",
      "avg / total      0.749     0.749     0.748      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.738     0.786     0.761      1560\n",
      "          1      0.743     0.689     0.715      1401\n",
      "\n",
      "avg / total      0.740     0.740     0.739      2961\n",
      "\n",
      "Testing features: 1, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.778     0.741      1560\n",
      "          1      0.722     0.641     0.679      1401\n",
      "\n",
      "avg / total      0.714     0.713     0.712      2961\n",
      "\n",
      "Testing features: 1, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.797     0.745      1560\n",
      "          1      0.733     0.620     0.672      1401\n",
      "\n",
      "avg / total      0.716     0.713     0.711      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.883     0.828      1560\n",
      "          1      0.848     0.722     0.780      1401\n",
      "\n",
      "avg / total      0.812     0.807     0.805      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.883     0.828      1560\n",
      "          1      0.848     0.722     0.780      1401\n",
      "\n",
      "avg / total      0.812     0.807     0.805      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.893     0.835      1560\n",
      "          1      0.859     0.727     0.787      1401\n",
      "\n",
      "avg / total      0.820     0.814     0.813      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.888     0.832      1560\n",
      "          1      0.853     0.725     0.784      1401\n",
      "\n",
      "avg / total      0.816     0.811     0.809      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.896     0.833      1560\n",
      "          1      0.861     0.714     0.781      1401\n",
      "\n",
      "avg / total      0.817     0.810     0.808      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.884     0.831      1560\n",
      "          1      0.849     0.729     0.784      1401\n",
      "\n",
      "avg / total      0.815     0.811     0.809      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.894     0.831      1560\n",
      "          1      0.858     0.713     0.779      1401\n",
      "\n",
      "avg / total      0.815     0.808     0.806      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.874     0.779      1560\n",
      "          1      0.807     0.586     0.679      1401\n",
      "\n",
      "avg / total      0.752     0.738     0.731      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.889     0.782      1560\n",
      "          1      0.823     0.572     0.675      1401\n",
      "\n",
      "avg / total      0.757     0.739     0.732      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.712     0.876     0.786      1560\n",
      "          1      0.815     0.605     0.695      1401\n",
      "\n",
      "avg / total      0.761     0.748     0.743      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.885     0.782      1560\n",
      "          1      0.818     0.578     0.678      1401\n",
      "\n",
      "avg / total      0.756     0.740     0.732      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.860     0.773      1560\n",
      "          1      0.792     0.594     0.679      1401\n",
      "\n",
      "avg / total      0.744     0.734     0.728      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.883     0.786      1560\n",
      "          1      0.820     0.596     0.690      1401\n",
      "\n",
      "avg / total      0.761     0.747     0.741      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.704     0.901     0.791      1560\n",
      "          1      0.840     0.578     0.685      1401\n",
      "\n",
      "avg / total      0.768     0.748     0.741      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.881     0.785      1560\n",
      "          1      0.819     0.596     0.690      1401\n",
      "\n",
      "avg / total      0.761     0.746     0.740      2961\n",
      "\n",
      "Testing features: 2, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.705     0.879     0.782      1560\n",
      "          1      0.815     0.590     0.684      1401\n",
      "\n",
      "avg / total      0.757     0.742     0.736      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.885     0.828      1560\n",
      "          1      0.848     0.718     0.778      1401\n",
      "\n",
      "avg / total      0.811     0.806     0.804      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.887     0.827      1560\n",
      "          1      0.850     0.714     0.776      1401\n",
      "\n",
      "avg / total      0.811     0.805     0.803      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.885     0.805      1560\n",
      "          1      0.835     0.649     0.730      1401\n",
      "\n",
      "avg / total      0.784     0.773     0.769      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.888     0.809      1560\n",
      "          1      0.841     0.656     0.737      1401\n",
      "\n",
      "avg / total      0.789     0.778     0.775      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.878     0.803      1560\n",
      "          1      0.828     0.656     0.732      1401\n",
      "\n",
      "avg / total      0.781     0.773     0.769      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.784\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.886     0.812      1560\n",
      "          1      0.841     0.670     0.745      1401\n",
      "\n",
      "avg / total      0.792     0.784     0.780      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.784\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.747     0.892     0.813      1560\n",
      "          1      0.847     0.663     0.744      1401\n",
      "\n",
      "avg / total      0.794     0.784     0.780      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.888     0.809      1560\n",
      "          1      0.840     0.657     0.738      1401\n",
      "\n",
      "avg / total      0.789     0.779     0.775      2961\n",
      "\n",
      "Testing features: 2, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.892     0.807      1560\n",
      "          1      0.843     0.645     0.731      1401\n",
      "\n",
      "avg / total      0.787     0.775     0.771      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.867     0.776      1560\n",
      "          1      0.799     0.592     0.680      1401\n",
      "\n",
      "avg / total      0.748     0.737     0.731      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.868     0.777      1560\n",
      "          1      0.801     0.591     0.680      1401\n",
      "\n",
      "avg / total      0.749     0.737     0.731      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.842     0.764      1560\n",
      "          1      0.773     0.599     0.675      1401\n",
      "\n",
      "avg / total      0.734     0.727     0.722      2961\n",
      "\n",
      "Testing features: 2, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.690\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.663     0.837     0.740      1560\n",
      "          1      0.743     0.527     0.617      1401\n",
      "\n",
      "avg / total      0.701     0.690     0.682      2961\n",
      "\n",
      "Testing features: 2, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.700\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.666     0.863     0.752      1560\n",
      "          1      0.772     0.518     0.620      1401\n",
      "\n",
      "avg / total      0.716     0.700     0.690      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.893     0.835      1560\n",
      "          1      0.859     0.725     0.786      1401\n",
      "\n",
      "avg / total      0.819     0.814     0.812      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.894     0.833      1560\n",
      "          1      0.859     0.717     0.782      1401\n",
      "\n",
      "avg / total      0.817     0.811     0.809      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.885     0.832      1560\n",
      "          1      0.850     0.730     0.786      1401\n",
      "\n",
      "avg / total      0.816     0.812     0.810      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.878     0.828      1560\n",
      "          1      0.843     0.731     0.783      1401\n",
      "\n",
      "avg / total      0.812     0.808     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.883     0.829      1560\n",
      "          1      0.847     0.724     0.781      1401\n",
      "\n",
      "avg / total      0.812     0.807     0.806      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.892     0.834      1560\n",
      "          1      0.857     0.725     0.786      1401\n",
      "\n",
      "avg / total      0.818     0.813     0.811      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.774     0.894     0.830      1560\n",
      "          1      0.857     0.710     0.777      1401\n",
      "\n",
      "avg / total      0.814     0.807     0.805      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.885     0.832      1560\n",
      "          1      0.851     0.732     0.787      1401\n",
      "\n",
      "avg / total      0.817     0.812     0.811      2961\n",
      "\n",
      "Testing features: 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.881     0.828      1560\n",
      "          1      0.845     0.726     0.781      1401\n",
      "\n",
      "avg / total      0.812     0.807     0.806      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.854     0.778      1560\n",
      "          1      0.792     0.620     0.695      1401\n",
      "\n",
      "avg / total      0.751     0.743     0.739      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.852     0.775      1560\n",
      "          1      0.788     0.613     0.690      1401\n",
      "\n",
      "avg / total      0.747     0.739     0.734      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.835     0.769      1560\n",
      "          1      0.773     0.625     0.691      1401\n",
      "\n",
      "avg / total      0.741     0.736     0.732      2961\n",
      "\n",
      "Testing features: 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.685     0.826     0.749      1560\n",
      "          1      0.749     0.578     0.652      1401\n",
      "\n",
      "avg / total      0.715     0.709     0.703      2961\n",
      "\n",
      "Testing features: 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.690     0.839     0.757      1560\n",
      "          1      0.764     0.580     0.659      1401\n",
      "\n",
      "avg / total      0.725     0.716     0.711      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.887     0.803      1560\n",
      "          1      0.836     0.640     0.725      1401\n",
      "\n",
      "avg / total      0.782     0.770     0.766      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.730     0.876     0.796      1560\n",
      "          1      0.822     0.640     0.719      1401\n",
      "\n",
      "avg / total      0.774     0.764     0.760      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.731     0.881     0.799      1560\n",
      "          1      0.829     0.639     0.721      1401\n",
      "\n",
      "avg / total      0.777     0.767     0.762      2961\n",
      "\n",
      "Testing features: 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.720     0.870     0.788      1560\n",
      "          1      0.812     0.624     0.705      1401\n",
      "\n",
      "avg / total      0.763     0.753     0.749      2961\n",
      "\n",
      "Testing features: 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.904     0.804      1560\n",
      "          1      0.853     0.615     0.715      1401\n",
      "\n",
      "avg / total      0.785     0.768     0.762      2961\n",
      "\n",
      "Testing features: 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.675     0.815     0.738      1560\n",
      "          1      0.732     0.562     0.636      1401\n",
      "\n",
      "avg / total      0.702     0.696     0.690      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.808     0.878     0.842      1560\n",
      "          1      0.850     0.768     0.807      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.872     0.835      1560\n",
      "          1      0.842     0.759     0.798      1401\n",
      "\n",
      "avg / total      0.820     0.818     0.817      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.826     0.774      1560\n",
      "          1      0.772     0.658     0.711      1401\n",
      "\n",
      "avg / total      0.749     0.746     0.744      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.852     0.786      1560\n",
      "          1      0.797     0.648     0.715      1401\n",
      "\n",
      "avg / total      0.761     0.755     0.752      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.851     0.793      1560\n",
      "          1      0.802     0.670     0.730      1401\n",
      "\n",
      "avg / total      0.770     0.766     0.763      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.746     0.878     0.807      1560\n",
      "          1      0.831     0.668     0.741      1401\n",
      "\n",
      "avg / total      0.786     0.778     0.775      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.755     0.846     0.797      1560\n",
      "          1      0.801     0.694     0.744      1401\n",
      "\n",
      "avg / total      0.777     0.774     0.772      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.858     0.797      1560\n",
      "          1      0.809     0.673     0.735      1401\n",
      "\n",
      "avg / total      0.775     0.770     0.768      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.851     0.789      1560\n",
      "          1      0.799     0.657     0.721      1401\n",
      "\n",
      "avg / total      0.765     0.760     0.757      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.789\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.771     0.852     0.809      1560\n",
      "          1      0.813     0.718     0.763      1401\n",
      "\n",
      "avg / total      0.791     0.789     0.787      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.861     0.815      1560\n",
      "          1      0.823     0.719     0.767      1401\n",
      "\n",
      "avg / total      0.797     0.794     0.792      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.770     0.865     0.815      1560\n",
      "          1      0.826     0.712     0.765      1401\n",
      "\n",
      "avg / total      0.797     0.793     0.791      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.871     0.821      1560\n",
      "          1      0.833     0.720     0.773      1401\n",
      "\n",
      "avg / total      0.803     0.799     0.798      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.871     0.828      1560\n",
      "          1      0.837     0.740     0.786      1401\n",
      "\n",
      "avg / total      0.811     0.809     0.808      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.875     0.826      1560\n",
      "          1      0.839     0.727     0.779      1401\n",
      "\n",
      "avg / total      0.809     0.805     0.804      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.860     0.815      1560\n",
      "          1      0.823     0.722     0.769      1401\n",
      "\n",
      "avg / total      0.797     0.795     0.793      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.716     0.818     0.764      1560\n",
      "          1      0.759     0.639     0.694      1401\n",
      "\n",
      "avg / total      0.736     0.733     0.731      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.728     0.833     0.777      1560\n",
      "          1      0.778     0.654     0.711      1401\n",
      "\n",
      "avg / total      0.752     0.748     0.746      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.725\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.787     0.751      1560\n",
      "          1      0.735     0.657     0.694      1401\n",
      "\n",
      "avg / total      0.726     0.725     0.724      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.722     0.804     0.761      1560\n",
      "          1      0.750     0.656     0.700      1401\n",
      "\n",
      "avg / total      0.736     0.734     0.732      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.715\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.804     0.748      1560\n",
      "          1      0.738     0.616     0.672      1401\n",
      "\n",
      "avg / total      0.718     0.715     0.712      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.732     0.817     0.772      1560\n",
      "          1      0.766     0.667     0.713      1401\n",
      "\n",
      "avg / total      0.748     0.746     0.744      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.837     0.776      1560\n",
      "          1      0.780     0.645     0.706      1401\n",
      "\n",
      "avg / total      0.751     0.746     0.743      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.830     0.763      1560\n",
      "          1      0.765     0.615     0.681      1401\n",
      "\n",
      "avg / total      0.734     0.728     0.724      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.732     0.816     0.772      1560\n",
      "          1      0.765     0.667     0.713      1401\n",
      "\n",
      "avg / total      0.748     0.746     0.744      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.863     0.831      1560\n",
      "          1      0.833     0.762     0.796      1401\n",
      "\n",
      "avg / total      0.816     0.815     0.814      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.874     0.834      1560\n",
      "          1      0.843     0.752     0.795      1401\n",
      "\n",
      "avg / total      0.819     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.869     0.837      1560\n",
      "          1      0.840     0.769     0.803      1401\n",
      "\n",
      "avg / total      0.823     0.821     0.821      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.880     0.838      1560\n",
      "          1      0.850     0.754     0.799      1401\n",
      "\n",
      "avg / total      0.823     0.821     0.820      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.879     0.835      1560\n",
      "          1      0.848     0.746     0.793      1401\n",
      "\n",
      "avg / total      0.819     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.881     0.833      1560\n",
      "          1      0.848     0.738     0.789      1401\n",
      "\n",
      "avg / total      0.817     0.814     0.812      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.871     0.829      1560\n",
      "          1      0.838     0.744     0.789      1401\n",
      "\n",
      "avg / total      0.814     0.811     0.810      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.805     0.776      1560\n",
      "          1      0.763     0.700     0.730      1401\n",
      "\n",
      "avg / total      0.756     0.755     0.754      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.757\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.827     0.782      1560\n",
      "          1      0.779     0.679     0.725      1401\n",
      "\n",
      "avg / total      0.759     0.757     0.755      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.776     0.744      1560\n",
      "          1      0.724     0.655     0.688      1401\n",
      "\n",
      "avg / total      0.719     0.719     0.717      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.808     0.753      1560\n",
      "          1      0.745     0.625     0.680      1401\n",
      "\n",
      "avg / total      0.724     0.721     0.719      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.718\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.798     0.749      1560\n",
      "          1      0.737     0.630     0.679      1401\n",
      "\n",
      "avg / total      0.720     0.718     0.716      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.821     0.764      1560\n",
      "          1      0.761     0.634     0.692      1401\n",
      "\n",
      "avg / total      0.736     0.733     0.730      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.698     0.831     0.758      1560\n",
      "          1      0.761     0.599     0.670      1401\n",
      "\n",
      "avg / total      0.727     0.721     0.717      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.818     0.754      1560\n",
      "          1      0.750     0.610     0.673      1401\n",
      "\n",
      "avg / total      0.724     0.719     0.716      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.701\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.683     0.807     0.740      1560\n",
      "          1      0.731     0.584     0.649      1401\n",
      "\n",
      "avg / total      0.706     0.701     0.697      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.849     0.819      1560\n",
      "          1      0.817     0.750     0.782      1401\n",
      "\n",
      "avg / total      0.803     0.802     0.801      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.853     0.823      1560\n",
      "          1      0.822     0.754     0.786      1401\n",
      "\n",
      "avg / total      0.807     0.806     0.805      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.847     0.813      1560\n",
      "          1      0.812     0.736     0.772      1401\n",
      "\n",
      "avg / total      0.796     0.795     0.794      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.791\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.857     0.812      1560\n",
      "          1      0.819     0.718     0.765      1401\n",
      "\n",
      "avg / total      0.794     0.791     0.790      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.782\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.766     0.844     0.803      1560\n",
      "          1      0.804     0.713     0.756      1401\n",
      "\n",
      "avg / total      0.784     0.782     0.781      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.846     0.819      1560\n",
      "          1      0.815     0.756     0.784      1401\n",
      "\n",
      "avg / total      0.804     0.803     0.803      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.858     0.817      1560\n",
      "          1      0.822     0.729     0.773      1401\n",
      "\n",
      "avg / total      0.800     0.797     0.796      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.852     0.815      1560\n",
      "          1      0.817     0.734     0.773      1401\n",
      "\n",
      "avg / total      0.798     0.796     0.795      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.838     0.812      1560\n",
      "          1      0.806     0.748     0.776      1401\n",
      "\n",
      "avg / total      0.796     0.795     0.795      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.753     0.769     0.761      1560\n",
      "          1      0.737     0.719     0.728      1401\n",
      "\n",
      "avg / total      0.745     0.745     0.745      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.786     0.764      1560\n",
      "          1      0.745     0.697     0.721      1401\n",
      "\n",
      "avg / total      0.744     0.744     0.743      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.732     0.778     0.754      1560\n",
      "          1      0.734     0.683     0.708      1401\n",
      "\n",
      "avg / total      0.733     0.733     0.732      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.769     0.734      1560\n",
      "          1      0.712     0.636     0.672      1401\n",
      "\n",
      "avg / total      0.707     0.706     0.705      2961\n",
      "\n",
      "Testing features: 0, 1, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.700     0.775     0.735      1560\n",
      "          1      0.715     0.630     0.670      1401\n",
      "\n",
      "avg / total      0.707     0.706     0.704      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.801\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.872     0.822      1560\n",
      "          1      0.836     0.722     0.775      1401\n",
      "\n",
      "avg / total      0.805     0.801     0.800      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.879     0.826      1560\n",
      "          1      0.843     0.722     0.777      1401\n",
      "\n",
      "avg / total      0.809     0.804     0.803      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.885     0.831      1560\n",
      "          1      0.851     0.727     0.784      1401\n",
      "\n",
      "avg / total      0.815     0.811     0.809      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.898     0.834      1560\n",
      "          1      0.863     0.716     0.783      1401\n",
      "\n",
      "avg / total      0.819     0.812     0.810      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.895     0.837      1560\n",
      "          1      0.862     0.730     0.791      1401\n",
      "\n",
      "avg / total      0.822     0.817     0.815      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.888     0.830      1560\n",
      "          1      0.853     0.720     0.781      1401\n",
      "\n",
      "avg / total      0.814     0.809     0.807      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.880     0.827      1560\n",
      "          1      0.844     0.724     0.780      1401\n",
      "\n",
      "avg / total      0.811     0.806     0.805      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.849     0.774      1560\n",
      "          1      0.786     0.615     0.690      1401\n",
      "\n",
      "avg / total      0.746     0.738     0.734      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.876     0.784      1560\n",
      "          1      0.813     0.600     0.690      1401\n",
      "\n",
      "avg / total      0.758     0.745     0.740      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.828     0.770      1560\n",
      "          1      0.769     0.640     0.699      1401\n",
      "\n",
      "avg / total      0.743     0.739     0.736      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.856     0.779      1560\n",
      "          1      0.794     0.619     0.696      1401\n",
      "\n",
      "avg / total      0.752     0.744     0.739      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.836     0.766      1560\n",
      "          1      0.771     0.615     0.684      1401\n",
      "\n",
      "avg / total      0.737     0.731     0.727      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.717     0.862     0.783      1560\n",
      "          1      0.801     0.622     0.700      1401\n",
      "\n",
      "avg / total      0.757     0.748     0.744      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.882     0.788      1560\n",
      "          1      0.821     0.602     0.694      1401\n",
      "\n",
      "avg / total      0.763     0.749     0.744      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.855     0.775      1560\n",
      "          1      0.791     0.609     0.688      1401\n",
      "\n",
      "avg / total      0.747     0.739     0.734      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.849     0.776      1560\n",
      "          1      0.787     0.621     0.694      1401\n",
      "\n",
      "avg / total      0.749     0.741     0.737      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.866     0.818      1560\n",
      "          1      0.829     0.721     0.771      1401\n",
      "\n",
      "avg / total      0.801     0.797     0.796      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.771     0.869     0.817      1560\n",
      "          1      0.830     0.713     0.767      1401\n",
      "\n",
      "avg / total      0.799     0.795     0.794      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.861     0.797      1560\n",
      "          1      0.811     0.665     0.731      1401\n",
      "\n",
      "avg / total      0.774     0.768     0.766      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.870     0.800      1560\n",
      "          1      0.820     0.660     0.731      1401\n",
      "\n",
      "avg / total      0.778     0.770     0.767      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.846     0.787      1560\n",
      "          1      0.794     0.660     0.721      1401\n",
      "\n",
      "avg / total      0.763     0.758     0.756      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.886     0.808      1560\n",
      "          1      0.838     0.660     0.738      1401\n",
      "\n",
      "avg / total      0.788     0.779     0.775      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.884     0.807      1560\n",
      "          1      0.836     0.658     0.736      1401\n",
      "\n",
      "avg / total      0.787     0.777     0.774      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.874     0.801      1560\n",
      "          1      0.823     0.656     0.730      1401\n",
      "\n",
      "avg / total      0.779     0.771     0.767      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.871     0.798      1560\n",
      "          1      0.819     0.652     0.726      1401\n",
      "\n",
      "avg / total      0.775     0.767     0.764      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.724\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.707     0.811     0.756      1560\n",
      "          1      0.749     0.627     0.682      1401\n",
      "\n",
      "avg / total      0.727     0.724     0.721      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.854     0.773      1560\n",
      "          1      0.788     0.603     0.683      1401\n",
      "\n",
      "avg / total      0.744     0.735     0.730      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.703     0.821     0.757      1560\n",
      "          1      0.754     0.613     0.676      1401\n",
      "\n",
      "avg / total      0.727     0.722     0.719      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.677\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.663     0.788     0.720      1560\n",
      "          1      0.701     0.553     0.619      1401\n",
      "\n",
      "avg / total      0.681     0.677     0.672      2961\n",
      "\n",
      "Testing features: 0, 2, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.669     0.833     0.742      1560\n",
      "          1      0.745     0.541     0.627      1401\n",
      "\n",
      "avg / total      0.705     0.695     0.688      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.876     0.828      1560\n",
      "          1      0.841     0.734     0.784      1401\n",
      "\n",
      "avg / total      0.812     0.809     0.808      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.888     0.831      1560\n",
      "          1      0.853     0.723     0.783      1401\n",
      "\n",
      "avg / total      0.815     0.810     0.808      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.869     0.824      1560\n",
      "          1      0.834     0.733     0.780      1401\n",
      "\n",
      "avg / total      0.807     0.804     0.803      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.863     0.824      1560\n",
      "          1      0.830     0.740     0.782      1401\n",
      "\n",
      "avg / total      0.807     0.805     0.804      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.771     0.860     0.813      1560\n",
      "          1      0.821     0.716     0.765      1401\n",
      "\n",
      "avg / total      0.795     0.792     0.790      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.881     0.831      1560\n",
      "          1      0.847     0.732     0.785      1401\n",
      "\n",
      "avg / total      0.815     0.811     0.809      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.891     0.836      1560\n",
      "          1      0.858     0.732     0.790      1401\n",
      "\n",
      "avg / total      0.821     0.816     0.814      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.796     0.875     0.834      1560\n",
      "          1      0.843     0.750     0.794      1401\n",
      "\n",
      "avg / total      0.818     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.801\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.875     0.823      1560\n",
      "          1      0.838     0.719     0.774      1401\n",
      "\n",
      "avg / total      0.806     0.801     0.800      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.824     0.763      1560\n",
      "          1      0.762     0.627     0.688      1401\n",
      "\n",
      "avg / total      0.735     0.731     0.728      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.712     0.844     0.772      1560\n",
      "          1      0.781     0.620     0.691      1401\n",
      "\n",
      "avg / total      0.745     0.738     0.734      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.725\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.821     0.759      1560\n",
      "          1      0.756     0.619     0.681      1401\n",
      "\n",
      "avg / total      0.729     0.725     0.722      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.697\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.687     0.783     0.732      1560\n",
      "          1      0.713     0.602     0.653      1401\n",
      "\n",
      "avg / total      0.699     0.697     0.695      2961\n",
      "\n",
      "Testing features: 0, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.690     0.812     0.746      1560\n",
      "          1      0.739     0.594     0.658      1401\n",
      "\n",
      "avg / total      0.713     0.709     0.704      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.872     0.795      1560\n",
      "          1      0.818     0.640     0.718      1401\n",
      "\n",
      "avg / total      0.771     0.762     0.758      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.726     0.866     0.790      1560\n",
      "          1      0.810     0.637     0.713      1401\n",
      "\n",
      "avg / total      0.766     0.758     0.754      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.862     0.790      1560\n",
      "          1      0.807     0.644     0.716      1401\n",
      "\n",
      "avg / total      0.766     0.759     0.755      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.840     0.778      1560\n",
      "          1      0.783     0.644     0.707      1401\n",
      "\n",
      "avg / total      0.752     0.747     0.744      2961\n",
      "\n",
      "Testing features: 0, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.730     0.880     0.798      1560\n",
      "          1      0.827     0.637     0.719      1401\n",
      "\n",
      "avg / total      0.776     0.765     0.761      2961\n",
      "\n",
      "Testing features: 0, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.689\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.679     0.779     0.726      1560\n",
      "          1      0.706     0.589     0.642      1401\n",
      "\n",
      "avg / total      0.691     0.689     0.686      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.879     0.840      1560\n",
      "          1      0.850     0.762     0.803      1401\n",
      "\n",
      "avg / total      0.826     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.873     0.837      1560\n",
      "          1      0.844     0.762     0.801      1401\n",
      "\n",
      "avg / total      0.822     0.821     0.820      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.879     0.842      1560\n",
      "          1      0.850     0.767     0.806      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.831\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.814     0.880     0.846      1560\n",
      "          1      0.853     0.776     0.813      1401\n",
      "\n",
      "avg / total      0.832     0.831     0.830      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.882     0.844      1560\n",
      "          1      0.854     0.768     0.809      1401\n",
      "\n",
      "avg / total      0.830     0.828     0.827      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.865     0.837      1560\n",
      "          1      0.838     0.776     0.806      1401\n",
      "\n",
      "avg / total      0.824     0.823     0.822      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.872     0.839      1560\n",
      "          1      0.844     0.771     0.806      1401\n",
      "\n",
      "avg / total      0.825     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.846     0.791      1560\n",
      "          1      0.797     0.675     0.731      1401\n",
      "\n",
      "avg / total      0.769     0.765     0.762      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.863     0.793      1560\n",
      "          1      0.810     0.650     0.721      1401\n",
      "\n",
      "avg / total      0.770     0.762     0.759      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.759     0.853     0.803      1560\n",
      "          1      0.810     0.698     0.750      1401\n",
      "\n",
      "avg / total      0.783     0.780     0.778      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.743     0.865     0.799      1560\n",
      "          1      0.816     0.667     0.734      1401\n",
      "\n",
      "avg / total      0.778     0.771     0.769      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.728     0.842     0.781      1560\n",
      "          1      0.787     0.650     0.712      1401\n",
      "\n",
      "avg / total      0.756     0.751     0.748      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.755     0.859     0.804      1560\n",
      "          1      0.815     0.690     0.747      1401\n",
      "\n",
      "avg / total      0.783     0.779     0.777      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.874     0.807      1560\n",
      "          1      0.827     0.675     0.743      1401\n",
      "\n",
      "avg / total      0.786     0.779     0.777      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.776\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.863     0.802      1560\n",
      "          1      0.817     0.678     0.741      1401\n",
      "\n",
      "avg / total      0.781     0.776     0.773      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.756     0.854     0.802      1560\n",
      "          1      0.810     0.693     0.747      1401\n",
      "\n",
      "avg / total      0.781     0.778     0.776      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.873     0.834      1560\n",
      "          1      0.842     0.754     0.796      1401\n",
      "\n",
      "avg / total      0.819     0.817     0.816      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.872     0.833      1560\n",
      "          1      0.841     0.754     0.795      1401\n",
      "\n",
      "avg / total      0.818     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.865     0.826      1560\n",
      "          1      0.832     0.746     0.787      1401\n",
      "\n",
      "avg / total      0.811     0.809     0.808      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.870     0.826      1560\n",
      "          1      0.836     0.737     0.783      1401\n",
      "\n",
      "avg / total      0.810     0.807     0.806      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.857     0.813      1560\n",
      "          1      0.819     0.719     0.766      1401\n",
      "\n",
      "avg / total      0.795     0.792     0.791      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.874     0.828      1560\n",
      "          1      0.840     0.736     0.784      1401\n",
      "\n",
      "avg / total      0.812     0.809     0.807      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.878     0.828      1560\n",
      "          1      0.843     0.728     0.781      1401\n",
      "\n",
      "avg / total      0.811     0.807     0.806      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.869     0.819      1560\n",
      "          1      0.832     0.719     0.771      1401\n",
      "\n",
      "avg / total      0.802     0.798     0.796      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.867     0.825      1560\n",
      "          1      0.833     0.738     0.783      1401\n",
      "\n",
      "avg / total      0.809     0.806     0.805      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.750     0.840     0.792      1560\n",
      "          1      0.794     0.689     0.738      1401\n",
      "\n",
      "avg / total      0.771     0.768     0.767      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.844     0.790      1560\n",
      "          1      0.795     0.676     0.731      1401\n",
      "\n",
      "avg / total      0.768     0.764     0.762      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.822     0.773      1560\n",
      "          1      0.769     0.660     0.710      1401\n",
      "\n",
      "avg / total      0.748     0.745     0.743      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.797     0.758      1560\n",
      "          1      0.745     0.660     0.700      1401\n",
      "\n",
      "avg / total      0.733     0.732     0.731      2961\n",
      "\n",
      "Testing features: 1, 2, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.828     0.779      1560\n",
      "          1      0.777     0.668     0.719      1401\n",
      "\n",
      "avg / total      0.755     0.752     0.750      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.872     0.837      1560\n",
      "          1      0.843     0.764     0.802      1401\n",
      "\n",
      "avg / total      0.823     0.821     0.821      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.877     0.840      1560\n",
      "          1      0.848     0.765     0.805      1401\n",
      "\n",
      "avg / total      0.826     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.872     0.838      1560\n",
      "          1      0.843     0.765     0.802      1401\n",
      "\n",
      "avg / total      0.823     0.822     0.821      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.803     0.883     0.841      1560\n",
      "          1      0.853     0.759     0.804      1401\n",
      "\n",
      "avg / total      0.827     0.824     0.823      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.881     0.839      1560\n",
      "          1      0.851     0.756     0.801      1401\n",
      "\n",
      "avg / total      0.825     0.822     0.821      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.881     0.842      1560\n",
      "          1      0.853     0.764     0.806      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.881     0.837      1560\n",
      "          1      0.850     0.751     0.798      1401\n",
      "\n",
      "avg / total      0.823     0.820     0.819      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.869     0.836      1560\n",
      "          1      0.840     0.767     0.802      1401\n",
      "\n",
      "avg / total      0.822     0.821     0.820      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.793     0.880     0.834      1560\n",
      "          1      0.848     0.744     0.793      1401\n",
      "\n",
      "avg / total      0.819     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.761     0.826     0.792      1560\n",
      "          1      0.786     0.712     0.747      1401\n",
      "\n",
      "avg / total      0.773     0.772     0.771      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.835     0.790      1560\n",
      "          1      0.790     0.688     0.735      1401\n",
      "\n",
      "avg / total      0.768     0.766     0.764      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.816     0.778      1560\n",
      "          1      0.770     0.687     0.727      1401\n",
      "\n",
      "avg / total      0.756     0.755     0.754      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.716     0.805     0.758      1560\n",
      "          1      0.748     0.645     0.693      1401\n",
      "\n",
      "avg / total      0.732     0.729     0.727      2961\n",
      "\n",
      "Testing features: 1, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.824     0.761      1560\n",
      "          1      0.760     0.622     0.684      1401\n",
      "\n",
      "avg / total      0.733     0.728     0.725      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.812     0.853     0.832      1560\n",
      "          1      0.827     0.779     0.802      1401\n",
      "\n",
      "avg / total      0.819     0.818     0.818      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.850     0.829      1560\n",
      "          1      0.823     0.776     0.799      1401\n",
      "\n",
      "avg / total      0.815     0.815     0.815      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.793     0.858     0.824      1560\n",
      "          1      0.826     0.750     0.786      1401\n",
      "\n",
      "avg / total      0.808     0.807     0.806      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.838     0.812      1560\n",
      "          1      0.806     0.747     0.775      1401\n",
      "\n",
      "avg / total      0.796     0.795     0.794      2961\n",
      "\n",
      "Testing features: 1, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.796     0.841     0.818      1560\n",
      "          1      0.811     0.760     0.785      1401\n",
      "\n",
      "avg / total      0.803     0.803     0.802      2961\n",
      "\n",
      "Testing features: 1, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.774     0.760      1560\n",
      "          1      0.737     0.705     0.721      1401\n",
      "\n",
      "avg / total      0.741     0.742     0.741      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.883     0.830      1560\n",
      "          1      0.848     0.727     0.783      1401\n",
      "\n",
      "avg / total      0.814     0.810     0.808      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.883     0.828      1560\n",
      "          1      0.847     0.721     0.779      1401\n",
      "\n",
      "avg / total      0.811     0.806     0.804      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.873     0.825      1560\n",
      "          1      0.838     0.730     0.780      1401\n",
      "\n",
      "avg / total      0.809     0.805     0.804      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.880     0.833      1560\n",
      "          1      0.847     0.740     0.790      1401\n",
      "\n",
      "avg / total      0.817     0.814     0.813      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.858     0.819      1560\n",
      "          1      0.823     0.737     0.777      1401\n",
      "\n",
      "avg / total      0.802     0.800     0.799      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.892     0.834      1560\n",
      "          1      0.858     0.726     0.786      1401\n",
      "\n",
      "avg / total      0.819     0.813     0.811      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.891     0.834      1560\n",
      "          1      0.857     0.726     0.786      1401\n",
      "\n",
      "avg / total      0.818     0.813     0.811      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.881     0.829      1560\n",
      "          1      0.846     0.727     0.782      1401\n",
      "\n",
      "avg / total      0.813     0.809     0.807      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.883     0.826      1560\n",
      "          1      0.847     0.717     0.776      1401\n",
      "\n",
      "avg / total      0.810     0.804     0.803      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.863     0.781      1560\n",
      "          1      0.800     0.612     0.694      1401\n",
      "\n",
      "avg / total      0.754     0.744     0.740      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.708     0.879     0.784      1560\n",
      "          1      0.816     0.597     0.689      1401\n",
      "\n",
      "avg / total      0.759     0.745     0.739      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.854     0.773      1560\n",
      "          1      0.788     0.603     0.683      1401\n",
      "\n",
      "avg / total      0.745     0.736     0.731      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.709     0.862     0.778      1560\n",
      "          1      0.798     0.606     0.689      1401\n",
      "\n",
      "avg / total      0.751     0.741     0.736      2961\n",
      "\n",
      "Testing features: 2, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.873     0.786      1560\n",
      "          1      0.812     0.611     0.697      1401\n",
      "\n",
      "avg / total      0.761     0.749     0.744      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.872     0.824      1560\n",
      "          1      0.836     0.726     0.777      1401\n",
      "\n",
      "avg / total      0.807     0.803     0.802      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.882     0.830      1560\n",
      "          1      0.848     0.730     0.785      1401\n",
      "\n",
      "avg / total      0.814     0.810     0.809      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.801\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.865     0.821      1560\n",
      "          1      0.829     0.731     0.777      1401\n",
      "\n",
      "avg / total      0.804     0.801     0.800      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.862     0.798      1560\n",
      "          1      0.813     0.669     0.734      1401\n",
      "\n",
      "avg / total      0.777     0.771     0.768      2961\n",
      "\n",
      "Testing features: 2, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.752     0.872     0.808      1560\n",
      "          1      0.827     0.680     0.746      1401\n",
      "\n",
      "avg / total      0.788     0.781     0.779      2961\n",
      "\n",
      "Testing features: 2, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.713     0.832     0.768      1560\n",
      "          1      0.770     0.627     0.692      1401\n",
      "\n",
      "avg / total      0.740     0.735     0.732      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.880     0.831      1560\n",
      "          1      0.846     0.734     0.786      1401\n",
      "\n",
      "avg / total      0.815     0.811     0.810      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.883     0.830      1560\n",
      "          1      0.848     0.728     0.783      1401\n",
      "\n",
      "avg / total      0.814     0.810     0.808      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.886     0.830      1560\n",
      "          1      0.851     0.724     0.782      1401\n",
      "\n",
      "avg / total      0.814     0.809     0.807      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.867     0.824      1560\n",
      "          1      0.832     0.735     0.781      1401\n",
      "\n",
      "avg / total      0.807     0.804     0.803      2961\n",
      "\n",
      "Testing features: 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.883     0.833      1560\n",
      "          1      0.850     0.734     0.788      1401\n",
      "\n",
      "avg / total      0.817     0.813     0.811      2961\n",
      "\n",
      "Testing features: 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.711     0.833     0.767      1560\n",
      "          1      0.771     0.623     0.689      1401\n",
      "\n",
      "avg / total      0.739     0.734     0.730      2961\n",
      "\n",
      "Testing features: 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.736     0.858     0.792      1560\n",
      "          1      0.806     0.658     0.725      1401\n",
      "\n",
      "avg / total      0.769     0.763     0.760      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.848     0.817      1560\n",
      "          1      0.815     0.746     0.779      1401\n",
      "\n",
      "avg / total      0.801     0.800     0.799      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.852     0.820      1560\n",
      "          1      0.820     0.749     0.783      1401\n",
      "\n",
      "avg / total      0.804     0.803     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.871     0.837      1560\n",
      "          1      0.842     0.768     0.803      1401\n",
      "\n",
      "avg / total      0.823     0.822     0.821      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.882     0.839      1560\n",
      "          1      0.852     0.755     0.801      1401\n",
      "\n",
      "avg / total      0.825     0.822     0.821      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.820     0.866     0.843      1560\n",
      "          1      0.841     0.789     0.814      1401\n",
      "\n",
      "avg / total      0.830     0.829     0.829      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.798     0.870     0.833      1560\n",
      "          1      0.839     0.755     0.795      1401\n",
      "\n",
      "avg / total      0.818     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.799     0.868     0.832      1560\n",
      "          1      0.837     0.757     0.795      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.822     0.773      1560\n",
      "          1      0.769     0.659     0.710      1401\n",
      "\n",
      "avg / total      0.748     0.745     0.743      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.757\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.732     0.849     0.787      1560\n",
      "          1      0.796     0.655     0.718      1401\n",
      "\n",
      "avg / total      0.763     0.757     0.754      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.807     0.769      1560\n",
      "          1      0.758     0.674     0.714      1401\n",
      "\n",
      "avg / total      0.745     0.744     0.743      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.740     0.826     0.780      1560\n",
      "          1      0.777     0.677     0.723      1401\n",
      "\n",
      "avg / total      0.757     0.755     0.753      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.742\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.727     0.815     0.769      1560\n",
      "          1      0.762     0.660     0.707      1401\n",
      "\n",
      "avg / total      0.744     0.742     0.740      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.753     0.829     0.789      1560\n",
      "          1      0.785     0.697     0.738      1401\n",
      "\n",
      "avg / total      0.768     0.766     0.765      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.752     0.855     0.800      1560\n",
      "          1      0.809     0.685     0.742      1401\n",
      "\n",
      "avg / total      0.779     0.775     0.773      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.839     0.782      1560\n",
      "          1      0.786     0.659     0.717      1401\n",
      "\n",
      "avg / total      0.758     0.754     0.751      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.833     0.788      1560\n",
      "          1      0.787     0.687     0.734      1401\n",
      "\n",
      "avg / total      0.766     0.764     0.762      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.801\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.847     0.818      1560\n",
      "          1      0.815     0.749     0.781      1401\n",
      "\n",
      "avg / total      0.802     0.801     0.800      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.853     0.818      1560\n",
      "          1      0.819     0.741     0.778      1401\n",
      "\n",
      "avg / total      0.802     0.800     0.799      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.838     0.804      1560\n",
      "          1      0.801     0.726     0.762      1401\n",
      "\n",
      "avg / total      0.786     0.785     0.784      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.851     0.813      1560\n",
      "          1      0.815     0.729     0.769      1401\n",
      "\n",
      "avg / total      0.795     0.793     0.792      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.765     0.833     0.798      1560\n",
      "          1      0.794     0.715     0.753      1401\n",
      "\n",
      "avg / total      0.779     0.777     0.776      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.858     0.819      1560\n",
      "          1      0.823     0.736     0.777      1401\n",
      "\n",
      "avg / total      0.802     0.800     0.799      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.778     0.869     0.821      1560\n",
      "          1      0.832     0.724     0.774      1401\n",
      "\n",
      "avg / total      0.803     0.800     0.799      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.789\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.771     0.853     0.810      1560\n",
      "          1      0.814     0.717     0.763      1401\n",
      "\n",
      "avg / total      0.791     0.789     0.788      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.854     0.817      1560\n",
      "          1      0.819     0.737     0.776      1401\n",
      "\n",
      "avg / total      0.800     0.799     0.798      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.727     0.797     0.760      1560\n",
      "          1      0.747     0.667     0.705      1401\n",
      "\n",
      "avg / total      0.737     0.736     0.734      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.808     0.771      1560\n",
      "          1      0.761     0.679     0.717      1401\n",
      "\n",
      "avg / total      0.748     0.747     0.746      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.806     0.762      1560\n",
      "          1      0.752     0.655     0.700      1401\n",
      "\n",
      "avg / total      0.737     0.735     0.733      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.702     0.770     0.734      1560\n",
      "          1      0.713     0.635     0.672      1401\n",
      "\n",
      "avg / total      0.707     0.706     0.705      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.724     0.807     0.763      1560\n",
      "          1      0.754     0.658     0.703      1401\n",
      "\n",
      "avg / total      0.738     0.737     0.735      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.808     0.853     0.830      1560\n",
      "          1      0.826     0.774     0.799      1401\n",
      "\n",
      "avg / total      0.816     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.867     0.829      1560\n",
      "          1      0.835     0.750     0.790      1401\n",
      "\n",
      "avg / total      0.813     0.812     0.811      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.854     0.829      1560\n",
      "          1      0.826     0.769     0.797      1401\n",
      "\n",
      "avg / total      0.815     0.814     0.814      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.803     0.862     0.831      1560\n",
      "          1      0.833     0.764     0.797      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.838     0.811      1560\n",
      "          1      0.805     0.745     0.774      1401\n",
      "\n",
      "avg / total      0.795     0.794     0.793      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.872     0.838      1560\n",
      "          1      0.843     0.768     0.804      1401\n",
      "\n",
      "avg / total      0.824     0.823     0.822      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.878     0.837      1560\n",
      "          1      0.848     0.755     0.799      1401\n",
      "\n",
      "avg / total      0.822     0.820     0.819      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.865     0.837      1560\n",
      "          1      0.838     0.776     0.806      1401\n",
      "\n",
      "avg / total      0.824     0.823     0.822      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.857     0.822      1560\n",
      "          1      0.824     0.744     0.782      1401\n",
      "\n",
      "avg / total      0.805     0.804     0.803      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.797     0.770      1560\n",
      "          1      0.754     0.695     0.724      1401\n",
      "\n",
      "avg / total      0.749     0.749     0.748      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.744     0.816     0.778      1560\n",
      "          1      0.770     0.687     0.727      1401\n",
      "\n",
      "avg / total      0.756     0.755     0.754      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.735\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.723     0.806     0.762      1560\n",
      "          1      0.752     0.655     0.700      1401\n",
      "\n",
      "avg / total      0.737     0.735     0.733      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.706     0.776     0.739      1560\n",
      "          1      0.719     0.640     0.677      1401\n",
      "\n",
      "avg / total      0.712     0.712     0.710      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.714     0.808     0.758      1560\n",
      "          1      0.749     0.640     0.690      1401\n",
      "\n",
      "avg / total      0.731     0.728     0.726      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.846     0.816      1560\n",
      "          1      0.813     0.749     0.780      1401\n",
      "\n",
      "avg / total      0.801     0.800     0.799      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.847     0.821      1560\n",
      "          1      0.817     0.760     0.787      1401\n",
      "\n",
      "avg / total      0.806     0.806     0.805      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.841     0.810      1560\n",
      "          1      0.807     0.738     0.771      1401\n",
      "\n",
      "avg / total      0.793     0.792     0.792      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.768     0.830     0.798      1560\n",
      "          1      0.792     0.720     0.754      1401\n",
      "\n",
      "avg / total      0.779     0.778     0.777      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.790\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.830     0.806      1560\n",
      "          1      0.797     0.744     0.770      1401\n",
      "\n",
      "avg / total      0.790     0.790     0.789      2961\n",
      "\n",
      "Testing features: 0, 1, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.735     0.767     0.751      1560\n",
      "          1      0.728     0.692     0.710      1401\n",
      "\n",
      "avg / total      0.732     0.732     0.731      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.863     0.817      1560\n",
      "          1      0.826     0.721     0.770      1401\n",
      "\n",
      "avg / total      0.799     0.796     0.795      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.869     0.820      1560\n",
      "          1      0.832     0.719     0.772      1401\n",
      "\n",
      "avg / total      0.802     0.798     0.797      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.780     0.856     0.817      1560\n",
      "          1      0.821     0.732     0.774      1401\n",
      "\n",
      "avg / total      0.799     0.797     0.796      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.862     0.824      1560\n",
      "          1      0.829     0.744     0.785      1401\n",
      "\n",
      "avg / total      0.808     0.806     0.806      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.787\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.839     0.806      1560\n",
      "          1      0.803     0.729     0.764      1401\n",
      "\n",
      "avg / total      0.788     0.787     0.786      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.879     0.831      1560\n",
      "          1      0.845     0.737     0.788      1401\n",
      "\n",
      "avg / total      0.815     0.812     0.811      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.791     0.878     0.832      1560\n",
      "          1      0.845     0.742     0.790      1401\n",
      "\n",
      "avg / total      0.817     0.814     0.812      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.869     0.818      1560\n",
      "          1      0.831     0.716     0.769      1401\n",
      "\n",
      "avg / total      0.800     0.797     0.795      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.863     0.817      1560\n",
      "          1      0.826     0.724     0.771      1401\n",
      "\n",
      "avg / total      0.800     0.797     0.796      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.721     0.823     0.768      1560\n",
      "          1      0.766     0.645     0.700      1401\n",
      "\n",
      "avg / total      0.742     0.739     0.736      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.716     0.856     0.780      1560\n",
      "          1      0.796     0.622     0.698      1401\n",
      "\n",
      "avg / total      0.754     0.746     0.741      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.830     0.766      1560\n",
      "          1      0.767     0.623     0.688      1401\n",
      "\n",
      "avg / total      0.737     0.732     0.729      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.717     0.799     0.756      1560\n",
      "          1      0.744     0.648     0.693      1401\n",
      "\n",
      "avg / total      0.729     0.728     0.726      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.737\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.718     0.823     0.767      1560\n",
      "          1      0.765     0.640     0.697      1401\n",
      "\n",
      "avg / total      0.740     0.737     0.734      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.775     0.854     0.813      1560\n",
      "          1      0.817     0.723     0.767      1401\n",
      "\n",
      "avg / total      0.795     0.792     0.791      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.867     0.819      1560\n",
      "          1      0.830     0.723     0.773      1401\n",
      "\n",
      "avg / total      0.802     0.799     0.797      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.769     0.836     0.801      1560\n",
      "          1      0.798     0.721     0.757      1401\n",
      "\n",
      "avg / total      0.783     0.781     0.780      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.829     0.779      1560\n",
      "          1      0.778     0.665     0.717      1401\n",
      "\n",
      "avg / total      0.755     0.752     0.750      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.739     0.856     0.793      1560\n",
      "          1      0.805     0.664     0.728      1401\n",
      "\n",
      "avg / total      0.770     0.765     0.762      2961\n",
      "\n",
      "Testing features: 0, 2, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.712     0.782     0.745      1560\n",
      "          1      0.728     0.648     0.686      1401\n",
      "\n",
      "avg / total      0.719     0.719     0.717      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.870     0.826      1560\n",
      "          1      0.836     0.738     0.784      1401\n",
      "\n",
      "avg / total      0.810     0.807     0.806      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.869     0.824      1560\n",
      "          1      0.834     0.734     0.781      1401\n",
      "\n",
      "avg / total      0.808     0.805     0.804      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.779     0.854     0.815      1560\n",
      "          1      0.818     0.730     0.772      1401\n",
      "\n",
      "avg / total      0.798     0.796     0.795      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.772     0.817     0.794      1560\n",
      "          1      0.782     0.732     0.756      1401\n",
      "\n",
      "avg / total      0.777     0.777     0.776      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.853     0.819      1560\n",
      "          1      0.820     0.744     0.780      1401\n",
      "\n",
      "avg / total      0.803     0.802     0.801      2961\n",
      "\n",
      "Testing features: 0, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.720\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.792     0.749      1560\n",
      "          1      0.735     0.640     0.684      1401\n",
      "\n",
      "avg / total      0.722     0.720     0.718      2961\n",
      "\n",
      "Testing features: 0, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.829     0.778      1560\n",
      "          1      0.777     0.664     0.716      1401\n",
      "\n",
      "avg / total      0.754     0.751     0.749      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.872     0.837      1560\n",
      "          1      0.843     0.764     0.801      1401\n",
      "\n",
      "avg / total      0.822     0.821     0.820      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.870     0.834      1560\n",
      "          1      0.840     0.760     0.798      1401\n",
      "\n",
      "avg / total      0.820     0.818     0.817      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.807     0.862     0.833      1560\n",
      "          1      0.833     0.771     0.801      1401\n",
      "\n",
      "avg / total      0.820     0.819     0.818      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.827\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.810     0.878     0.842      1560\n",
      "          1      0.850     0.771     0.808      1401\n",
      "\n",
      "avg / total      0.829     0.827     0.826      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.810\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.856     0.826      1560\n",
      "          1      0.826     0.757     0.790      1401\n",
      "\n",
      "avg / total      0.811     0.810     0.809      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.810     0.876     0.841      1560\n",
      "          1      0.848     0.771     0.807      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.830\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.814     0.878     0.845      1560\n",
      "          1      0.851     0.776     0.812      1401\n",
      "\n",
      "avg / total      0.831     0.830     0.829      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.872     0.836      1560\n",
      "          1      0.843     0.761     0.800      1401\n",
      "\n",
      "avg / total      0.822     0.820     0.819      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.816     0.870     0.842      1560\n",
      "          1      0.843     0.781     0.811      1401\n",
      "\n",
      "avg / total      0.829     0.828     0.827      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.758     0.842     0.798      1560\n",
      "          1      0.800     0.700     0.747      1401\n",
      "\n",
      "avg / total      0.778     0.775     0.774      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.745     0.864     0.800      1560\n",
      "          1      0.816     0.671     0.736      1401\n",
      "\n",
      "avg / total      0.779     0.773     0.770      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.728     0.831     0.776      1560\n",
      "          1      0.776     0.654     0.710      1401\n",
      "\n",
      "avg / total      0.751     0.747     0.745      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.748     0.828     0.786      1560\n",
      "          1      0.782     0.689     0.732      1401\n",
      "\n",
      "avg / total      0.764     0.762     0.760      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.783\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.762     0.854     0.805      1560\n",
      "          1      0.813     0.702     0.753      1401\n",
      "\n",
      "avg / total      0.786     0.783     0.781      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.861     0.832      1560\n",
      "          1      0.832     0.767     0.798      1401\n",
      "\n",
      "avg / total      0.818     0.817     0.816      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.865     0.835      1560\n",
      "          1      0.837     0.768     0.801      1401\n",
      "\n",
      "avg / total      0.821     0.819     0.819      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.858     0.825      1560\n",
      "          1      0.826     0.754     0.789      1401\n",
      "\n",
      "avg / total      0.810     0.809     0.808      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.857     0.820      1560\n",
      "          1      0.823     0.741     0.780      1401\n",
      "\n",
      "avg / total      0.804     0.802     0.801      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.863     0.821      1560\n",
      "          1      0.828     0.733     0.778      1401\n",
      "\n",
      "avg / total      0.804     0.802     0.801      2961\n",
      "\n",
      "Testing features: 1, 2, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.749\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.742     0.804     0.772      1560\n",
      "          1      0.760     0.688     0.722      1401\n",
      "\n",
      "avg / total      0.750     0.749     0.748      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.825\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.810     0.874     0.841      1560\n",
      "          1      0.846     0.772     0.807      1401\n",
      "\n",
      "avg / total      0.827     0.825     0.825      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.881     0.844      1560\n",
      "          1      0.853     0.771     0.810      1401\n",
      "\n",
      "avg / total      0.831     0.829     0.828      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.795     0.875     0.833      1560\n",
      "          1      0.843     0.749     0.793      1401\n",
      "\n",
      "avg / total      0.818     0.815     0.814      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.804     0.860     0.831      1560\n",
      "          1      0.831     0.767     0.797      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.874     0.839      1560\n",
      "          1      0.845     0.766     0.803      1401\n",
      "\n",
      "avg / total      0.824     0.823     0.822      2961\n",
      "\n",
      "Testing features: 1, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.752     0.813     0.781      1560\n",
      "          1      0.771     0.701     0.734      1401\n",
      "\n",
      "avg / total      0.761     0.760     0.759      2961\n",
      "\n",
      "Testing features: 1, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.843     0.822      1560\n",
      "          1      0.815     0.768     0.791      1401\n",
      "\n",
      "avg / total      0.808     0.807     0.807      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.783     0.870     0.824      1560\n",
      "          1      0.835     0.732     0.780      1401\n",
      "\n",
      "avg / total      0.808     0.805     0.804      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.875     0.830      1560\n",
      "          1      0.842     0.740     0.788      1401\n",
      "\n",
      "avg / total      0.814     0.811     0.810      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.781     0.855     0.816      1560\n",
      "          1      0.820     0.733     0.774      1401\n",
      "\n",
      "avg / total      0.799     0.797     0.796      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.790     0.859     0.823      1560\n",
      "          1      0.826     0.745     0.783      1401\n",
      "\n",
      "avg / total      0.807     0.805     0.804      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.874     0.825      1560\n",
      "          1      0.838     0.728     0.779      1401\n",
      "\n",
      "avg / total      0.808     0.805     0.803      2961\n",
      "\n",
      "Testing features: 2, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.740\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.710     0.856     0.776      1560\n",
      "          1      0.792     0.611     0.690      1401\n",
      "\n",
      "avg / total      0.749     0.740     0.735      2961\n",
      "\n",
      "Testing features: 2, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.862     0.821      1560\n",
      "          1      0.827     0.735     0.778      1401\n",
      "\n",
      "avg / total      0.804     0.802     0.801      2961\n",
      "\n",
      "Testing features: 3, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.871     0.826      1560\n",
      "          1      0.836     0.735     0.782      1401\n",
      "\n",
      "avg / total      0.809     0.806     0.805      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 7, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.848     0.816      1560\n",
      "          1      0.814     0.742     0.777      1401\n",
      "\n",
      "avg / total      0.799     0.798     0.797      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 7, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.792     0.849     0.819      1560\n",
      "          1      0.817     0.751     0.783      1401\n",
      "\n",
      "avg / total      0.804     0.803     0.802      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.837     0.812      1560\n",
      "          1      0.806     0.751     0.777      1401\n",
      "\n",
      "avg / total      0.797     0.796     0.796      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.838     0.821      1560\n",
      "          1      0.811     0.774     0.792      1401\n",
      "\n",
      "avg / total      0.808     0.807     0.807      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 6, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.791\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.829     0.807      1560\n",
      "          1      0.797     0.748     0.772      1401\n",
      "\n",
      "avg / total      0.791     0.791     0.790      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.856     0.831      1560\n",
      "          1      0.828     0.771     0.799      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.816     0.874     0.844      1560\n",
      "          1      0.847     0.780     0.812      1401\n",
      "\n",
      "avg / total      0.831     0.829     0.829      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.858     0.822      1560\n",
      "          1      0.825     0.743     0.782      1401\n",
      "\n",
      "avg / total      0.806     0.804     0.803      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 5, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.797     0.856     0.825      1560\n",
      "          1      0.826     0.757     0.790      1401\n",
      "\n",
      "avg / total      0.810     0.809     0.808      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.734     0.796     0.764      1560\n",
      "          1      0.749     0.680     0.713      1401\n",
      "\n",
      "avg / total      0.741     0.741     0.740      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.741     0.823     0.780      1560\n",
      "          1      0.775     0.680     0.725      1401\n",
      "\n",
      "avg / total      0.757     0.755     0.754      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.733     0.817     0.773      1560\n",
      "          1      0.766     0.669     0.714      1401\n",
      "\n",
      "avg / total      0.749     0.747     0.745      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.737     0.799     0.767      1560\n",
      "          1      0.754     0.683     0.717      1401\n",
      "\n",
      "avg / total      0.745     0.744     0.743      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 3, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.760\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.749     0.819     0.782      1560\n",
      "          1      0.775     0.694     0.732      1401\n",
      "\n",
      "avg / total      0.761     0.760     0.758      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.837     0.810      1560\n",
      "          1      0.804     0.745     0.774      1401\n",
      "\n",
      "avg / total      0.794     0.794     0.793      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.847     0.823      1560\n",
      "          1      0.818     0.764     0.790      1401\n",
      "\n",
      "avg / total      0.809     0.808     0.808      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.788\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.782     0.828     0.804      1560\n",
      "          1      0.795     0.743     0.768      1401\n",
      "\n",
      "avg / total      0.788     0.788     0.787      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.768     0.819     0.793      1560\n",
      "          1      0.782     0.725     0.753      1401\n",
      "\n",
      "avg / total      0.775     0.774     0.774      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.783\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.834     0.802      1560\n",
      "          1      0.797     0.727     0.760      1401\n",
      "\n",
      "avg / total      0.784     0.783     0.782      2961\n",
      "\n",
      "Testing features: 0, 1, 2, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.725\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.721     0.779     0.749      1560\n",
      "          1      0.730     0.665     0.696      1401\n",
      "\n",
      "avg / total      0.725     0.725     0.724      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.800     0.855     0.827      1560\n",
      "          1      0.825     0.762     0.792      1401\n",
      "\n",
      "avg / total      0.812     0.811     0.810      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.857     0.831      1560\n",
      "          1      0.829     0.770     0.798      1401\n",
      "\n",
      "avg / total      0.817     0.816     0.815      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.842     0.814      1560\n",
      "          1      0.809     0.748     0.777      1401\n",
      "\n",
      "avg / total      0.798     0.797     0.797      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.788     0.809     0.798      1560\n",
      "          1      0.781     0.758     0.769      1401\n",
      "\n",
      "avg / total      0.785     0.785     0.785      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.840     0.821      1560\n",
      "          1      0.812     0.769     0.790      1401\n",
      "\n",
      "avg / total      0.807     0.807     0.806      2961\n",
      "\n",
      "Testing features: 0, 1, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.729     0.790     0.758      1560\n",
      "          1      0.742     0.672     0.705      1401\n",
      "\n",
      "avg / total      0.735     0.734     0.733      2961\n",
      "\n",
      "Testing features: 0, 1, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.789\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.787     0.821     0.804      1560\n",
      "          1      0.791     0.753     0.771      1401\n",
      "\n",
      "avg / total      0.789     0.789     0.789      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.776     0.853     0.813      1560\n",
      "          1      0.816     0.727     0.769      1401\n",
      "\n",
      "avg / total      0.795     0.793     0.792      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.786     0.860     0.821      1560\n",
      "          1      0.825     0.739     0.780      1401\n",
      "\n",
      "avg / total      0.805     0.803     0.802      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.832     0.804      1560\n",
      "          1      0.797     0.734     0.764      1401\n",
      "\n",
      "avg / total      0.787     0.786     0.785      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.777     0.819     0.798      1560\n",
      "          1      0.786     0.738     0.761      1401\n",
      "\n",
      "avg / total      0.781     0.781     0.780      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.784     0.840     0.811      1560\n",
      "          1      0.807     0.742     0.773      1401\n",
      "\n",
      "avg / total      0.795     0.794     0.793      2961\n",
      "\n",
      "Testing features: 0, 2, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.721     0.794     0.756      1560\n",
      "          1      0.742     0.657     0.697      1401\n",
      "\n",
      "avg / total      0.731     0.729     0.728      2961\n",
      "\n",
      "Testing features: 0, 2, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.774     0.814     0.794      1560\n",
      "          1      0.780     0.735     0.757      1401\n",
      "\n",
      "avg / total      0.777     0.777     0.776      2961\n",
      "\n",
      "Testing features: 0, 3, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.773     0.819     0.795      1560\n",
      "          1      0.784     0.732     0.757      1401\n",
      "\n",
      "avg / total      0.778     0.778     0.777      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 8, 9\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.806     0.856     0.830      1560\n",
      "          1      0.828     0.770     0.798      1401\n",
      "\n",
      "avg / total      0.816     0.816     0.815      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 8, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.809     0.878     0.842      1560\n",
      "          1      0.849     0.769     0.807      1401\n",
      "\n",
      "avg / total      0.828     0.826     0.825      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 7, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.794     0.855     0.823      1560\n",
      "          1      0.824     0.753     0.787      1401\n",
      "\n",
      "avg / total      0.808     0.807     0.806      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 6, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.805     0.853     0.828      1560\n",
      "          1      0.825     0.769     0.796      1401\n",
      "\n",
      "avg / total      0.814     0.814     0.813      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 5, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.811     0.864     0.837      1560\n",
      "          1      0.837     0.776     0.805      1401\n",
      "\n",
      "avg / total      0.823     0.822     0.822      2961\n",
      "\n",
      "Testing features: 1, 2, 3, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.747     0.826     0.785      1560\n",
      "          1      0.781     0.689     0.732      1401\n",
      "\n",
      "avg / total      0.763     0.761     0.760      2961\n",
      "\n",
      "Testing features: 1, 2, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.813\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.857     0.828      1560\n",
      "          1      0.827     0.763     0.794      1401\n",
      "\n",
      "avg / total      0.814     0.813     0.812      2961\n",
      "\n",
      "Testing features: 1, 3, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.802     0.865     0.832      1560\n",
      "          1      0.835     0.762     0.797      1401\n",
      "\n",
      "avg / total      0.818     0.816     0.816      2961\n",
      "\n",
      "Testing features: 2, 3, 5, 6, 7, 8, 9, 10\n",
      "finished training svm\n",
      "finished predicting\n",
      "Accuracy: 0.802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.789     0.854     0.820      1560\n",
      "          1      0.821     0.745     0.781      1401\n",
      "\n",
      "avg / total      0.804     0.802     0.802      2961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for s2 in new_indices:\n",
    "    indices = list(s2)\n",
    "    print \"Testing features: \" + str(indices).strip('[]')\n",
    "    #build X\n",
    "\n",
    "    X_neg = [[] for i in range(0, len(X_neg_all_features[0]))]\n",
    "    X_pos = [[] for i in range(0, len(X_pos_all_features[0]))]\n",
    "    for i in indices:\n",
    "        neg_features = X_neg_all_features[i]\n",
    "        j = 0\n",
    "        for feature in neg_features:\n",
    "            X_neg[j].append(feature)\n",
    "            j += 1\n",
    "        \n",
    "        pos_features = X_pos_all_features[i]\n",
    "        j = 0\n",
    "        for feature in pos_features:\n",
    "            X_pos[j].append(feature)\n",
    "            j += 1\n",
    "    \n",
    "    X = X_neg + X_pos\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    #train test accuracy\n",
    "    clf = train(X_train, y_train, 1000, 2)\n",
    "    predictions = test(clf, X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
